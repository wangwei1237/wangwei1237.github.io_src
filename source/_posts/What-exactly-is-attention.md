---
title: è‡ªæ³¨æ„åŠ›ç©¶ç«Ÿæ˜¯ä»€ä¹ˆï¼Ÿ
reward: false
top: false
date: 2024-10-16 17:19:11
authors:
categories:
  - ç®—æ³•ä¸æ•°å­¦
tags:
  - Attention
  - è‡ªæ³¨æ„åŠ›æœºåˆ¶
  - Transformer
---

![](1.png)
<!--more-->

## å‘é‡çš„ç‚¹ç§¯å’ŒçŸ©é˜µè¡¨ç¤º
ä»¤ $\mathbf{x}$ å’Œ $\mathbf{y}$ è¡¨ç¤ºç»´åº¦ä¸º $n$ çš„å‘é‡ï¼Œå³ï¼š

$$
\begin{aligned}
\mathbf{x} &= [x_1, x_2, ..., x_n] \\
\mathbf{y} &= [y_1, y_2, ..., y_n]
\end{aligned}
$$

åˆ™ $\mathbf{x}$ å’Œ $\mathbf{y}$ çš„ç‚¹ç§¯è¿ç®—ï¼ˆ$\cdot$ï¼‰å¯ä»¥è¡¨ç¤ºè¿™ä¸¤ä¸ªå‘é‡ä¹‹é—´çš„ç›¸ä¼¼åº¦ï¼Œå‘é‡ç‚¹ç§¯è¶Šå¤§ï¼Œè¡¨æ˜ä¸¤ä¸ªå‘é‡è¶Šç›¸ä¼¼ã€‚

$$
\mathbf{x} \cdot \mathbf{y} = \sum_{i=1}^{n}{x_iy_i} = x_1y_1 + x_2y_2 +...+ x_ny_n
$$

å¦‚æœç”¨å‘é‡ $\mathbf{x}$ è¡¨ç¤ºä¸€ä¸ªè¯çš„è¯å‘é‡ï¼Œå¯¹äº $m$ ä¸ªè¯å‘é‡è€Œè¨€ï¼Œå…¶ä»»æ„ä¸¤ä¸ªè¯å‘é‡ä¹‹é—´çš„ç›¸ä¼¼åº¦å¯ä»¥è¡¨ç¤ºä¸ºï¼š

$$
\mathbf{x_i} \cdot \mathbf{x_j} = \sum_{k=1}^{n}{x_{ik}y_{jk}} = x_{i1}y_{j1} + x_{i2}y_{j2} + ... + x_{in}y_{jn} \quad \forall i,j = 1,2,\cdots,m
$$


å¦‚æœç”¨ $m \times n$ çš„çŸ©é˜µ $\mathbf{X}$ æ¥è¡¨ç¤º $m$ ä¸ªè¯å‘é‡ï¼Œ

$$
\mathbf{X} = \begin{bmatrix}
\mathbf{x_1} \\
\mathbf{x_2} \\
\vdots \\
\mathbf{x_m}
\end{bmatrix} = \begin{bmatrix}
x_{11} & x_{12} & \cdots & x_{1n} \\
x_{21} & x_{22} & \cdots & x_{2n} \\
\vdots & \vdots & \vdots & \vdots \\
x_{m1} & x_{m2} & \cdots & x_{mn}
\end{bmatrix}
$$

åˆ™ $\mathbf{S} = \mathbf{X} \mathbf{X}^T$ ä¸º $m \times m$ çš„çŸ©é˜µï¼Œå¹¶ä¸” $s_{ij} = \mathbf{x_i} \cdot \mathbf{x_j}$ è¡¨ç¤ºç¬¬ $i$ ä¸ªè¯å‘é‡å’Œç¬¬ $j$ ä¸ªè¯å‘é‡ä¹‹é—´çš„ç›¸ä¼¼åº¦ã€‚

$$
\mathbf{X}\mathbf{X}^T = \begin{bmatrix}
\mathbf{x_1} \cdot \mathbf{x_1} & \mathbf{x_1} \cdot \mathbf{x_2} & \cdots & \mathbf{x_1} \cdot \mathbf{x_m} \\
\mathbf{x_2} \cdot \mathbf{x_1} & \mathbf{x_2} \cdot \mathbf{x_2}  & \cdots & \mathbf{x_2} \cdot \mathbf{x_m} \\
\vdots & \vdots & \vdots & \vdots \\
\mathbf{x_m} \cdot \mathbf{x_1} & \mathbf{x_m} \cdot \mathbf{x_2}  & \cdots & \mathbf{x_m} \cdot \mathbf{x_m}
\end{bmatrix}
$$

ä½¿ç”¨ `softmax()` å‡½æ•°å¯¹çŸ©é˜µ $\mathbf{S}$ è¿›è¡Œå½’ä¸€åŒ–å¤„ç†å¾—åˆ°çŸ©é˜µ $\mathbf{W} = \text{softmax}\left(\mathbf{X}\mathbf{X}^T\right)$ï¼Œä½¿å¾— $\mathbf{W}$ ä¸­æ¯ä¸€è¡Œçš„æ‰€æœ‰å…ƒç´ ä¹‹å’Œéƒ½ä¸º 1ï¼Œäºæ˜¯æ¯ä¸€è¡Œçš„å„ä¸ªå…ƒç´ å°±å¯ä»¥çœ‹ä½œæ˜¯ä¸€ä¸ªæƒé‡ã€‚

$w_{i1},w_{i2},\cdots,w_{im}$ åˆ†åˆ«è¡¨ç¤ºç¬¬ $i$ ä¸ªè¯å‘é‡å’Œç¬¬ $1,2,\cdots,m$ ä¸ªè¯å‘é‡ä¹‹é—´çš„ç›¸ä¼¼åº¦æƒé‡ï¼Œç”¨è¯¥æƒé‡åˆ†åˆ«ä¹˜ä»¥å¯¹åº”çš„è¯å‘é‡ï¼Œäºæ˜¯æˆ‘ä»¬å¾—åˆ°äº†ç¬¬ $i$ ä¸ªè¯çš„æ–°çš„è¡¨è¾¾å½¢å¼ $\mathbf{z_i}$ï¼ŒæŸä¸ªè¯å‘é‡çš„æƒé‡è¶Šå¤§ï¼Œè¡¨ç¤ºç›¸ä¼¼åº¦è¶Šé«˜ã€‚

$$
\mathbf{z_i} = \sum_{k=1}^{m}{w_{ik}\mathbf{x_k}} = w_{i1}\mathbf{x_1} + w_{i2}\mathbf{x_2} + \cdots + w_{im}\mathbf{x_m} 
$$ 

å¯¹äº $m$ ä¸ªè¯å‘é‡éƒ½æ‰§è¡Œå¦‚ä¸Šçš„æ“ä½œå¯ä»¥å¾—åˆ°ï¼š
$$
\mathbf{Z} = \begin{bmatrix}
\mathbf{z_1} \\
\mathbf{z_2} \\
\vdots \\
\mathbf{z_m}
\end{bmatrix} = \begin{bmatrix}
w_{11}\mathbf{x_1} + w_{12}\mathbf{x_2} + \cdots + w_{1m}\mathbf{x_m} \\
w_{21}\mathbf{x_1} + w_{22}\mathbf{x_2} + \cdots + w_{2m}\mathbf{x_m} \\
\vdots \\
w_{m1}\mathbf{x_1} + w_{m2}\mathbf{x_2} + \cdots + w_{mm}\mathbf{x_m}
\end{bmatrix}
$$

å®é™…ä¸Šï¼Œå¯ä»¥è¯æ˜ï¼š

$$
\mathbf{Z} = \mathbf{W}\mathbf{X} = \text{softmax}\left(\mathbf{X}\mathbf{X}^T\right)\mathbf{X}
$$

äºæ˜¯ï¼Œå¯¹äºåŸå§‹çš„è¯å‘é‡çŸ©é˜µ $\mathbf{X}$ è€Œè¨€ï¼Œç»è¿‡ä¸€äº›åˆ—çš„çŸ©é˜µä¹˜æ³•è¿ç®—ï¼Œæˆ‘ä»¬å¾—åˆ°äº†æ ¹æ®ç›¸å…³æ€§æƒé‡çš„åŠ æƒè¯å‘é‡è¡¨ç¤ºã€‚ä¹Ÿå°±æ˜¯è¯´åŸå§‹çš„è¯å‘é‡ $\mathbf{x_i}$ å¯ä»¥è¡¨ç¤ºä¸ºæ‰€æœ‰è¯å‘é‡çš„åŠ æƒè¡¨ç¤º $\mathbf{z_i}$ã€‚æ‰€ä»¥ï¼Œå¯ä»¥ç”¨ `Attention` æ¥è§£é‡Šä¸€å¥è¯ä¸­ä¸åŒè¯ä¹‹é—´çš„ç›¸äº’å…³ç³»ã€‚Transformer æ¨¡å‹ä¸­çš„ `Attention` å…¶å®å°±æ˜¯çŸ©é˜µ $\mathbf{Z}$ï¼Œæ‰€ä»¥ Transformer å¯ä»¥ç†è§£è¾“å…¥åºåˆ—ä¸­çš„ä¸åŒçš„éƒ¨åˆ†ï¼Œå¹¶åˆ†æè¾“å…¥åºåˆ—ä¸­ä¸åŒè¯ä¹‹é—´çš„å…³ç³»ï¼Œè¿›è€Œæ•è·åˆ°ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚

äº†è§£å¦‚ä¸Šä»‹ç»çš„ $\text{softmax}\left(\mathbf{X}\mathbf{X}^T\right)\mathbf{X}$ èƒŒåçš„é€»è¾‘å¯¹äºç†è§£ Transformer ä¸­çš„å„ä¸ªçŸ©é˜µçš„å«ä¹‰è‡³å…³é‡è¦ï¼Œå› æ­¤æˆ‘ä»¬èŠ±äº†å¾ˆå¤§çš„ç¯‡å¹…æ¥å¯¹å…¶è¿›è¡Œåˆ†æã€‚

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ç”¨ä¸€ä¸ªå…·ä½“çš„ä¾‹å­æ¥å±•ç¤ºå¦‚ä¸Šçš„è¿‡ç¨‹ã€‚

### ä¸¾ä¸ªä¾‹å­ğŸŒ°
ä»¥ `I am good` è¿™å¥è¯ä¸ºä¾‹ï¼Œæˆ‘ä»¬ç”¨è¯å‘é‡ $\mathbf{x_1}$ è¡¨ç¤º `I`ï¼Œ$\mathbf{x_2}$ è¡¨ç¤º `am`ï¼Œ$\mathbf{x_3}$ è¡¨ç¤º `good`ï¼Œå¯¹åº”çš„è¯å‘é‡åˆ†åˆ«ä¸ºï¼š

$$
\begin{aligned}
\mathbf{x_1} &= [1, 3, 2] \\
\mathbf{x_2} &= [1, 1, 3] \\
\mathbf{x_3} &= [1, 2, 1]
\end{aligned}
$$

æ‰€ä»¥ï¼Œæˆ‘ä»¬æœ‰çŸ©é˜µ $\mathbf{X}$ï¼š

$$
\mathbf{X} = \begin{bmatrix}
1 & 3 & 2 \\
1 & 1 & 3 \\
1 & 2 & 1
\end{bmatrix}
$$

äºæ˜¯ï¼Œ$\mathbf{W}$ çŸ©é˜µçš„è®¡ç®—è¿‡ç¨‹å¦‚ä¸‹ï¼š

![](zmatrix.png)

æƒé‡çŸ©é˜µ $\mathbf{W}$ ä¸­æŸä¸€è¡Œåˆ†åˆ«ä¸ $\mathbf{X}$ çš„ä¸€åˆ—ç›¸ä¹˜ï¼Œå¦‚å‰æ‰€è¿°ï¼Œè¯¥æ“ä½œç›¸å½“äºå¯¹ $\mathbf{X}$ ä¸­å„è¡Œå‘é‡ï¼ˆä¸åŒçš„è¯å‘é‡ï¼‰åŠ æƒæ±‚å’Œï¼Œå¾—åˆ°çš„ç»“æœæ˜¯æ¯ä¸ªè¯å‘é‡ $\mathbf{x_i}$ ç»è¿‡åŠ æƒæ±‚å’Œä¹‹åçš„æ–°è¡¨ç¤º $\mathbf{z_i}$ï¼Œè€Œæƒé‡çŸ©é˜µåˆ™æ˜¯ç»è¿‡ç›¸ä¼¼åº¦å’Œå½’ä¸€åŒ–è®¡ç®—è€Œå¾—åˆ°çš„ã€‚

![](vec_matrix.png)

$$
\begin{aligned}
\mathbf{z_{I}}    &= 0.97 \cdot \mathbf{x_{I}} + 0.02 \cdot \mathbf{x_{am}} + 0.01 \cdot \mathbf{x_{good}} \\
\mathbf{z_{am}}   &= 0.27 \cdot \mathbf{x_{I}} + 0.73 \cdot \mathbf{x_{am}} + 0.00 \cdot \mathbf{x_{good}} \\
\mathbf{z_{good}} &= 0.90 \cdot \mathbf{x_{I}} + 0.05 \cdot \mathbf{x_{am}} + 0.05 \cdot \mathbf{x_{good}}
\end{aligned}
$$

### R å®ç°è¿‡ç¨‹

```R
softmax <- function(mat) {
  exp_mat <- exp(mat)           # å¯¹çŸ©é˜µä¸­çš„æ¯ä¸ªå…ƒç´ å–æŒ‡æ•°
  row_sums <- rowSums(exp_mat)  # è®¡ç®—æ¯è¡Œçš„æ€»å’Œ
  softmax_mat <- sweep(exp_mat, 1, row_sums, FUN = "/")  # å¯¹æ¯è¡Œè¿›è¡Œå½’ä¸€åŒ–
  return(softmax_mat)
}

X <- matrix(c(1, 3, 2, 1, 1, 3, 1, 2, 1), nrow = 3, byrow = TRUE)
W <- softmax(X %*% t(X))
Z <- W %*% X
print(Z)

#=================================================================

     [,1]     [,2]     [,3]
[1,]    1 2.957691 2.011295
[2,]    1 1.540148 2.722573
[3,]    1 2.864164 2.000000
```

### å¼•å…¥æ–°çš„çŸ©é˜µ
ç°åœ¨ï¼Œæˆ‘ä»¬å¼•å…¥ä¸€ä¸ªæ–°çš„çŸ©é˜µ $\mathbf{Y}$ï¼Œå¹¶ä»¤ 

$$
\mathbf{Z} = \text{softmax}\left(\mathbf{X}\mathbf{Y}^T\right)\mathbf{Y} = \mathbf{W}\mathbf{Y}
$$

æ ¹æ®ä¹‹å‰çš„æè¿°ï¼Œæ­¤æ—¶çš„ $\mathbf{Z}$ ä¸º $\mathbf{X}$ ä¸­çš„æ¯ä¸€ä¸ªè¯å‘é‡ $\mathbf{x_i}$ åœ¨ $\mathbf{Y}$ ä¸­çš„æ¯ä¸€ä¸ªè¯å‘é‡ $\mathbf{y_j}$ çš„åŠ æƒè¡¨ç¤ºï¼Œå³ï¼š

$$
\mathbf{x_i} = \sum_{j=1}^{m}{w_{ij}\mathbf{y_j}}
$$

![](w_xy.png)

$$
\begin{aligned}
\mathbf{z_{I}}    &= 0.10 \cdot \mathbf{y_{\text{æˆ‘}}} + 0.66 \cdot \mathbf{y_{\text{å¾ˆ}}} + 0.24 \cdot \mathbf{y_{\text{å¥½}}} \\
\mathbf{z_{am}}   &= 0.49 \cdot \mathbf{y_{\text{æˆ‘}}} + 0.49 \cdot \mathbf{y_{\text{å¾ˆ}}} + 0.02 \cdot \mathbf{y_{\text{å¥½}}} \\
\mathbf{z_{good}} &= 0.16 \cdot \mathbf{y_{\text{æˆ‘}}} + 0.42 \cdot \mathbf{y_{\text{å¾ˆ}}} + 0.42 \cdot \mathbf{y_{\text{å¥½}}}
\end{aligned}
$$

å¦‚ä¸Šçš„è®¡ç®—å’Œæƒé‡åªæ˜¯ä¸ºäº†è¯´æ˜åŸç†è€Œéšæœºé€‰æ‹©çš„æ•°æ®ï¼Œå¹¶ä¸ä»£è¡¨çœŸå®çš„ç›¸å…³æ€§ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä¼šçœ‹åˆ° `I` å’Œ `å¾ˆ` ä¹‹é—´çš„å…³ç³»æœ€ç›¸ä¼¼ï¼Œå®é™…ä¸Š `I` å’Œ `æˆ‘` ä¹‹é—´æœ€ç›¸ä¼¼ã€‚

## Transformer ä¸­çš„è‡ªæ³¨æ„åŠ›
è®ºæ–‡ [Attention Is All You Need](https://arxiv.org/html/1706.03762v7) çš„ 3.2 èŠ‚å¯¹ Attention çš„æè¿°å¦‚ä¸‹[^1]ï¼š

> An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors.
>  
> The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.

è®ºæ–‡ä¸­ä¹Ÿç»™å‡ºäº† Self-Attention çš„è®¡ç®—å…¬å¼ï¼š

$$
\text{Attention}(\mathbf{Q},\mathbf{K},\mathbf{V})=\text{softmax}\left(\frac{\mathbf{Q}\mathbf{K}^T}{\sqrt{d_k}}\right)\mathbf{V}
$$

å…¶ä¸­ï¼Œ$\mathbf{Q}$ã€$\mathbf{K}$ å’Œ $\mathbf{V}$ éƒ½æ˜¯çŸ©é˜µï¼Œåˆ†åˆ«ä»£è¡¨ `Query`ã€`Key` å’Œ `Value`ï¼Œ$d_k$ æ˜¯ $\mathbf{K}$ çš„è¡Œå‘é‡ç»´åº¦ã€‚ `Query`ã€`Key` å’Œ `Value` éƒ½æ˜¯ä¸ºäº†è®¡ç®— `è‡ªæ³¨æ„åŠ›` è€Œå¼•å…¥çš„æŠ½è±¡çš„æ¦‚å¿µï¼Œå®ƒä»¬éƒ½æ˜¯å¯¹åŸå§‹çš„è¾“å…¥ $\mathbf{X}$ çš„çº¿æ€§å˜æ¢ã€‚

$$
\begin{aligned}
\mathbf{Q} &= \mathbf{X}\mathbf{W}^Q \\
\mathbf{K} &= \mathbf{X}\mathbf{W}^K \\
\mathbf{V} &= \mathbf{X}\mathbf{W}^V
\end{aligned}
$$

å› ä¸º $\mathbf{X}$ æ˜¯ $m \times n$ çš„çŸ©é˜µï¼Œæ‰€ä»¥å¦‚æœä»¤ $\mathbf{W}^Q$ã€$\mathbf{W}^K$ã€$\mathbf{W}^V$ å‡æ˜¯ $n \times n$ çš„å•ä½çŸ©é˜µ $\mathbf{I}$ï¼Œé‚£ä¹ˆ $\mathbf{Q}$ã€$\mathbf{K}$ã€$\mathbf{V}$ ç»è¿‡çº¿æ€§å˜æ¢ï¼ˆ$\mathbf{I}$ï¼‰åä»ç„¶æ˜¯ $\mathbf{X}$ï¼Œæ­¤æ—¶æˆ‘ä»¬å¯ä»¥ç”¨ $\mathbf{X}$ æ›¿æ¢ $\mathbf{Q}$ã€$\mathbf{K}$ã€$\mathbf{V}$ï¼Œé‚£ä¹ˆå…¬å¼å°±å˜æˆäº†ï¼š

$$
\text{Attention}(\mathbf{Q},\mathbf{K},\mathbf{V})=\text{softmax}\left(\frac{\mathbf{X}\mathbf{X}^T}{\sqrt{d_k}}\right)\mathbf{X}
$$

è¿™ä¹Ÿå°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬ä¹‹å‰é¢è¯´ï¼šTransformer æ¨¡å‹ä¸­çš„ `Attention` å…¶å®å°±æ˜¯å¯¹åŸå§‹è¾“å…¥è¯å‘é‡çš„åŠ æƒæ±‚å’Œè€Œå¾—åˆ°çš„æ–°çš„è¡¨ç¤ºï¼Œåœ¨æ–°çš„è¡¨ç¤ºä¸­ï¼ŒTransformer å¯ä»¥ç†è§£è¾“å…¥åºåˆ—ä¸­çš„ä¸åŒçš„éƒ¨åˆ†ï¼Œå¹¶åˆ†æè¾“å…¥åºåˆ—ä¸­ä¸åŒè¯ä¹‹é—´çš„å…³ç³»ï¼Œè¿›è€Œæ•è·åˆ°ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚

å®é™…ä¸Šï¼Œä¸ºäº†å¢å¼ºå¢å¼ºæ¨¡å‹çš„æ‹Ÿåˆèƒ½åŠ›ï¼Œæˆ‘ä»¬å¹¶ä¸ä¼šé‡‡ç”¨å•ä½çŸ©é˜µ $\mathbf{I}$ å¯¹çŸ©é˜µ $\mathbf{X}$ åšçº¿æ€§å˜æ¢ï¼Œè€Œæ˜¯åˆ†åˆ«é‡‡ç”¨ $\mathbf{W}^Q$ã€$\mathbf{W}^K$ã€$\mathbf{W}^V$ è¿™ä¸‰ä¸ªå¯ä»¥é€šè¿‡å¤§é‡è¯­æ–™è®­ç»ƒè€Œå­¦ä¹ åˆ°çš„å‚æ•°çŸ©é˜µï¼ˆå‚æ•°çŸ©é˜µå¯ä»¥æ˜¯ä»»ä½•ç»´åº¦ï¼Œä½†è¡Œå‘é‡ä¸ªæ•°å¿…é¡»å’Œ $\mathbf{X}$ çš„è¡Œå‘é‡ç»´åº¦ä¸€è‡´ï¼‰ã€‚

é™¤æ­¤ä¹‹å¤–ï¼Œåœ¨è®¡ç®— `softmax()` ä¹‹å‰ï¼ŒTransformer ä¼šç”¨ $\sqrt{d_k}$ å¯¹ $\mathbf{Q}\mathbf{K}^T$ çš„ç»“æœè¿›è¡Œç¼©æ”¾ï¼Œæ‰€ä»¥è®ºæ–‡ä¸­ä¹ŸæŠŠ `è‡ªæ³¨æ„åŠ›` å«ä½œï¼šScaled Dot-Product Attentionã€‚å½“ $d_k$ å¾ˆå¤§æ—¶ï¼Œ$\mathbf{Q}\mathbf{K}^T$ çš„å‘é‡ç‚¹ç§¯ä¼šéå¸¸å¤§ï¼Œå¦‚æœä¸è¿›è¡Œç¼©æ”¾ï¼Œ`softmax()` çš„æ¢¯åº¦ä¼šéå¸¸å°ï¼Œè¿›è€Œå¯¼è‡´åœ¨åå‘ä¼ æ’­æ—¶ï¼Œæ¨¡å‹å‚æ•°æ›´æ–°çš„é€Ÿåº¦ä¼šå˜æ…¢ï¼Œç”šè‡³æ¥è¿‘åœæ»ï¼Œå½±å“æ¨¡å‹çš„å­¦ä¹ æ•ˆç‡ã€‚

è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„æ•´ä½“æµç¨‹å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![è‡ªæ³¨æ„åŠ›æœºåˆ¶](sattention.png)

æ‰€ä»¥ï¼Œæ ¹æ® $\text{Attention}(\mathbf{Q},\mathbf{K},\mathbf{V})$ï¼ŒTransformer å¯ä»¥ç†è§£ä¸€å¥è¯ä¸­ä¸åŒè¯ä¹‹é—´çš„ç›¸äº’å…³ç³»ã€‚

$\mathbf{Q},\mathbf{K},\mathbf{V}$ å’ŒåŸå§‹è¾“å…¥ä¹‹é—´çš„å…³ç³»å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![è®¡ç®— Qã€Kã€V çŸ©é˜µçš„ç¤ºä¾‹](qkv.png)


### Scaled çš„è¯´æ˜
å¯¹äºä¸€ä¸ª $m\times n$ çš„çŸ©é˜µ $\mathbf{A}$ï¼Œ`softmax()` çš„è®¡ç®—å¦‚ä¸‹ï¼š

$$
\text{softmax}(\mathbf{A}_{ij}) = \frac{e^{\mathbf{A}_{ij}}}{\sum_{k=1}^{n}e^{\mathbf{A}_{ik}}}
$$

$d_k$ å˜å¤§æ—¶ï¼Œä¼šå­˜åœ¨å‘é‡ç‚¹ç§¯ç»“æœè¿‡å¤§çš„å¯èƒ½æ€§ï¼Œå¦‚æœ $\mathbf{A}_{ij}$ å¾ˆå¤§ï¼Œåˆ™ $e^{\mathbf{A}_{ij}}$ ä¼šè¿…é€Ÿè¶‹è¿‘äºæ— ç©·å¤§ï¼Œè€Œè¡Œå‘é‡ä¸­å…¶ä½™è¾ƒå°çš„ $\mathbf{A}_{ij}$ åˆ™ä¼šå˜å¾—ç›¸å¯¹è¾ƒå°ã€‚æ­¤æ—¶ï¼Œ`softmax()` çš„ç»“æœä¼šå‘ 0 æˆ– 1 çš„ä¸¤ä¸ªæç«¯å€¼æ¨ç§»ã€‚

æ ¹æ® `softmax()` çš„æ¢¯åº¦å®šä¹‰[^2]ï¼š

$$
\frac{\partial \text{softmax}(\mathbf{A}_{ij})}{\partial \mathbf{A}_{ij}} = \text{softmax}(\mathbf{A}_{ij})(1 - \text{softmax}(\mathbf{A}_{ij}))
$$

å½“ `softmax()` çš„è¾“å‡ºå€¼è¿‡äºæç«¯ï¼ˆè¾“å‡º 0 æˆ– 1ï¼‰æ—¶ï¼Œ`softmax()` çš„æ¢¯åº¦ç»“æœå°†è¶‹è¿‘äºé›¶ï¼Œåœ¨åå‘ä¼ æ’­æ—¶ï¼Œæ¨¡å‹å‚æ•°çš„æ›´æ–°é€Ÿåº¦ä¼šéå¸¸æ…¢ï¼Œç”šè‡³æ¥è¿‘åœæ»ã€‚è¿™ä¹Ÿå°±æ˜¯æ‰€è°“çš„æ¢¯åº¦æ¶ˆå¤±ï¼ˆvanishing gradientï¼‰é—®é¢˜ï¼Œæ¢¯åº¦æ¶ˆå¤±ä¼šå¯¼è‡´æ¨¡å‹è®­ç»ƒæ•ˆç‡æå¤§é™ä½ï¼Œç‰¹åˆ«æ˜¯åœ¨æ·±åº¦ç¥ç»ç½‘ç»œä¸­ï¼Œæ¢¯åº¦æ¶ˆå¤±å¯èƒ½ä¼šå¯¼è‡´ç½‘ç»œæ— æ³•æœ‰æ•ˆå­¦ä¹ ã€‚

ä¸ºäº†é¿å…æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼Œå¯ä»¥ä½¿ç”¨ $\frac{1}{\sqrt{d_k}}$ ç¼©æ”¾ç‚¹ç§¯ç»“æœï¼š$\frac{\mathbf{Q}\mathbf{K}^T}{\sqrt{d_k}}$ã€‚ç¼©æ”¾ä¹‹åï¼Œç‚¹ç§¯ç»“æœçš„æ•°å€¼ä¼šå˜å°ï¼Œç¡®ä¿ `softmax()` çš„è¾“å…¥ä¸ä¼šè¿‡å¤§ï¼Œé¿å… `softmax()` è¾“å‡ºè¿‡äºæç«¯çš„å€¼ï¼Œä»è€Œä½¿å¾— `softmax()` çš„æ¢¯åº¦æ›´åŠ é€‚ä¸­ï¼Œè¿›è€Œæ›´æœ‰åˆ©äºæ¨¡å‹çš„æœ‰æ•ˆè®­ç»ƒã€‚

### Multi-Head Attention
å›æƒ³ä¸€ä¸‹å‰é¢æˆ‘ä»¬ä»‹ç»çš„ `I am good` çš„ä¾‹å­ï¼Œæˆ‘ä»¬å‡è®¾è¯¥ç»“æœå°±æ˜¯æŒ‰ç…§ Transforer ä¸­çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶è®¡ç®—è€Œæ¥ï¼Œé‚£ä¹ˆ `good` çš„è‡ªæ³¨æ„åŠ›ä¸ºï¼š

$$
\mathbf{z_{good}} = 0.90 \cdot \mathbf{v_{I}} + 0.05 \cdot \mathbf{v_{am}} + 0.05 \cdot \mathbf{v_{good}}
$$

ç”±æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹å‡ºï¼Œ`good` çš„è‡ªæ³¨æ„åŠ›å€¼å®é™…ä¸Šç”± `I` çš„å€¼å‘é‡æ¥ä¸»å¯¼ã€‚

ä½†æ˜¯å¦‚æœæˆ‘ä»¬è¦è®¡ç®—ä¸‹å¥ä¸­çš„ `it` çš„è‡ªæ³¨æ„åŠ›å€¼å‘¢ï¼Ÿ`it` çš„è‡ªæ³¨æ„åŠ›å€¼åº”è¯¥ç”± `dog` ä¸»å¯¼è¿˜æ˜¯ç”± `food` ä¸»å¯¼å‘¢ï¼Ÿå½“ç„¶ï¼Œæˆ‘ä»¬å¯ä»¥å¾ˆå®¹æ˜“çš„çœ‹å‡ºï¼Œ`it` çš„è‡ªæ³¨æ„åŠ›å€¼åº”è¯¥ç”± `dog` ä¸»å¯¼ï¼Œä½†æ˜¯æœºå™¨å¦‚ä½•æ¥åˆ¤æ–­å‘¢ï¼Ÿ

> A dog ate the food because it was hungry.

å¦‚æœ `it` çš„è‡ªæ³¨æ„åŠ›å€¼çš„ç»“æœå¦‚ä¸‹æ‰€ç¤ºï¼Œé‚£ä¹ˆ `it` çš„è‡ªæ³¨æ„åŠ›å€¼ç¡®å®ç”± `dog` ä¸»å¯¼ï¼š

![](msattention.png)

ä½†å¦‚æœ `it` çš„è‡ªæ³¨æ„åŠ›å€¼çš„ç»“æœå¦‚ä¸‹æ‰€ç¤ºå‘¢ï¼Ÿ

![](msattention_2.png)

å› æ­¤ï¼Œä¸ºäº†ç¡®ä¿ç»“æœçš„å‡†ç¡®æ€§ï¼Œæˆ‘ä»¬ä¸èƒ½ä¾èµ–ä¸€ä¸ªæ³¨æ„åŠ›çŸ©é˜µï¼Œè€Œåº”è¯¥è®¡ç®—å¤šä¸ªæ³¨æ„åŠ›çŸ©é˜µï¼Œå¹¶å°†è¿™å¤šä¸ªæ³¨æ„åŠ›çŸ©é˜µçš„ç»“æœæ•´åˆèµ·æ¥ï¼Œè¿™ä¹Ÿå°±æ˜¯ `Multi-Head Attention` çš„ç”±æ¥[^3]ã€‚å¦‚æœæˆ‘ä»¬æœ‰ $h$ ä¸ªæ³¨æ„åŠ›çŸ©é˜µï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥æŒ‰å¦‚ä¸‹çš„æ–¹å¼å°†è¿™ $h$ ä¸ªæ³¨æ„åŠ›çŸ©é˜µçš„ç»“æœæ•´åˆèµ·æ¥å¾—åˆ°æœ€ç»ˆçš„æ³¨æ„åŠ›çŸ©é˜µï¼š

$$
\text{MultiHeadAttention} = \text{Concatenate}(\mathbf{Z}_1,\cdots,\mathbf{Z}_h)\mathbf{W}_0
$$

å…¶ä¸­ï¼Œ$\mathbf{W}_0$ æ˜¯ä¸€ä¸ªæ–°çš„æƒé‡çŸ©é˜µã€‚

## Transformer ä¸­çš„ Encoder
åœ¨ `Multi-Head Attention` çš„åŸºç¡€ä¹‹ä¸Šï¼Œå†å¢åŠ å‰é¦ˆç½‘ç»œã€å åŠ å’Œå½’ä¸€ç»„ä»¶ï¼Œå°±å¾—åˆ°äº†å®Œæ•´çš„ Transformer Encoderã€‚å½“ç„¶ï¼Œå®é™…ä¸­ï¼Œå¯ä»¥å°† $N$ ä¸ª Encoder ä¸€ä¸ªä¸€ä¸ªçš„å åŠ èµ·æ¥ï¼Œæœ€åä¸€ä¸ª Encoder çš„è¾“å‡ºå°±æ˜¯åŸå§‹è¾“å…¥å†…å®¹çš„ç‰¹å¾å€¼ $\mathbf{R}$ã€‚

![Encoder æ¶æ„å›¾](encoder.png)

## Transformer ä¸­çš„ Decoder

## å‚è€ƒæ–‡çŒ®
[^1]: [Attention Is All You Need](https://arxiv.org/html/1706.03762v7)
[^2]: [The Softmax function and its derivative](https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/)
[^3]: [Intuition for Multi-headed Attention](https://medium.com/@ngiengkianyew/multi-headed-attention-8b940b76c351)