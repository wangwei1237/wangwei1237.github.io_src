---
title: ä» Transformer åˆ° GPT
reward: false
top: false
date: 2024-10-31 18:19:48
authors:
categories:
  - ç®—æ³•ä¸æ•°å­¦
tags:
  - GPT
  - Transformer
---

![](1.png)
åœ¨ [è‡ªæ³¨æ„åŠ›ç©¶ç«Ÿæ˜¯ä»€ä¹ˆï¼Ÿ](/2024/10/16/What-exactly-is-attention/) ä¸€æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„ Transformer æ¨¡å‹çš„åŸºæœ¬åŸç†å’Œæ¶æ„ã€‚

* 2017å¹´ 6 æœˆï¼Œè°·æ­Œæœºå™¨ç¿»è¯‘å›¢é˜Ÿæå‡ºçš„æœºå™¨ç¿»è¯‘æ¨¡å‹ Transformer å°±åƒå¤§è¯­è¨€æ¨¡å‹çš„ä¸€é¢—ç§å­ä¸€æ ·ï¼Œæ‚„ç„¶è½åœ°ç”Ÿæ ¹ï¼Œå¹¶è¿…é€Ÿå¸­å·äº† AI é¢†åŸŸã€‚
* ä¸€å¹´ä¹‹åï¼Œ2018 å¹´ 6 æœˆï¼ŒOpenAI å‘å¸ƒäº†åŸºäº Transformer æ¶æ„çš„ GPT-1[^gpt1]ï¼Œè™½ç„¶å½“æ—¶è¿˜å­˜åœ¨ä¸€äº›å±€é™æ€§ï¼Œä¾‹å¦‚å½“æ—¶è¿˜ä¸èƒ½æ ¹æ®ä¸€ä¸ªç»™å®šçš„æ ‡é¢˜æ¥ç”Ÿæˆä¸€ç¯‡æ–°é—»æŠ¥é“ï¼›ä½†æ˜¯ï¼Œè°ä¹Ÿæ²¡æƒ³åˆ°ï¼Œå°±æ˜¯è¿™ä¸ªæ¡†æ¶ï¼Œåœ¨ 4 å¹´ä¹‹åæˆä¸ºäº† AI é¢†åŸŸæœ€ç‚™æ‰‹å¯çƒ­çš„æ¨¡å‹ã€‚
* 4 ä¸ªæœˆåï¼Œ2018 å¹´ 10 æœˆï¼Œè°·æ­Œä¹Ÿå‘å¸ƒäº†åŸºäº Transformer æ¶æ„çš„ BERT æ¨¡å‹[^bert]ï¼Œä¸ GPT-1 ç›¸æ¯”ï¼ŒBERT åœ¨å¾ˆå¤šä¸‹æ¸¸ä»»åŠ¡ä¸Šè¡¨ç°å‡ºæ›´å¼ºåŠ²çš„æ€§èƒ½ï¼Œå¹¶ä¸”ä¹Ÿåˆ·æ–°äº†å¤šä¸ªæ¦œå•çš„è®°å½•ã€‚åœ¨å¾ˆé•¿ä¸€æ®µæ—¶é—´é‡Œï¼ŒBERTï¼ˆåŠå…¶å˜ä½“ï¼‰ä¸€ç›´å¤„äºå„ç±»æ¦œå•çš„é¦–ä½ï¼Œæ˜¯äººä»¬è°ˆè®ºçš„ç„¦ç‚¹ã€‚
* ç›´åˆ° 2022 å¹´ 3 æœˆï¼ŒOpenAI å‘å¸ƒäº† GPT-3.5[^gpt35]ï¼Œå¹¶åŸºäº GPT-3.5 äºå½“å¹´çš„ 11 æœˆ 30 æ—¥æ­£å¼å‘å¸ƒäº†é¢å‘æ¶ˆè´¹ç”¨æˆ·çš„äº§å“â€”â€”ChatGPTï¼Œå¤§æ¨¡å‹å†æ¬¡å¼•èµ·äº†åœˆå†…ã€åœˆå¤–çš„å¹¿æ³›è®¨è®ºï¼Œå¼€å¯äº†æ–°ä¸€è½®çš„å¤§æ¨¡å‹æ—¶ä»£ã€‚

è¿™ç¯‡æ–‡ç« ï¼Œæˆ‘ä»¬å°±æ¥è¯¦ç»†çš„ä»‹ç»ä¸€ä¸‹ä¼ å¥‡çš„ GPT æ¨¡å‹ä»¥åŠå…¶åŸç†ï¼Œæ…¢æ…¢æ­å¼€ GPT é‚£ç¥ç§˜çš„é¢çº±ï¼Œä¹Ÿä¸ºåç»­å¯¹ [Prompt Caching](https://openai.com/index/api-prompt-caching/) çš„è®¨è®ºæ‰“ä¸‹åšå®çš„åŸºç¡€ã€‚
<!--more-->

## å¤§è¯­è¨€æ¨¡å‹æ—è°±
[A Survey on ChatGPT and Beyond](https://arxiv.org/abs/2304.13712) ç»™å‡ºäº†åŸºäº Transformer æ¶æ„çš„å¤§è¯­è¨€æ¨¡å‹çš„å‘å±•æ¦‚å†µå’Œä¸åŒæ¨¡å‹ä¹‹é—´çš„æ¼”åŒ–å…³ç³»[^harnessingpowerllmspractice]ï¼š

![å¤§è¯­è¨€æ¨¡å‹è¿›åŒ–æ ‘](LLMTree.jpeg)

ä»å›¾ä¸­å¯ä»¥çœ‹å‡ºï¼Œå¤§è¯­è¨€æ¨¡å‹çš„æ¶æ„æ•´ä½“ä¸Šå¯ä»¥åˆ†ä¸º Enoder-Onlyã€Encoder-Decoderã€Decoder-Only ä¸‰ç±»ï¼š

* Encoder-Only æ¨¡å‹ï¼šä»…åŒ…å« Transformer ä¸­çš„ç¼–ç å™¨éƒ¨åˆ†ï¼Œä¸»è¦ç”¨äºä»è¾“å…¥æ•°æ®æå–ç‰¹å¾æˆ–è¡¨ç¤ºï¼Œæœ€å…¸å‹çš„ä»£è¡¨å°±æ˜¯ 2018 å¹´ 10 æœˆï¼Œè°·æ­Œä¹Ÿå‘å¸ƒçš„ BERT[^bert]ã€‚ 
* Encoder-Decoder æ¨¡å‹ï¼šåŒ…å«å®Œæ•´çš„ Transformer çš„ç¼–ç å™¨å’Œè§£ç å™¨ï¼Œç¼–ç å™¨è´Ÿè´£å°†è¾“å…¥åºåˆ—è½¬æ¢ä¸ºå‹ç¼©çš„ä¸­é—´è¡¨ç¤ºï¼Œè§£ç å™¨åˆ™åŸºäºè¿™ä¸ªä¸­é—´è¡¨ç¤ºç”Ÿæˆç›®æ ‡è¾“å‡ºåºåˆ—ï¼Œæœ€å…¸å‹çš„ä»£è¡¨å°±æ˜¯ 2019 å¹´ è°·æ­Œå‘å¸ƒçš„ T5 æ¨¡å‹[^t5]ã€‚
* Decoder-Only æ¨¡å‹ï¼šä»…åŒ…å« Transformer ä¸­çš„è§£ç å™¨éƒ¨åˆ†ï¼Œä¸“æ³¨äºä½¿ç”¨ **è‡ªå›å½’** çš„æ–¹å¼æ ¹æ®å‰é¢ç”Ÿæˆçš„å†…å®¹ç”Ÿæˆæ–°çš„åºåˆ—ï¼Œæ–°çš„è¾“å‡ºä»…ä¾èµ–äºå‰é¢ç”Ÿæˆçš„æ‰€æœ‰å†…å®¹ï¼Œæœ€å…¸å‹çš„ä»£è¡¨å°±æ˜¯ 2018 å¹´ 6 æœˆï¼ŒOpenAI å‘å¸ƒçš„ GPT-1[^gpt1]ã€‚2020 å¹´ 1æœˆï¼ŒOpenAI å‘å¸ƒäº†æ‰€è°“çš„è¯­è¨€æ¨¡å‹çš„ **Scaling Law** [^scalinglaw]ï¼Œå°¤å…¶æ˜¯åœ¨ å½“å‰çš„ 5 æœˆå‘å¸ƒ GPT-3[^gpt3] ä¹‹åï¼ŒDecoder-Only æ¶æ„æˆä¸ºäº†è¯­è¨€æ¨¡å‹é¢†åŸŸçš„ä¸»æµæ¶æ„ã€‚åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬æ‰€ç†ŸçŸ¥çš„å¤§éƒ¨åˆ†çš„å¤§è¯­è¨€æ¨¡å‹éƒ½æ˜¯åŸºäº Decoder-Only æ¶æ„ï¼ŒçœŸå¯è°“æ˜¯ä¸€æç‹¬ç§€ã€‚

## è‡ªç¼–ç &è‡ªå›å½’
å¯¹äº **ã€åŒ—äº¬çš„ç§‹å¤©æ˜¯æœ€ç¾çš„å­£èŠ‚ï¼Œæˆ‘çˆ±åŒ—äº¬ã€** è¿™å¥è¯ï¼Œå¦‚æœæˆ‘ä»¬å¯¹è¿™å¥è¯ä¸­çš„è¯è¿›è¡Œéšæœºæ©ç ï¼ˆé®ç›–ï¼‰ï¼Œæˆ‘ä»¬å¯ä»¥å¾—åˆ°å¦‚ä¸‹çš„æ©ç ç»“æœï¼š

> åŒ—äº¬çš„`__`æ˜¯æœ€ç¾çš„å­£èŠ‚ï¼Œæˆ‘çˆ±åŒ—äº¬

æˆ‘ä»¬çš„ç›®æ ‡å°±æ˜¯è®©æ¨¡å‹æ¥é¢„æµ‹ç©ºç™½å¤„çš„è¢«æ©ç æ‰çš„è¯ã€‚æœ‰ä¸¤ç§æ¨¡å‹å¯ä»¥å®Œæˆè¿™ä¸ªä»»åŠ¡ï¼šè‡ªç¼–ç æ¨¡å‹ï¼ˆAuto-Encoderï¼‰å’Œè‡ªå›å½’æ¨¡å‹ï¼ˆAuto-Regressiveï¼‰ã€‚

### è‡ªç¼–ç æ¨¡å‹
è‡ªç¼–ç æ¨¡å‹åœ¨é¢„æµ‹ `ç©ºç™½è¯` æ—¶ä¼šåŒæ—¶ä»ä¸¤ä¸ªæ–¹å‘é˜…è¯»å¥å­ï¼Œå¯ä»¥åŒæ—¶åˆ©ç”¨æ­£å‘é¢„æµ‹å’Œåå‘é¢„æµ‹çš„ä¼˜åŠ¿æ¥å®Œæˆé¢„æµ‹ï¼š

* å¦‚æœä½¿ç”¨æ­£å‘é¢„æµ‹ï¼Œé‚£ä¹ˆæ¨¡å‹ä¼šä»å·¦åˆ°å³è¯»å–æ‰€æœ‰çš„è¯ï¼Œç›´åˆ°é‡åˆ° `ç©ºç™½è¯`ï¼Œç„¶åè¿›è¡Œé¢„æµ‹ï¼š
> åŒ—äº¬çš„`__`

* å¦‚æœä½¿ç”¨åå‘é¢„æµ‹ï¼Œé‚£ä¹ˆæ¨¡å‹ä¼šä»å³åˆ°å·¦è¯»å–æ‰€æœ‰çš„è¯ï¼Œç›´åˆ°é‡åˆ° `ç©ºç™½è¯`ï¼Œç„¶åè¿›è¡Œé¢„æµ‹ï¼š
> `__`æ˜¯æœ€ç¾çš„å­£èŠ‚ï¼Œæˆ‘çˆ±åŒ—äº¬

ä»ä¸¤ä¸ªæ–¹å‘è¯»å–å¥å­ï¼Œæ¨¡å‹èƒ½å¤Ÿæ›´åŠ å…¨é¢å’Œæ¸…æ™°çš„ç†è§£å¥å­ï¼Œå› æ­¤è‡ªç¼–ç æ¨¡å‹èƒ½å¤Ÿç»™å‡ºæ›´å¥½çš„ç»“æœã€‚è¿™ä¹Ÿå°±æ˜¯ä¸ºä»€ä¹ˆæ¯” GPT-1 æ™š 4 ä¸ªæœˆå‘å¸ƒ BERT æ¨¡å‹åœ¨å½“æ—¶èƒ½å¤Ÿè·å¾—æ›´å¥½çš„æ•ˆæœçš„ä¸»è¦åŸå› [^bert]ï¼Œä¹Ÿæ˜¯ 2021 å¹´ä»¥å‰ï¼Œä»¥ BERT ä¸ºä»£è¡¨çš„è‡ªç¼–ç æ¨¡å‹èƒ½å¤Ÿä¸€éª‘ç»å°˜çš„åŸå› ã€‚

![æ¥æºï¼šBERT, Pre-training of Deep Bidirectional Transformers for Language Understanding](bert_result.png)

### è‡ªå›å½’æ¨¡å‹
ä¸è‡ªç¼–ç æ¨¡å‹ä¸åŒï¼Œè‡ªå›å½’æ¨¡å‹åœ¨é¢„æµ‹ `ç©ºç™½è¯` æ—¶è¦ä¹ˆä»…ä½¿ç”¨æ­£å‘é¢„æµ‹ï¼Œè¦ä¹ˆä»…ä½¿ç”¨åå‘é¢„æµ‹ï¼Œè€Œä¸èƒ½åƒè‡ªç¼–ç æ¨¡å‹é‚£æ ·å¯ä»¥åŒæ—¶åˆ©ç”¨æ­£å‘é¢„æµ‹å’Œåå‘é¢„æµ‹çš„ä¼˜åŠ¿æ¥å®Œæˆé¢„æµ‹ã€‚å¤§éƒ¨åˆ†æƒ…å†µä¸‹ï¼Œåœ¨é¢„æµ‹ `ç©ºç™½è¯` æ—¶ï¼Œè‡ªå›å½’æ¨¡å‹ä¼šä½¿ç”¨æ­£å‘é¢„æµ‹ï¼š
> åŒ—äº¬çš„`__`

æœ¬è´¨ä¸Šï¼Œè‡ªå›å½’æ¨¡å‹æ˜¯å•å‘é¢„æµ‹æ¨¡å‹ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œè‡ªå›å½’æ¨¡å‹åªèƒ½æ²¿ä¸€ä¸ªæ–¹å‘æ¥ç†è§£å¥å­å¹¶åšå‡ºé¢„æµ‹ã€‚è¿™ä¹Ÿå°±æ˜¯æˆ‘ä»¬é€šå¸¸æ‰€è¯´çš„ï¼šGPT åªèƒ½æ ¹æ®è¾“å…¥åºåˆ—ä¸­çš„å‰é¢çš„å†…å®¹æ¥é¢„æµ‹åºåˆ—ä¸­çš„ä¸‹ä¸€ä¸ªè¯ã€‚

## GPT æ¨¡å‹
### GPT-1
åœ¨ *Improving Language Understanding by Generative Pre-Training* [^gpt1] ä¸­ï¼ŒOpenAI æå‡ºäº† GPT-1 çš„æ¨¡å‹æ¶æ„ï¼ˆé¢„è®­ç»ƒæ¨¡å‹+ä¸‹æ¸¸ä»»åŠ¡å¾®è°ƒï¼‰ï¼Œå¹¶ä¸”å…¶æ ¸å¿ƒæ˜¯ä¸€ç§ **å¤šå±‚ Transformer è§£ç å™¨** çš„ Transformer æ¶æ„å˜ä½“ã€‚GPT-1 å…ˆè®¡ç®—è¾“å…¥çš„ä¸Šä¸‹æ–‡ tokens çš„å¤šå¤´è‡ªæ³¨æ„åŠ›ï¼Œç„¶ååœ¨ç»è¿‡å‰å‘åé¦ˆç½‘ç»œçš„å¤„ç†ï¼Œæœ€ç»ˆç”Ÿæˆç›®æ ‡è¾“å‡º token çš„æ¦‚ç‡åˆ†å¸ƒã€‚åœ¨è®ºæ–‡çš„ç¬¬ 3 èŠ‚ Framework ä¸­ï¼Œç»™å‡ºäº†æ¨¡å‹çš„æ•´ä½“æ¶æ„æè¿°ï¼š

> In our experiments, we use a **multi-layer Transformer decoder** for the language model, which is a variant of the transformer. This model applies a multi-headed self-attention operation over the input context tokens followed by position-wise feedforward layers to produce an output distribution over target tokens.

åœ¨è®ºæ–‡çš„ç¬¬ 4 èŠ‚ Experiments ä¸­ï¼Œç»™å‡ºäº†æ¨¡å‹çš„è¯¦ç»†å‚æ•°å’Œè®­ç»ƒç»†èŠ‚ï¼ŒåŒ…æ‹¬æ¨¡å‹è¶…å‚æ•°ã€è®­ç»ƒæ•°æ®é›†å’Œè®­ç»ƒè¿‡ç¨‹ç­‰ï¼š

> **Model specifications** 
> * Our model largely follows the original transformer work. 
> * We trained a 12-layer decoder-only transformer with masked self-attention heads (768 dimensional states and 12 attention heads). 
> * For the position-wise feed-forward networks, we used 3072 dimensional inner states. We used the Adam optimization scheme with a max learning rate of 2.5e-4. The learning rate was increased linearly from zero over the first 2000 updates and annealed to 0 using a cosine schedule. We train for 100 epochs on minibatches of 64 randomly sampled, contiguous sequences of 512 tokens. 
> * Since layernorm is used extensively throughout the model, a simple weight initialization of N(0,0.02) was sufficient. 
> * We used a bytepair encoding (BPE) vocabulary with 40,000 merges and residual, embedding, and attention dropouts with a rate of 0.1 for regularization. We also employed a modified version of L2 regularization proposed in , with w= 0.01 on all non bias or gain weights. For the activation function, we used the Gaussian Error Linear Unit (GELU). 
> * We used learned position embeddings instead of the sinusoidal version proposed in the original work.

### GPT-2
åœ¨ *Language Models are Unsupervised Multitask Learners* [^gpt2] ä¸­çš„ç¬¬ 2 èŠ‚ Approach ä¸­ï¼Œç»™å‡ºäº† GPT-2 å’Œ GPT-1 åœ¨å¤„ç†ä¸‹æ¸¸ä»»åŠ¡çš„æ•°æ®çš„ä¸åŒï¼š
* GPT-1 å¯¹ä¸‹æ¸¸ä»»åŠ¡çš„æ•°æ®è¿›è¡Œäº†æ ‡æ³¨å’Œå¤„ç†ï¼ŒåŠ å…¥äº†ä¸€äº›ç‰¹æ®Šçš„è¯å…ƒï¼ˆ\<s\>ï¼Œ\<e\>ï¼Œ\<$\>ï¼‰ï¼Œåˆ†åˆ«è¡¨ç¤ºå¼€å§‹ã€æå–ã€åˆ†éš”ã€‚åœ¨é¢„è®­ç»ƒé˜¶æ®µï¼Œæ¨¡å‹æ²¡æœ‰è§è¿‡è¿™äº›è¯å…ƒï¼Œå› æ­¤æ¨¡å‹å¿…é¡»ç»è¿‡å¾®è°ƒé˜¶æ®µçš„é‡æ–°è®­ç»ƒåæ‰ä¼šè®¤è¯†è¿™äº›æ–°å¢åŠ çš„è¯å…ƒã€‚
* åœ¨ GPT-2 çš„ Zero-Shot çš„è®¾å®šä¸‹ï¼Œå¯¹äºä¸‹æ¸¸ä»»åŠ¡è€Œè¨€ï¼Œæ¨¡å‹ä¸èƒ½å†æ¬¡è¢«è°ƒæ•´å’Œæ›´æ–°ï¼Œå› æ­¤å°±æ— æ³•å†å¼•å…¥ä¸€äº›æ–°çš„è¯å…ƒã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œå¯¹äºä¸‹æ¸¸ä»»åŠ¡çš„è¾“å…¥ï¼Œæ¨¡å‹åœ¨é¢„è®­ç»ƒé˜¶æ®µéƒ½åº”è¯¥è§è¿‡ï¼Œè¿™è¦æ±‚æ¨¡å‹çš„è¾“å…¥å¿…é¡»æ›´æ¥è¿‘äººç±»çš„è‡ªç„¶è¯­è¨€ã€‚åœ¨å»ºæ¨¡è¿‡ç¨‹ä¸­ï¼Œåªæœ‰ä¸€ä¸ªå­¦ä¹ ä»»åŠ¡ï¼Œå³ç»™å®šè¾“å…¥åºåˆ—æ¥é¢„æµ‹è¾“å‡ºå•è¯çš„æ¦‚ç‡ï¼š$p(output|input)$ã€‚åŒæ—¶ï¼Œæ¨¡å‹åº”è¯¥èƒ½å¤Ÿæ‰§è¡Œè®¸å¤šä¸åŒçš„ä»»åŠ¡ï¼Œä¸åŒçš„ä»»åŠ¡ï¼Œå³ä½¿è¾“å…¥ç›¸åŒï¼Œæ¨¡å‹çš„è¾“å‡ºä¹Ÿåº”è¯¥ä¸åŒï¼Œå› æ­¤æ¨¡å‹çš„å­¦ä¹ ç›®æ ‡å°±å˜æˆäº†åœ¨ç»™å®šè¾“å…¥å’Œä»»åŠ¡çš„å‰æä¸‹é¢„æµ‹è¾“å‡ºçš„æ¦‚ç‡ï¼š$p(output|input, task)$ã€‚

åœ¨æ¨¡å‹çš„åç»­å‘å±•ä¸­ï¼Œæˆ‘ä»¬ç§° GPT-2 ä¸­è¿™ç§æ›´æ¥è¿‘äººç±»è‡ªç„¶è¯­è¨€çš„è¾“å…¥ä¸º **æç¤ºè¯**ï¼ˆ*Prompt*ï¼‰ï¼ŒGPT-2 çš„è¿™ç§ Zero-Shot è®¾å®šå’Œä½¿ç”¨è‡ªç„¶è¯­è¨€æç¤ºè¯æ¥æŒ‡å¯¼é¢„è®­ç»ƒæ¨¡å‹æ‰§è¡Œä¸‹æ¸¸ä»»åŠ¡æ˜¯ GPT-2 çš„æœ€å¤§èƒ†çš„ä¸€æ­¥ã€‚

ä½†æ˜¯ï¼Œåœ¨æ¨¡å‹ç»“æ„ä¸Šï¼Œè®ºæ–‡çš„ç¬¬ 2.3 èŠ‚ Model ä¸­æŒ‡å‡ºï¼ŒGPT-2 ä¸ GPT-1 ç›¸æ¯”åŸºæœ¬ä¸€è‡´ï¼Œä»…ä»…æ˜¯åšäº†ä¸€äº›å°çš„ä¿®æ”¹è€Œå·²ï¼š
* æŠŠ Post-LN æ”¹æˆäº† Pre-LN
* åœ¨æœ€åä¸€ä¸ª Transformer-Decoder æ¨¡å—ä¹‹åå¢åŠ äº†ä¸€ä¸ª Norm å±‚ã€‚
* æ‰©å¤§äº† Transformer-Encoder æ¨¡å—çš„å±‚æ•°ä»¥åŠæ¨¡å‹å¯¹åº”çš„è¶…å‚æ•°çš„å¢åŠ ã€‚

> We use a Transformer based architecture for our LMs. 
> * The model largely follows the details of the OpenAI GPT model (Radford et al., 2018) with a few modifications. 
> * Layer normalization was moved to the input of each sub-block, similar to a pre-activation residual network and an additional layer normalization was added after the final self-attention block.

### GPT-3
GPT-1 æå‡ºçš„â€œé¢„è®­ç»ƒæ¨¡å‹+ä¸‹æ¸¸ä»»åŠ¡å¾®è°ƒâ€çš„æ¨¡å¼æœ‰ä¸€ä¸ªå¾ˆå¤§çš„é™åˆ¶ï¼šä¸‹æ¸¸ä»»åŠ¡éœ€è¦é’ˆå¯¹ç‰¹å®šä»»åŠ¡æ ‡æ³¨æ•°æ®é›†ã€‚GPT-2 è™½ç„¶å¯ä»¥å® Zero-Shotï¼Œå¹¶ä¸å†éœ€è¦é’ˆå¯¹ç‰¹å®šä»»åŠ¡æ ‡æ³¨æ•°æ®é›†ï¼Œä½†åœ¨å½“æ—¶ä»ç„¶å­˜åœ¨å¾ˆå¤šé™åˆ¶å¯¼è‡´æ¨¡å‹å¹¶æ²¡æœ‰è¾¾åˆ° SOTA çš„æ•ˆæœã€‚

ä»åº”ç”¨çš„è§’åº¦è®²ï¼ŒæŒç»­æå‡ Zero-Shot çš„æ•ˆæœéå¸¸å¿…è¦ï¼Œå› ä¸ºå¯¹äºâ€œé¢„è®­ç»ƒæ¨¡å‹+ä¸‹æ¸¸ä»»åŠ¡å¾®è°ƒâ€è€Œè¨€ï¼š
* ä¸åŒçš„ä¸‹æ¸¸ä»»åŠ¡éƒ½éœ€è¦å¤§é‡çš„æ ‡æ³¨æ•°æ®é›†ï¼Œä½†æ˜¯ä¸‹æ¸¸ä»»åŠ¡ç±»å‹éå¸¸å¤šï¼Œå¦‚æœæ¯ä¸ªä»»åŠ¡éƒ½è¦æ”¶é›†æ•°æ®é›†å¹¶å¾®è°ƒçš„è¯ï¼Œæˆæœ¬ç›¸å¯¹è¾ƒå¤§ã€‚
* åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šå¾®è°ƒä¹‹åçš„æ•ˆæœå¥½å¹¶ä¸ä¸€å®šèƒ½è¯´æ˜é¢„è®­ç»ƒçš„å¤§æ¨¡å‹æ³›åŒ–æ•ˆæœå¥½ï¼Œè¿˜æœ‰å¯èƒ½æ˜¯è¿‡æ‹Ÿåˆäº†é¢„è®­ç»ƒæ•°æ®é›†æ‰€åŒ…å«çš„ä¸€éƒ¨åˆ†å¾®è°ƒä»»åŠ¡çš„æ•°æ®è€Œå·²ã€‚å› æ­¤ï¼Œå¯¹äºä¸‹æ¸¸ä»»åŠ¡è€Œè¨€ï¼Œå¦‚æœä¸éœ€è¦é’ˆå¯¹æ€§çš„å¾®è°ƒï¼Œé‚£ä¹ˆèµ·å†³å®šæ€§ä½œç”¨çš„å°±æ˜¯é¢„è®­ç»ƒæ¨¡å‹çš„æ³›åŒ–æ€§ã€‚
* ä»äººç±»å­¦ä¹ ä¸€ä¸ªæ–°æŠ€èƒ½çš„è§’åº¦çœ‹ï¼Œæˆ‘ä»¬æ‰§è¡Œä¸€ä¸ªæ–°çš„ä»»åŠ¡æ—¶å¹¶ä¸éœ€è¦å·¨å¤§çš„æ•°æ®é›†ï¼Œå¯èƒ½çœ‹ä¸€ä¸¤ä¸ªä¾‹å­å°±å­¦ä¼šäº†ã€‚

å¯¹äºæ‰€æœ‰ç±»å‹çš„ä»»åŠ¡ï¼Œåœ¨æ²¡æœ‰ä»»ä½•æ¢¯åº¦æ›´æ–°æˆ–å¾®è°ƒçš„æƒ…å†µä¸‹ï¼ŒGPT-3 åªéœ€é€šè¿‡è‡ªç„¶è¯­è¨€æ–‡æœ¬çš„æ–¹å¼ç»™å‡ºå°‘é‡ç¤ºä¾‹å¹¶æŒ‡å®šä»»åŠ¡å°±å¯ä»¥å®Œæˆå¯¹åº”çš„ä»»åŠ¡ã€‚

åœ¨ *Language Models are Few-Shot Learners* [^gpt3] çš„ç¬¬ 2.1 èŠ‚ Model and Architectures ä¸­æŒ‡å‡ºï¼ŒGPT-3 å’Œ GPT-2 çš„æ¨¡å‹æ¶æ„æ˜¯ä¸€è‡´çš„ï¼Œå¹¶ä¸”ä¸ºäº†éªŒè¯å…ˆå‰çš„è®ºæ–‡ *Scaling Laws for Neural Language Models* [^scalinglaw] ä¸­æå‡ºçš„â€œScaling Lawsâ€ï¼ŒOpenAI ç‰¹æ„è®­ç»ƒäº† 8 ä¸ªä¸åŒè§„æ¨¡çš„æ¨¡å‹ã€‚

> * We use the same model and architecture as GPT-2, including the modified initialization, pre-normalization, and reversible tokenization described therein, with the exception that we use alternating dense and locally banded sparse attention patterns in the layers of the transformer, similar to the Sparse Transformer. 
> * To study the dependence of ML performance on model size, we train 8 different sizes of model, ranging over three orders of magnitude from 125 million parameters to 175 billion parameters, with the last being the model we call GPT-3.

### GPT æ¶æ„çš„æ¼”åŒ–
å› æ­¤ï¼Œæ ¹æ®å¦‚ä¸Šçš„æè¿°ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹å‡º GPT ç³»åˆ—æ¨¡å‹æ¶æ„çš„æ¼”åŒ–è¿‡ç¨‹å¦‚ä¸‹ï¼š

![ä» Transformer åˆ° GPT æ¶æ„çš„æ¼”åŒ–](GPTX_arch.png)

åˆ©ç”¨å¦‚ä¸Šçš„æ¨¡å‹æ¶æ„ï¼Œæˆ‘ä»¬å¯ä»¥å®ç°å¦‚ä¸‹æ‰€ç¤ºçš„å†…å®¹ç”Ÿæˆä»»åŠ¡ï¼š

![ä½¿ç”¨ GPT è¿›è¡Œå†…å®¹ç»­å†™çš„ç¤ºä¾‹](GPT_demo_for_student.gif)

## GPT çš„å‹ç¼©æœ¬è´¨
åœ¨å¦‚ä¸Šçš„ demo ä¸­ï¼ŒGPT æ˜¯å¦‚ä½•æ ¹æ®æç¤ºè¯æ¥ç”Ÿæˆä¸‹ä¸€ä¸ªè¯å‘¢ï¼Ÿ

åœ¨ [è‡ªæ³¨æ„åŠ›ç©¶ç«Ÿæ˜¯ä»€ä¹ˆï¼Ÿ](/2024/10/16/What-exactly-is-attention/) è¿™ç¯‡æ–‡ç« çš„ *ä¸¾ä¸ªä¾‹å­ğŸŒ°* è¿™ä¸€èŠ‚æˆ‘ä»¬æåˆ°ï¼Œç»è¿‡çŸ©é˜µçš„çº¿æ€§å˜æ¢åï¼Œ`I am good` è¿™ä¸‰ä¸ªè¯çš„è¯å‘é‡ä¹‹é—´å¯ä»¥è¡¨ç¤ºæˆå¦‚ä¸‹çš„å½¢å¼ï¼š

$$
\begin{aligned}
\mathbf{z_{I}}    &= 0.97 \cdot \mathbf{x_{I}} + 0.02 \cdot \mathbf{x_{am}} + 0.01 \cdot \mathbf{x_{good}} \\
\mathbf{z_{am}}   &= 0.27 \cdot \mathbf{x_{I}} + 0.73 \cdot \mathbf{x_{am}} + 0.00 \cdot \mathbf{x_{good}} \\
\mathbf{z_{good}} &= 0.90 \cdot \mathbf{x_{I}} + 0.05 \cdot \mathbf{x_{am}} + 0.05 \cdot \mathbf{x_{good}}
\end{aligned}
$$

æ¢å¥è¯è¯´ï¼Œç»è¿‡è‡ªæ³¨æ„åŠ›æœºåˆ¶åï¼Œæç¤ºè¯ä¸­çš„æœ€åä¸€ä¸ªè¯éƒ½å°†åŒ…å«æç¤ºè¯ä¸­çš„ä»»ä½•ä¸€ä¸ªå•è¯çš„ä¿¡æ¯ã€‚éšç€ Transformer-Decoder çš„å±‚çº§ä¸æ–­å¢åŠ ï¼Œæç¤ºè¯ä¸­çš„æœ€åä¸€ä¸ªè¯æ‰€åŒ…å«çš„ä¿¡æ¯å°†ä¼šè¶Šæ¥è¶Šä¸°å¯Œã€è¶Šæ¥è¶Šç²¾å‡†ã€‚ä»å¦ä¸€ä¸ªè§’åº¦æ¥è®²ï¼Œç»è¿‡å¤šå±‚ Transformer-Decoder ä¹‹åï¼Œæç¤ºè¯ä¸­çš„æ‰€æœ‰ä¿¡æ¯å°†å‹ç¼©è‡³äº†æœ€åä¸€ä¸ªè¯å‘é‡ä¸­ï¼Œç„¶åï¼Œæˆ‘ä»¬å°±å¯ä»¥åˆ©ç”¨æœ€åä¸€ä¸ªè¯å‘é‡æ¥ç”Ÿæˆä¸‹ä¸€ä¸ªè¯ã€‚

å…·ä½“çš„è¿‡ç¨‹å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![è¯å‘é‡çš„å¤„ç†æµç¨‹](llm_compress.png)

è¿™ä¹Ÿå°±æ˜¯ä¸ºä»€ä¹ˆ OpenAI å‰é¦–å¸­ç§‘å­¦å®¶ Ilya Sutskever åœ¨[å…¬å¼€é‡‡è®¿](https://www.youtube.com/watch?v=goOa0biX6Tc&ab_channel=FuVenture) ä¸­æŒ‡å‡ºå¤§è§„æ¨¡é¢„è®­ç»ƒæœ¬è´¨ä¸Šæ˜¯åœ¨åšä¸€ä¸ªä¸–ç•ŒçŸ¥è¯†çš„å‹ç¼©ï¼Œä»è€Œèƒ½å¤Ÿå­¦ä¹ åˆ°ä¸€ä¸ªç¼–ç ä¸–ç•ŒçŸ¥è¯†çš„å‚æ•°æ¨¡å‹ï¼Œè¿™ä¸ªæ¨¡å‹èƒ½å¤Ÿé€šè¿‡è§£å‹ç¼©æ‰€éœ€è¦çš„çŸ¥è¯†æ¥è§£å†³çœŸå®ä¸–ç•Œçš„ä»»åŠ¡ã€‚

æ‰€ä»¥ Ilya Sutskever ä¸€ç›´çš„ä¿¡å¿µå°±æ˜¯ï¼š
> å¦‚æœèƒ½å¤Ÿé«˜æ•ˆçš„å‹ç¼©ä¿¡æ¯ï¼Œå°±å·²ç»å¾—åˆ°äº†çŸ¥è¯†ã€‚æƒ³é«˜æ•ˆå‹ç¼©ä¿¡æ¯ï¼Œå°±ä¸€å®šå¾—æœ‰ä¸€äº›çŸ¥è¯†ï¼Œæ‰€ä»¥ä»–åšä¿¡ GPT-3 å’Œæœ€æ–°çš„ GPT-4ï¼Œå®ƒä»¬å·²ç»æœ‰äº†ä¸€ä¸ªä¸–ç•Œæ¨¡å‹åœ¨é‡Œé¢ï¼GPT å­¦çš„å…¶å®ä¸æ˜¯è¯­è¨€ï¼Œè€Œæ˜¯è¯­è¨€èƒŒåçš„é‚£ä¸ªçœŸå®ä¸–ç•Œã€‚

åœ¨ [Language Modeling Is Compression](https://arxiv.org/pdf/2309.10668) è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œä½œè€…ä¹Ÿæåˆ°ï¼š
> ç”±äºå¤§æ¨¡å‹è¡¨ç°å‡ºå¼ºæ‚çš„é¢„æµ‹èƒ½åŠ›ï¼Œå› æ­¤å®ƒä»¬éå¸¸é€‚åˆæˆä¸ºå¼ºå¤§çš„å‹ç¼©å™¨ã€‚æˆ‘ä»¬é€šè¿‡å‹ç¼©çš„è§†è§’æ¥çœ‹å¾…å¤§æ¨¡å‹çš„é¢„æµ‹é—®é¢˜ï¼Œå¹¶è¯„ä¼°äº†å¤§æ¨¡å‹çš„å‹ç¼©èƒ½åŠ›ã€‚

## GPT æ¨¡å‹çš„å‚æ•°é‡

![GPT é¢„æµ‹ä¸‹ä¸€ä¸ªè¯çš„è¯¦ç»†è¿‡ç¨‹](GPT_arch_detail.png)
## GPT æ¨¡å‹çš„è®¡ç®—é‡


## å‚è€ƒæ–‡çŒ®
[^gpt1]: [Improving Language Understanding by Generative Pre-Training](https://openai.com/index/language-unsupervised/)
[^bert]: [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)
[^gpt2]: [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
[^gpt3]: [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)
[^gpt35]: [Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155)
[^harnessingpowerllmspractice]: [Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond](https://arxiv.org/abs/2304.13712)
[^t5]: [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683)
[^scalinglaw]: [Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361)