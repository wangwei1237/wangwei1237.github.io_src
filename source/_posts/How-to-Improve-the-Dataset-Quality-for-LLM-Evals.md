---
title: 如何提升大模型评测数据集的质量？
reward: false
top: false
date: 2026-02-06 16:41:18
authors:
categories:
  - LLM
tags:
  - 数据集质量
---

![](dsquality.jpeg)

在 [大模型评测 7 条原则](/2025/03/29/Seven-Principles-for-Large-Model-Evaluation/) 中，我们把大模型评测过程中遇到的影响结果置信度的问题进行了总结。在 2025 年，这些实践总结也确实帮助我们提升了评测的质量。但是，在 2025 年末的时候，随着模型 SOTA 的不断突破（例如： *Nano Banana Pro*、*Seedream-4.5*、……），我们慢慢发现：我们的评测区分度存在不断下降的趋势，这导致我们很难通过评测来区分不同模型之间的性能差异。

然而，实际的情况并非我们的结果所呈现的那样，尤其在图片生成、视频生成等场景，不同的模型之间还是存在区分度的。在 Arena 平台的图生图对抗榜单显示：模型之间确实存在比较明显的差异。

<!--more-->

![](i2i_arena_rank.png)

**当评测的精度无法有效区分模型之间的差异时，评测工作本身也就失去了意义。** 于是，接下来的一段时间内，我们开始思考：**如何提升评测精度这一问题？**

!!! warning "注意事项"
    本文中的例子多会引用 𝕏 平台上的推文，因此如果您可以访问 𝕏，那么您将获得更好的阅读体验。

## 评测精度的本质

正如下图所示，大模型评测体系一般由如下五个部分构成：评测数据集、评测方法、打分与排序策略、评测指标、评测场景。

![](Evals_system.jpeg)




## TEST
{% twitter https://x.com/wangwei1237/status/2019673608726200554 %}

