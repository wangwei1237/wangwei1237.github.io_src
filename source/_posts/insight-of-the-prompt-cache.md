---
title: Prompt Cache ç©¶ç«Ÿæ˜¯ä»€ä¹ˆï¼Ÿ
reward: false
top: false
date: 2024-11-23 22:17:05
authors:
categories:
  - LLM
tags:
  - Prompt Cache
  - LLM Interface
---

![](context_caching.png)

åœ¨ä»‹ç»äº† [Transformer](/2024/10/16/What-exactly-is-attention/) æ¨¡å‹ã€[GPT](/2024/10/31/From-Transformer-To-GPT/) æ¨¡å‹ã€[å¤§æ¨¡å‹çš„è¿è¡Œæ—¶æ¨ç†å’Œ KV Cache](/2024/11/16/The-LLMs-Runtime-Inference-and-KV-Cache/) åï¼Œæˆ‘ä»¬ç»ˆäºè¶Šæ¥è¶Šæ¥è¿‘äºæœ€åŸå§‹çš„ç›®æ ‡ï¼šOpenAI 2024 å¹´ 10 æœˆ 1 æ—¥å‘å¸ƒçš„ [Prompt Caching in the API](https://openai.com/index/api-prompt-caching/)ã€‚è¿™ç¯‡æ–‡ç« ï¼Œæˆ‘ä»¬å°±æ¥ä»‹ç»ä¸€ä¸‹ `Prompt Cache` ç›¸å…³æŠ€æœ¯çš„å‘å±•å¹¶å¯¹ OpenAI çš„ `Prompt Caching` æŠ€æœ¯æ–¹æ¡ˆè¿›è¡Œç®€å•çš„åˆ†æã€‚
<!--more-->

## KV Cache
æ ¹æ® [å¤§æ¨¡å‹çš„è¿è¡Œæ—¶æ¨ç†å’Œ KV Cache](/2024/11/16/The-LLMs-Runtime-Inference-and-KV-Cache/) ä¸­çš„ä»‹ç»ï¼Œå¦‚æœè¾“å…¥ prompt çš„é•¿åº¦ä¸º $n$ ä¸ª tokensâ€”â€”$s_1, s_2,...,s_n$ï¼Œå¤§æ¨¡å‹ä¼šæ ¹æ®è¿™ $n$ ä¸ª tokens ç”Ÿæˆäº†åç»­çš„ $k$ ä¸ª tokensâ€”â€”$s_{n+1}, s_{n+2}, ...,s_{n+k}$ã€‚

å¦‚æœæ²¡æœ‰ `KV ç¼“å­˜`ï¼Œåˆ™æ¯ç”Ÿæˆä¸€ä¸ªæ–°çš„ token $s_{n+k}$ï¼Œéƒ½éœ€è¦é‡æ–°è®¡ç®— $s_1,s_2,...,s_{n+k}$ ä¹‹é—´çš„æ³¨æ„åŠ›ã€‚

ä½†æ˜¯ï¼Œåœ¨ `KV ç¼“å­˜` æœºåˆ¶ä¸‹ï¼Œ`prefill é˜¶æ®µ` ä¼šè®¡ç®—è¾“å…¥åºåˆ—ä¸­çš„ $n$ ä¸ª tokens ä¹‹é—´çš„æ³¨æ„åŠ› $S_0=\{(k_i, v_i)\ |\ i \le n\}$ï¼Œå¹¶å°† $(k_i, v_i)$ ç¼“å­˜åœ¨ `KV ç¼“å­˜` ä¸­ã€‚å¯¹äºåç»­ç”Ÿæˆçš„æ¯ä¸€ä¸ª token $s_{n+j}(j\le k)$ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ `KV ç¼“å­˜` ä¸­çš„ $S_j=\{(k_i, v_i)\ | \ i \lt n + j \}$ æ¥è®¡ç®—æ–° token $s_{n+j}(j\le k)$ çš„è‡ªæ³¨æ„åŠ›ã€‚

ä»¤æ¨¡å‹çš„ç»´åº¦ä¸º $d_{model}$ï¼Œ$\mathbf{W}^Q$ã€$\mathbf{W}^K$ã€$\mathbf{W}^V$ çš„ç»´åº¦ä¸º $d_{head}$ï¼Œåˆ™åˆ©ç”¨ `KV ç¼“å­˜` å¯ä»¥å°†è®¡ç®—æ³¨æ„åŠ›çš„æµ®ç‚¹æ•°è®¡ç®—é‡ä» $6nd_{model}d_{head} + 4n^2d_{head}$ é™ä½åˆ° $6d_{model}d_{head} + 4nd_{model}$[^yelu_prompt_cache]ï¼Œæ•´ä½“çš„è®¡ç®—é‡é™ä½äº† $100(1 - \frac{1}{n})\%$ã€‚

## Prompt Cache
`KV ç¼“å­˜` å¯ä»¥æ˜¾è‘—é™ä½ä¸€ä¸ªè¯·æ±‚ä¸­çš„æ³¨æ„åŠ›çš„é‡å¤è®¡ç®—ã€‚ä½†æ˜¯ï¼Œå®é™…åº”ç”¨ä¸­ï¼Œä¸åŒçš„è¯·æ±‚ä¹Ÿç»å¸¸ä¼šæœ‰å¾ˆå¤šé‡å¤çš„å†…å®¹ï¼Œä¾‹å¦‚ï¼š

* æ ¹æ® Prompt æ¨¡ç‰ˆæ´¾ç”Ÿçš„ Prompt åœ¨è¯·æ±‚ LLM æ—¶é€šå¸¸åŒ…å«ï¼šç³»ç»Ÿæç¤ºã€è§’è‰²æŒ‡å®šã€æŒ‡ä»¤æè¿°ã€æ ·ä¾‹è¯´æ˜ã€è‡ªå®šä¹‰çš„åŠ¨æ€å†…å®¹â€¦â€¦
    
    $$
    \text{System Prompt} \ | \ \text{Role Setting} \ | \ \text{Instruction Description} \ | \ \text{Few Shot} \ | \ \text{User Query......}
    $$

  è¿™å…¶ä¸­ï¼Œç³»ç»Ÿæç¤ºè¯ã€è§’è‰²è®¾å®šã€æŒ‡ä»¤æè¿°ã€å…·ä½“ä¾‹å­ç­‰éƒ½æ˜¯é€šç”¨çš„æ¨¡ç‰ˆå†…å®¹ï¼ŒåŒä¸€ç±»å‹çš„ä»»åŠ¡ä»…åœ¨æœ€åçš„ç”¨æˆ·è‡ªå®šä¹‰éƒ¨åˆ†ä¸åŒã€‚
    
* Chat Robot ä¹‹é—´çš„å¤šè½®å¯¹è¯ï¼Œéšç€å¯¹è¯è½®æ¬¡çš„ä¸æ–­å¢åŠ ï¼Œæ–°çš„å¯¹è¯å†…å®¹ä¼šä½œä¸ºå¯¹è¯å†å²ä¸æ–­çš„å¢åŠ åˆ° Prompt ä¸­å¹¶é‡æ–°å‘é€ç»™å¤§ LLMï¼Œå½“å¯¹è¯å†å²è¾ƒå¤šçš„æ—¶å€™ï¼Œå°±ä¼šå¯¼è‡´å’Œå¤§æ¨¡å‹çš„æ¯æ¬¡å¯¹è¯éƒ½ä¼šæºå¸¦å¤§é‡çš„é‡å¤å†…å®¹ã€‚

* ä½¿ç”¨æ‰¹å¤„ç†æ–¹å¼è°ƒç”¨ LLM è§£å†³ç‰¹å®šçš„ä»»åŠ¡ï¼Œæ¯”å¦‚ä»¥ä½¿ç”¨å¤§æ¨¡å‹è¿›è¡Œå›¾ç‰‡åˆ†æï¼Œæ­¤æ—¶è¾“å…¥çš„ Prompt çš„å‰é¢éƒ¨åˆ†å†…å®¹éƒ½æ˜¯ä¸€è‡´çš„ï¼Œåªæ˜¯æœ€åè¦åˆ†æçš„å›¾ç‰‡å†…å®¹ä¸ä¸€è‡´ã€‚

å¦‚å‰æ‰€è¿°ï¼Œ`KV ç¼“å­˜` å¯ä»¥è§£å†³å•ä¸ªè¯·æ±‚å†…çš„è‡ªå›å½’è¿‡ç¨‹ä¸­çš„æ³¨æ„åŠ›æœºåˆ¶çš„é‡å¤è®¡ç®—é—®é¢˜ï¼Œæå‡ LLM çš„æ€§èƒ½ï¼Œä½†æ˜¯æ— æ³•è§£å†³å¦‚ä¸Šåœºæ™¯ä¸­å­˜åœ¨çš„è·¨è¯·æ±‚é—´çš„æ³¨æ„åŠ›æœºåˆ¶çš„é‡å¤è®¡ç®—é—®é¢˜ã€‚`Prompt Cache` å°±æ˜¯è§£å†³è·¨è¯·æ±‚é—´çš„æ³¨æ„åŠ›æœºåˆ¶é‡å¤è®¡ç®—çš„é—®é¢˜ï¼Œä»è€Œæå‡åç»­è¯·æ±‚çš„æ€§èƒ½ï¼Œå°¤å…¶æ˜¯æå‡ `prefill é˜¶æ®µ` çš„æ€§èƒ½ï¼ˆé™ä½ TTFTï¼‰ã€‚
* 2024 å¹´ 5 æœˆ 14 æ—¥ï¼Œè°·æ­Œä¸º Gemini æ¨¡å‹å®£å¸ƒäº†ä¸€ç³»åˆ—çš„æ–°ç‰¹æ€§ï¼Œå…¶ä¸­å°±åŒ…æ‹¬å¯ä»¥é™ä½è´¹ç”¨çš„ `Context Caching`ã€‚[^gemini_pc_intro]
* 2024 å¹´ 7 æœˆ 1 æ—¥ï¼Œæœˆä¹‹æš—é¢å®£å¸ƒå¯åŠ¨ `Context Cache` çš„å…¬æµ‹ï¼Œå¯¹äºè¯·æ±‚é¢‘ç¹ã€é‡å¤å¼•ç”¨å¤§é‡åˆå§‹ä¸Šä¸‹æ–‡çš„åœºæ™¯ï¼Œå¯ä»¥é™ä½ 83% çš„ TTFT åŒæ—¶è´¹ç”¨æœ€é«˜é™ä½ 90%ã€‚[^kimi_pc_intro]
* 2024 å¹´ 8 æœˆ 2 æ—¥ï¼ŒDeepSeek åœ¨ API æ¨¡å‹ä¸Šæ”¯æŒäº†ç¡¬ç›˜ `Prompt Cache`ï¼ŒæŠŠæœ¬æ¥å°±ç™½èœä»·çš„ API ä»·æ ¼åˆé™ä½äº†ä¸€ä¸ªæ•°é‡çº§[^deepseek_pc_intro]ï¼Œåœ¨å‘½ä¸­ç¼“å­˜çš„æƒ…å†µä¸‹ï¼Œæ¯ç™¾ä¸‡è¾“å…¥ tokens ä»… 0.014$ï¼Œå‡ ä¹è¦ç­‰åŒäºå…è´¹äº†ã€‚
* 2024 å¹´ 8 æœˆ 15 æ—¥ï¼ŒClaude ä¹Ÿå®£å¸ƒåœ¨ Claude 3.5 Sonnetã€Claude 3 Opusã€Claude 3 Haiku æ¨¡å‹ä¸Šæ”¯æŒ `Prompt Cache`ã€‚å¯¹äºè¾ƒé•¿çš„ promptsï¼Œä½¿ç”¨ `Prompt Cache` å¯ä»¥æŠŠè´¹ç”¨æœ€é«˜é™ä½ 90%ã€è¯·æ±‚å»¶è¿Ÿæœ€é«˜é™ä½ 85%ã€‚[^claude_pc_intro]
* 2024 å¹´ 10 æœˆ 1 æ—¥ï¼ŒOpenAI ä¹Ÿå®£å¸ƒåœ¨å…¶æœ€æ–°çš„æ¨¡å‹ç‰ˆæœ¬ GPT-4oã€GPT-4o miniã€o1-previewã€o1-mini ä¸Šæ”¯æŒ `Prompt Cache`ï¼Œä½¿ç”¨ `Prompt Cache` æŠ€æœ¯ï¼Œå¯ä»¥ä¸ºå¼€å‘è€…æä¾›æ›´å¿«çš„è¯·æ±‚å“åº”é€Ÿåº¦å’Œä½è‡³ 50% çš„è´¹ç”¨ã€‚[^openai_pc_intro]

## Prompt Cache æŠ€æœ¯æ–¹æ¡ˆ
`Prompt Cache` å¹¶ä¸æ˜¯ä¸€ç§ä¼ ç»Ÿçš„ç¼“å­˜æŠ€æœ¯ï¼Œä¼ ç»Ÿçš„ç¼“å­˜æŠ€æœ¯é€šå¸¸ç”¨äºä¸´æ—¶å­˜å‚¨æ•°æ®ä»¥ä¾¿äºå†æ¬¡è¯·æ±‚ç›¸åŒçš„æ•°æ®æ—¶å¯ä»¥åŠ é€Ÿæ•°æ®çš„è®¿é—®é€Ÿåº¦ã€‚ä¼ ç»Ÿçš„ç¼“å­˜æŠ€æœ¯é€šå¸¸ç”¨äºå¤„ç†é™æ€å†…å®¹è¾“å‡ºï¼Œä¾‹å¦‚ç½‘é¡µå†…å®¹ï¼Œjsã€cssã€image ç­‰é™æ€å†…å®¹ï¼Œæ•°æ®åº“æŸ¥è¯¢ç»“æœâ€¦â€¦

åœ¨ä¼ ç»Ÿçš„ç¼“å­˜æŠ€æœ¯ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ `key-value` çš„ç»“æ„æ¥ç»„ç»‡å¹¶è®¿é—®ç¼“å­˜å†…å®¹ï¼Œä¾‹å¦‚æˆ‘ä»¬å¯ä»¥æŠŠæŸä¸ª `url` ä½œä¸º `key`ã€å¹¶å°†å…¶å¯¹åº”çš„ç½‘é¡µå†…å®¹ä½œä¸º `value` ç¼“å­˜èµ·æ¥ï¼Œä»¥ä¾¿æœ‰å…¶ä»–ç”¨æˆ·å†æ¬¡è¯·æ±‚è¯¥ `url` æ—¶å¯ä»¥ç›´æ¥ä»ç¼“å­˜ä¸­æå–åˆ°è¯¥ç½‘é¡µçš„å†…å®¹ï¼Œä»è€ŒåŠ é€Ÿç½‘é¡µçš„è®¿é—®é€Ÿåº¦ã€‚

åœ¨å®é™…ä¸­ï¼Œç¡®å®å­˜åœ¨ä¸åŒç”¨æˆ·è¯·æ±‚çš„å†…å®¹å¤§è‡´ç›¸ä¼¼ï¼Œå› æ­¤å¯ä»¥åˆ©ç”¨ `key-value` çš„ä¼ ç»Ÿç¼“å­˜æŠ€æœ¯æ¥è§£å†³ä¸€éƒ¨åˆ†åœºæ™¯çš„è®¿é—®é€Ÿåº¦é—®é¢˜ï¼Œä¾‹å¦‚ï¼š
* Query 1ï¼šå¤©ç©ºä¸ºä»€ä¹ˆæ˜¯è“è‰²çš„ï¼Ÿ
* Query 2ï¼šä¸ºä»€ä¹ˆå¤©ç©ºæ˜¯è“è‰²çš„ï¼Ÿ
* Query 3ï¼šå¤©ç©ºä¸ºä»€ä¹ˆæ˜¯è“çš„ï¼Ÿ
* â€¦â€¦

ä½†æ˜¯ï¼Œå¯¹äº LLM è€Œè¨€ï¼Œåœ¨å¤§éƒ¨åˆ†åœºæ™¯ä¸‹ï¼ŒLLM æ ¹æ®å›ºå®šçš„ä¸Šä¸‹æ–‡å†…å®¹å’Œå¯å˜çš„ç”¨æˆ·è¾“å…¥å†…å®¹åŠ¨æ€ç”Ÿæˆè¾“å‡ºã€‚å› æ­¤ï¼Œåœ¨å¤§éƒ¨åˆ†åœºæ™¯ä¸‹ï¼Œä¼ ç»Ÿçš„ `key-value` ç¼“å­˜æŠ€æœ¯å¹¶ä¸é€‚ç”¨äºå¤§æ¨¡å‹é¢†åŸŸã€‚

æ ¹æ®å¦‚ä¸Šçš„è¿™ä¸¤ç§åœºæ™¯ï¼Œ`Prompt Cache` æŠ€æœ¯ä¹ŸåŸºæœ¬ä¸Šåˆ’åˆ†ä¸ºä¸¤ç§ï¼š
* è¾“å…¥æ•´ä½“ä¸€è‡´æ—¶ï¼Œåˆ©ç”¨ `key-value` çš„ä¼ ç»Ÿç¼“å­˜æŠ€æœ¯å¯¹ç›¸ä¼¼è¯·æ±‚çš„è¾“å‡ºå†…å®¹è¿›è¡Œç¼“å­˜ï¼Œä¾‹å¦‚  GPTCache: Semantic Cache for LLMs[^gpt_semantic_cache]ã€‚
* è¾“å…¥å‰ç¼€ä¸€è‡´æ—¶ï¼Œåˆ©ç”¨ `KV ç¼“å­˜` æŠ€æœ¯é¿å…æ³¨æ„åŠ›é‡å¤è®¡ç®—å¸¦æ¥çš„è®¡ç®—å¼€é”€ï¼Œä»è€Œæå‡å¤§æ¨¡å‹å“åº”é€Ÿåº¦ï¼Œä¾‹å¦‚è€¶é²å¤§å­¦å’Œè°·æ­Œç ”å‘çš„ Prompt Cache: Modular Attention Reuse æŠ€æœ¯[^yelu_prompt_cache]ã€‚

### GPTCache: Semantic Cache for LLMs
GPTCache: Semantic Cache for LLMs [^gpt_semantic_cache] æ˜¯ä¸€ç§å…¸å‹çš„ä¼ ç»Ÿç¼“å­˜æŠ€æœ¯ï¼ŒGPTCache çš„æœ¬è´¨æ˜¯æŠŠ `Query-Response` ç¼“å­˜åˆ° Redis ä¸­ï¼Œå½“æœ‰è¯­ä¹‰ä¸Šç›¸ä¼¼çš„ `Query` æ—¶ï¼Œç›´æ¥ä» Redis ä¸­è¿”å›ç¼“å­˜çš„ç»“æœè€Œä¸éœ€è¦ LLM å†é‡å¤è®¡ç®—å¹¶ç”Ÿæˆç»“æœï¼Œä»è€Œé™ä½å¤§æ¨¡å‹çš„æ¨ç†æˆæœ¬å¹¶æå‡å“åº”æ€§èƒ½ã€‚ä½†æ˜¯è¿™ç§æ–¹å¼åªé€‚ç”¨äºè¯­ä¹‰ä¸Šéå¸¸ç›¸ä¼¼çš„è¯·æ±‚ï¼Œå¯¹äºå¦‚ä¸‹çš„è¯·æ±‚åˆ™ä¸é€‚ç”¨ï¼š

* Query 1ï¼šå¤©ç©ºä¸ºä»€ä¹ˆæ˜¯è“è‰²çš„ï¼Ÿ
* Query 2ï¼šå¤©ç©ºä¸ºä»€ä¹ˆæ˜¯è“è‰²çš„ï¼Œè¯·ç»™å‡ºè¯¦ç»†çš„ç‰©ç†å­¦ä¸Šçš„è§£é‡Šï¼Ÿ

GPTCache çš„æ¶æ„å¦‚ä¸‹æ‰€ç¤º[^git_gptcache]ï¼š

![](17325377216331.jpg)

### Prompt Cache: Modular Attention Reuse
è€¶é²å¤§å­¦å’Œè°·æ­Œè”åˆå‘å¸ƒçš„ Prompt Cache: Modular Attention Reuse [^yelu_prompt_cache] åœ¨æœ¬è´¨ä¸Šæ—¶åŸºäº `KV ç¼“å­˜` çš„ `Prompt Cache` æŠ€æœ¯ï¼Œå°†è¯·æ±‚å†…çš„ `KV ç¼“å­˜` æŠ€æœ¯æ‰©å±•åˆ°äº†è¯·æ±‚é—´ï¼Œä»è€Œå¯ä»¥éå¸¸å¥½çš„è§£å†³å¦‚ä¸‹çš„åœºæ™¯ï¼š

* Query 1ï¼šå¤©ç©ºä¸ºä»€ä¹ˆæ˜¯è“è‰²çš„ï¼Ÿ
* Query 2ï¼šå¤©ç©ºä¸ºä»€ä¹ˆæ˜¯è“è‰²çš„ï¼Œè¯·ç»™å‡ºè¯¦ç»†çš„ç‰©ç†å­¦ä¸Šçš„è§£é‡Šï¼Ÿ

> æœ¬èŠ‚ä¸­æåˆ°çš„ `Prompt Cache` å‡æŒ‡ä»£è®ºæ–‡ *Prompt Cache: Modular Attention Reuse* ä¸­çš„ `Prompt Cache`ã€‚

`Prompt Cache` çš„åŸºæœ¬æ€æƒ³æ˜¯ï¼šåœ¨æ¨ç†é˜¶æ®µï¼Œä»¥æ–‡æœ¬ç‰‡æ®µï¼ˆtext segmentï¼‰ä¸ºå•ä½ï¼ŒæŠŠé¢‘ç¹å‡ºç°çš„æ–‡æœ¬æ®µå¯¹åº”çš„æ³¨æ„åŠ›çŠ¶æ€ï¼ˆç»“æœï¼‰å­˜å‚¨ä¸‹æ¥ï¼›åœ¨ä¹‹åè¯·æ±‚çš„æ¨ç†æ—¶ï¼Œå¦‚æœé‡åˆ°ç›¸åŒçš„æ–‡æœ¬æ®µï¼Œå°±ç›´æ¥ä½¿ç”¨ç¼“å­˜çš„æ³¨æ„åŠ›çŠ¶æ€ï¼ˆç»“æœï¼‰ã€‚

ä¸ºäº†å®ç°è¿™ä¸ªç›®æ ‡ï¼Œæœ‰ä¸¤ä¸ªæŒ‘æˆ˜éœ€è¦è§£å†³ï¼š
* Transformers æ¶æ„ä¸­å­˜åœ¨ä½ç½®ç¼–ç ï¼Œå› æ­¤ tokens ä¹‹é—´çš„è‡ªæ³¨æ„åŠ›çŠ¶æ€ï¼ˆç»“æœï¼‰ä¸ä½ç½®æœ‰å…³ã€‚æ‰€ä»¥ï¼Œåªæœ‰å½“ç›¸åŒçš„æ–‡æœ¬æ®µå‡ºç°åœ¨ç›¸åŒä½ç½®æ—¶ï¼Œæ‰èƒ½å¤ç”¨è¯¥æ–‡æœ¬æ®µçš„è‡ªæ³¨æ„åŠ›çŠ¶æ€ï¼ˆç»“æœï¼‰ã€‚
* å¿…é¡»å…·å¤‡æœ‰æ•ˆè¯†åˆ«å·²ç»ç¼“å­˜äº†è‡ªæ³¨æ„åŠ›çŠ¶æ€ï¼ˆç»“æœï¼‰çš„æ–‡æœ¬æ®µçš„èƒ½åŠ›ï¼Œè¿™æ ·ç³»ç»Ÿæ‰èƒ½å¤Ÿä½¿ç”¨è¿™äº›æ–‡æœ¬æ®µç¼“å­˜çš„è‡ªæ³¨æ„åŠ›çŠ¶æ€ï¼ˆç»“æœï¼‰ã€‚

ä¸ºäº†è§£å†³å¦‚ä¸Šçš„ä¸¤ä¸ªéš¾é¢˜ï¼Œ`Prompt Cache` é‡‡å–äº†å¦‚ä¸‹çš„æŠ€æœ¯æ–¹æ¡ˆå’Œæ¢ç´¢ï¼š
* è®ºæ–‡ä¸­æåˆ°ï¼Œä½œè€…é€šè¿‡å®éªŒè¯æ˜ï¼šè™½ç„¶è‡ªæ³¨æ„åŠ›æœºåˆ¶å’Œä½ç½®ç¼–ç æœ‰å…³ï¼Œä½†æ˜¯åªè¦ä¿æŒè¾“å…¥ tokens ä¹‹é—´çš„ç›¸å¯¹ä½ç½®ä¸å˜ï¼Œé‚£ä¹ˆå¤§è¯­è¨€æ¨¡å‹çš„è¾“å‡ºè´¨é‡å°±ä¸ä¼šå—åˆ°å½±å“ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬å¯ä»¥æå–è¾“å…¥ prompts çš„ä¸åŒç‰‡æ®µçš„æ³¨æ„åŠ›çŠ¶æ€ï¼ˆç»“æœï¼‰ï¼Œå¹¶å°†å®ƒä»¬æ‹¼æ¥åœ¨ä¸€èµ·ï¼Œä»è€Œæ„å»ºæ–°çš„è¯­ä¹‰ã€‚
* æå‡ºäº†ä¸€ç§æç¤ºè¯æ ‡è®°è¯­è¨€ (*PMLï¼šPrompt Markup Language*) ä»¥æ˜ç¡® prompt çš„ç»“æ„ã€‚PML  å°†å¯é‡ç”¨çš„æ–‡æœ¬æ®µè¡¨ç¤ºä¸ºæ¨¡å—ï¼Œä»è€Œè§£å†³äº†æœ‰æ•ˆè¯†åˆ«å¯å¤ç”¨çš„æ–‡æœ¬æ®µçš„é—®é¢˜ã€‚

ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬æœ‰ä¸€æ¡å¦‚ä¸‹çš„ promptï¼š

```
PROMPT = """
SystemPrompt
Context
Examples

Query: q?
Answer: 
"""
```

æ ¹æ®è®ºæ–‡ä¸­çš„ PML çš„å®šä¹‰ï¼Œæˆ‘ä»¬å¯ä»¥æ„å»ºå¦‚ä¸‹çš„ prompt schemaï¼š

```
<prompt schema="TaskPrompt">
  <SystemPrompt/>
  <Context/>
  <Examples/>
  Query: q? Answer: 
</prompt>
``` 

åœ¨æ¨ç†æ—¶ï¼Œ`Prompt Cache` çš„å¤„ç†æµç¨‹å¦‚ä¸‹ï¼š
* æ£€ç´¢ç¼“å­˜çš„æ³¨æ„åŠ›çŠ¶æ€ï¼ˆç»“æœï¼‰ï¼šä»ç¼“å­˜ä¸­è·å– SystemPrompt, Context, Examples çš„æ³¨æ„åŠ›çŠ¶æ€ï¼ˆç»“æœï¼‰ã€‚
* å¤„ç†æ–°æ–‡æœ¬ï¼šç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬éƒ½æœªç¼“å­˜ï¼Œéœ€è¦é‡æ–°è®¡ç®—æ³¨æ„åŠ›çŠ¶æ€ï¼ˆç»“æœï¼‰ï¼Œä¾‹å¦‚ä¸Šä¾‹ä¸­çš„ `Query: q? Answer: ` å°±æ˜¯æœªç¼“å­˜çš„éƒ¨åˆ†ã€‚
* åˆå¹¶æ³¨æ„åŠ›çŠ¶æ€ï¼ˆç»“æœï¼‰ï¼šæŒ‰é¡ºåºæ‹¼æ¥ SystemPrompt + Context + Examples + UserPrompt çš„æ³¨æ„åŠ›çŠ¶æ€ï¼ˆç»“æœï¼‰ã€‚
* ç”Ÿæˆå“åº”ï¼šä½¿ç”¨åˆå¹¶åçš„æ³¨æ„åŠ›çŠ¶æ€ï¼ˆç»“æœï¼‰æ¥ç”Ÿæˆ LLM çš„å“åº”ã€‚

æ›´è¯¦ç»†çš„ä¾‹å­å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![](17325398113275.jpg)

## OpenAI API ä¸­çš„ Prompt Caching
å¯¹äº OpenAI API ä¸­æä¾›çš„ `Prompt Caching` æŠ€æœ¯å¤§æ¦‚ç‡ä¹Ÿæ˜¯åŸºäº `KV ç¼“å­˜` æŠ€æœ¯æ¥é¿å…è·¨è¯·æ±‚é—´çš„æ³¨æ„åŠ›é‡å¤è®¡ç®—æ¥æå‡ `prefill é˜¶æ®µ` çš„æ€§èƒ½å¹¶å®ç°è´¹ç”¨é™ä½çš„ç›®çš„ã€‚OpenAI çš„ `Prompt Caching` è¦æ±‚å¿…é¡»æœ‰ç²¾å‡†åŒ¹é…çš„å‰ç¼€ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥æ¨æµ‹ï¼ŒOpenAI å¯èƒ½å¹¶æ²¡æœ‰ä½¿ç”¨è€¶é²å¤§å­¦çš„ `Prompt Cache` æŠ€æœ¯ï¼Œè€Œæ˜¯é‡‡ç”¨äº†ä¸€ç§æ›´ä¸ºç®€ä¾¿çš„æ–¹æ¡ˆã€‚

æ ¹æ® OpenAI å®˜ç½‘çš„ä»‹ç»ï¼ŒOpenAI ä¼šè‡ªåŠ¨ä¸º GPT-4oã€GPT-4o-miniã€o1-previewã€o1-mini æ¨¡å‹ä»¥åŠè¿™äº›æ¨¡å‹çš„å¾®è°ƒç‰ˆæœ¬å¼€å¯ `Prompt Caching` åŠŸèƒ½ã€‚å½“é€šè¿‡ API è°ƒç”¨å¯¹åº”çš„æ¨¡å‹ï¼Œå¹¶ä¸”è¾“å…¥ prompts çš„é•¿åº¦å¤§äº 1024 æ—¶ï¼ŒOpenAI ä¼šè‡ªåŠ¨ä¸ºè¿™äº›è¯·æ±‚å¼€å¯ `Prompt Caching` åŠŸèƒ½ï¼Œä»¥æå‡æ¨¡å‹çš„å“åº”é€Ÿåº¦ã€‚

ä¹‹æ‰€ä»¥é‡‡ç”¨ 1024 ä¸ª tokens çš„é˜ˆå€¼ï¼Œå¯èƒ½æ˜¯ä¸€ä¸ªæƒè¡¡çš„ç»“æœï¼Œæ¯•ç«Ÿ `Prompt Caching` åœ¨æœ¬è´¨ä¸Šæ—¶ä¸€ç§ **é€šè¿‡ç©ºé—´æ¢æ—¶é—´** çš„æŠ€æœ¯ï¼Œè™½ç„¶å¯ä»¥æå‡è¯·æ±‚çš„å“åº”é€Ÿåº¦ï¼Œä½†æ˜¯ä¹Ÿä¼šå¯¼è‡´ GPU å†…å­˜çš„è¿‡æ¸¡æ¶ˆè€—ï¼Œä»è€Œå½±å“åˆ°å•ä¸ª GPU çš„ååã€‚å› æ­¤ï¼Œå½“è¯·æ±‚ä¸­çš„ prompts é•¿åº¦æ¯”è¾ƒå°æ—¶ï¼Œ`Prompt Caching` çš„ ROI å°±æ¯”è¾ƒä½äº†ã€‚

SARATHI æå‡ºçš„ `chunked-prefills` æŠ€æœ¯å¯ä»¥å…è®¸æˆ‘ä»¬å°†ä¸€ä¸ª `prefill é˜¶æ®µ` çš„è¯·æ±‚æ‹†åˆ†ä¸ºå¤§å°ç›¸ç­‰çš„å—ï¼ŒåŒæ—¶åˆ©ç”¨æŠŠä¸€ä¸ª `prefill-chunk` å’Œå¤šä¸ª `decode` æ„å»ºæˆä¸€ä¸ªæ‰¹å¤„ç†è¯·æ±‚çš„è¿™ç§æ­ä¾¿è½¦ï¼ˆ*piggyback*ï¼‰çš„æŠ€æœ¯æ¥å®ç°è§£ç æœ€å¤§åŒ–æ‰¹å¤„ç†ï¼ˆdecode-maximal batchingï¼‰ï¼Œè¿›è€Œé¿å…äº†æ¨¡å‹åœ¨æµæ°´çº¿å¹¶è¡Œä¸­å­˜åœ¨çš„æµæ°´çº¿ç­‰å¾…é—®é¢˜ï¼Œä»è€Œæé«˜å¤§æ¨¡å‹çš„æ¨ç†æ€§èƒ½ã€‚[^SARATHI]

![](17325284975396.jpg)

OpenAI å¤§æ¦‚ç‡ä¹Ÿé‡‡ç”¨äº†ç±»ä¼¼ SARATHI ä¸­çš„ `chunked-prefills` æŠ€æœ¯ï¼Œå› æ­¤å¯ä»¥å®ç°ç¼“å­˜ä¸åŒè¯·æ±‚é—´çš„æœ€é•¿å‰ç¼€æ¥è¾¾åˆ°åŠ é€Ÿè®¡ç®—çš„æ•ˆæœã€‚OpenAI åœ¨å¯¹è¾“å…¥ prompts è¿›è¡Œåˆ†å—æ—¶ï¼Œé‡‡ç”¨çš„åˆ†å—å¤§å°ä¸º 128 tokensï¼Œä»¥å®ç°æœ€å¤§ç¨‹åº¦çš„å¯æ§æ€§ã€‚

æ ¹æ® OpenAI çš„ä»‹ç»[^openai_pc_intro]ï¼Œå½“ç”¨æˆ·è¾“å…¥ prompts çš„é•¿åº¦è¶…è¿‡ 1024 ä¸ª tokens æ—¶ï¼š
* API é¦–å…ˆä¼šè®¡ç®—è¯¥è¯·æ±‚çš„æœ€é•¿å‰ç¼€ï¼Œå¹¶å°†è¯¥æœ€é•¿å‰ç¼€çš„ç»“æœç¼“å­˜èµ·æ¥ï¼ˆ`KV ç¼“å­˜`ï¼‰ï¼Œè¯¥æœ€é•¿å‰ç¼€çš„é•¿åº¦ä» 1024 å¼€å§‹ï¼Œå¹¶ä¸”ä»¥ 128 çš„å¤§å°é€æ­¥é€’å¢ã€‚
* å½“è¯¥ç”¨æˆ·çš„ä¸‹ä¸€ä¸ªè¯·æ±‚åŒ…å«è¯¥æœ€é•¿å‰ç¼€æ—¶ï¼ŒAPI åˆ™å¯ä»¥ç›´æ¥å¤ç”¨æœ€é•¿å‰ç¼€çš„æ³¨æ„åŠ›ç»“æœã€å¹¶ä¸”ä¾èµ–ç¼“å­˜ç»“æœä»…è®¡ç®—æœ€é•¿å‰ç¼€ä¹‹åçš„ tokens çš„æ³¨æ„åŠ›ç»“æœå®ç°è¯·æ±‚çš„åŠ é€Ÿã€‚

åŸºäº Transformer æ¶æ„çš„å¤§æ¨¡å‹åˆ©ç”¨ **å› æœè®¡ç®—**ï¼ˆ*causal computation*ï¼‰æ¥è®¡ç®—è‡ªæ³¨æ„åŠ›ï¼Œæ‰€ä»¥åœ¨ä½¿ç”¨ `Prompt Caching` æ—¶å¿…é¡»ä¿è¯è¯·æ±‚é—´å­˜åœ¨ä¸€è‡´çš„å‰ç¼€ã€‚å› æ­¤ï¼ŒOpenAI çš„å®˜æ–¹æ–‡æ¡£ä¸­è¯´ï¼šåªæœ‰å½“è¯·æ±‚é—´å­˜åœ¨å®Œå…¨ä¸€è‡´çš„å‰ç¼€æ—¶ï¼Œæ‰ä¼šå‘½ä¸­ `Prompt Caching`[^pc_docs]ã€‚

> Cache hits are only possible for exact prefix matches within a prompt. 
> 
> To realize caching benefits, place static content like instructions and examples at the beginning of your prompt, and put variable content, such as user-specific information, at the end.
> ![](17325296211790.jpg)

æ ¹æ® OpenAI çš„å®˜æ–¹æ–‡æ¡£ï¼Œ`Prompt Caching` çš„æµç¨‹å¦‚ä¸‹[^pc_docs]ï¼š

* **Cache Lookup**ï¼šæ£€æŸ¥è¾“å…¥ prompts çš„å‰ç¼€æ˜¯å¦ä½äºç¼“å­˜
* **Cache Hit**ï¼šå¦‚æœæ‰¾åˆ°åŒ¹é…çš„å‰ç¼€ï¼Œåˆ™ä½¿ç”¨ç¼“å­˜ç»“æœ
* **Cache Miss**ï¼šå¦åˆ™ï¼ŒæŒ‰ç…§å®Œæˆçš„å¤„ç†æ–¹å¼ä»å¤´å¤„ç†è¯¥è¯·æ±‚ï¼Œå¹¶å°†å¯¹åº”çš„ç»“æœç¼“å­˜èµ·æ¥ä»¥å¤‡å°†æ¥ä½¿ç”¨

ä¸ºäº†é¿å…æ— æ•ˆçš„ç¼“å­˜å ç”¨ï¼Œå¯¹äºç¼“å­˜çš„æœ€é•¿å‰ç¼€è®¡ç®—ç»“æœï¼Œå¦‚æœæ²¡æœ‰åœ¨ 5~10 åˆ†é’Ÿå†…å†æ¬¡è®¿é—®è¯¥ç¼“å­˜ç»“æœï¼ŒOpenAI å°†æ¸…ç©ºè¯¥ç¼“å­˜ã€‚åœ¨éé«˜å³°æ—¶æ®µï¼Œç¼“å­˜å¯ä»¥ä¿å­˜é•¿è¾¾ 1 å°æ—¶ã€‚

`Prompt Caching` å¯ç”¨äº 1024 ä¸ªæˆ–æ›´å¤š tokens çš„ promptsï¼Œå¹¶ä¸”ç¼“å­˜ä»¥ 128 ä¸ª token ä¸æ–­å¢åŠ ï¼Œä¹Ÿå°±æ˜¯è¯´è¯·æ±‚ä¸­ç¼“å­˜çš„ tokens æ•°é‡å°†å§‹ç»ˆä¸ºï¼š1024ã€1152ã€1280ã€1408â€¦â€¦

å¯ä»¥é€šè¿‡ API å“åº”ä¸­çš„ `usage.prompt_tokens_details.cached_tokens` å­—æ®µæ¥æŸ¥çœ‹è¯·æ±‚æ˜¯å¦å‘½ä¸­ `Prompt Caching` ä»¥åŠå‘½ä¸­ `Prompt Caching` çš„ tokens æ•°é‡ã€‚

```
"usage": {
  "prompt_tokens": 2006,
  "completion_tokens": 300,
  "total_tokens": 2306,
  "prompt_tokens_details": {
    "cached_tokens": 1920
  },
  "completion_tokens_details": {
    "reasoning_tokens": 0,
    "accepted_prediction_tokens": 0,
    "rejected_prediction_tokens": 0
  }
}
```

## ä¸¾ä¸ªä¾‹å­ğŸŒ°
æˆ‘æœ‰ä¸€ä¸ªå›¾ç‰‡å†…å®¹åˆ†æçš„æœåŠ¡ï¼Œè¯¥æœåŠ¡å…è®¸ç”¨æˆ·è¾“å…¥ä¸€å¼ å›¾ç‰‡å’Œä¸€æ®µæ–‡æœ¬æè¿°ï¼Œç„¶ååˆ¤æ–­å›¾ç‰‡å†…å®¹å’Œæ–‡æœ¬æè¿°ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œå¹¶æ ¹æ®ç›¸å…³æ€§ç»™äºä¸€ä¸ªæ‰“åˆ†ã€‚

ç®€ä¾¿æœŸé—´ï¼Œæˆ‘ä½¿ç”¨ GPT-4o æ¨¡å‹æ¥å®ç°è¿™ä¸ªåŠŸèƒ½ï¼Œæ•´ä½“çš„ prompts å¾—æ ¼å¼å¦‚ä¸‹æ‰€ç¤ºï¼š

![](pic_prompt.png)

ä¸€èˆ¬è€Œè¨€ï¼Œæˆ‘ä¼šä¸€æ¬¡åˆ†æå‡ ç™¾å¼ å›¾ç‰‡ï¼Œå¹¶ä¸”ç”¨äºå›¾ç‰‡åˆ†æçš„ prompts çš„é•¿åº¦éå¸¸å¤§ï¼Œå¹³å‡é•¿åº¦åœ¨ 1200 ä¸ª tokens å·¦å³ã€‚è€Œå…¶ä¸­çš„è§’è‰²è®¾å®šã€åˆ†æè§„åˆ™ã€æ‰“åˆ†è§„åˆ™çš„æè¿°éƒ¨åˆ†åœ¨ 1000 ä¸ª tokens å·¦å³ï¼Œå¹¶ä¸”æ˜¯å›ºå®šä¸å˜çš„ï¼Œå˜åŒ–çš„å›¾ç‰‡å’Œæ–‡æœ¬miaonæè¿°éƒ¨åˆ†åœ¨ 200 ä¸ª tokens å·¦å³ï¼ˆå…¶ä¸­å›¾ç‰‡å›ºå®š 85 ä¸ª tokensï¼‰ã€‚æ‰€ä»¥åœ¨æ²¡æœ‰ `Prompt Caching` ä¹‹å‰ï¼Œæ¯æ¬¡è¯·æ±‚éƒ½ä¼šé‡å¤è®¡ç®— 1000 ä¸ª tokensï¼ˆå æ¯” 80%ï¼‰ï¼Œä»è€Œé€ æˆå¤§é‡çš„é‡å¤è®¡ç®—ï¼Œè¿›è€Œå¸¦æ¥äº†æ›´å¤šçš„è´¹ç”¨ã€‚

åœ¨ `Prompt Caching` ä¹‹åï¼Œæˆ‘æ¯æ¬¡è¯·æ±‚åªéœ€è¦è®¡ç®— 200 ä¸ª tokensï¼ˆå æ¯” 20%ï¼‰ï¼Œä»è€ŒèŠ‚çœäº† 80% çš„è®¡ç®—é‡å’Œè´¹ç”¨ï¼Œå…·ä½“çš„ GPT-4o çš„ API å“åº”å¦‚ä¸‹æ‰€ç¤ºï¼š

```
"usage": {
  "prompt_tokens": 1135, 
  "completion_tokens": 284, 
  "total_tokens": 1419, 
  "prompt_tokens_details": {
    "cached_tokens": 1024
  }, 
  "completion_tokens_details": {
    "reasoning_tokens": 0
  }
}
```

ä»ä¸Šè¿°çš„ç»“æœå¯ä»¥å‘ç°ï¼Œä½¿ç”¨ `Prompt Caching` ä¹‹åï¼Œè¯¥è¯·æ±‚æœ‰ 1024 ä¸ª tokensï¼ˆå æ¯” 90%ï¼‰å‘½ä¸­äº†ç¼“å­˜ï¼Œä»è€ŒèŠ‚çœäº† 90% çš„è´¹ç”¨ã€‚

## å‚è€ƒæ–‡çŒ®
[^yelu_prompt_cache]: [Prompt Cache: Modular Attention Reuse For Low-Latency Inference](https://arxiv.org/abs/2311.04934v2)
[^gemini_pc_intro]: [Context caching for Google Gemini](https://simonwillison.net/2024/May/14/context-caching-for-google-gemini/)
[^kimi_pc_intro]: [Context Caching æ­£å¼å…¬æµ‹](https://platform.moonshot.cn/blog/posts/context-caching)
[^deepseek_pc_intro]: [DeepSeek API introduces Context Caching on Disk, cutting prices by an order of magnitude](https://api-docs.deepseek.com/news/news0802)
[^claude_pc_intro]: [Prompt caching with Claude](https://www.anthropic.com/news/prompt-caching)
[^openai_pc_intro]: [Prompt Caching in the API](https://openai.com/index/api-prompt-caching/)
[^gpt_semantic_cache]: [GPT Semantic Cache: Reducing LLM Costs and Latency via Semantic Embedding Caching](https://arxiv.org/abs/2411.05276v1)
[^SARATHI]: [SARATHI: Efficient LLM Inference by Piggybacking Decodes with Chunked Prefills](https://arxiv.org/abs/2308.16369)
[^pc_docs]: [Prompt caching: Reduce latency and cost with Prompt Caching](https://platform.openai.com/docs/guides/prompt-caching)
[^git_gptcache]: [GPTCache : A Library for Creating Semantic Cache for LLM Queries](https://github.com/zilliztech/GPTCache)