
<!DOCTYPE HTML>
<html lang="" >
    <head>
        <meta charset="UTF-8">
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <title>10.4 Testing out software running on Kubernetes · Chaos Engineering</title>
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="description" content="">
        <meta name="generator" content="GitBook 3.2.3">
        <meta name="author" content="wangwei17">
        
        
    
    <link rel="stylesheet" href="../gitbook/style.css">

    
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-page-toc-button/plugin.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-hints/plugin-hints.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-highlight/website.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-search/search.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-fontsettings/website.css">
                
            
        

    

    
        
    
        
    
        
    
        
    
        
    
        
    

        
    
    
    <meta name="HandheldFriendly" content="true"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <link rel="apple-touch-icon-precomposed" sizes="152x152" href="../gitbook/images/apple-touch-icon-precomposed-152.png">
    <link rel="shortcut icon" href="../gitbook/images/favicon.ico" type="image/x-icon">

    
    <link rel="next" href="Summary_10.html" />
    
    
    <link rel="prev" href="Setting_up_a_Kubernetes_cluster.html" />
    

    </head>
    <body>
        
<div class="book">
    <div class="book-summary">
        
            
<div id="book-search-input" role="search">
    <input type="text" placeholder="Type to search" />
</div>

            
                <nav role="navigation">
                


<ul class="summary">
    
    
    
        
        <li>
            <a href="https://wangwei1237.gitee.io/" target="_blank" class="custom-link">17哥的个人网站</a>
        </li>
    
    

    
    <li class="divider"></li>
    

    
        
        
    
        <li class="chapter " data-level="1.1" data-path="../">
            
                <a href="../">
            
                    
                    Introduction
            
                </a>
            

            
        </li>
    

    
        
        <li class="header">PART 3: Chaos Engineering beyond machines</li>
        
        
    
        <li class="chapter " data-level="2.1" data-path="Chaos_in_Kubernetes.html">
            
                <a href="Chaos_in_Kubernetes.html">
            
                    
                    10 Chaos in Kubernetes
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="2.1.1" data-path="Porting_things_onto_Kubernetes.html">
            
                <a href="Porting_things_onto_Kubernetes.html">
            
                    
                    10.1 Porting things onto Kubernetes
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="2.1.2" data-path="Whats_Kubernetes_in_7_minutes.html">
            
                <a href="Whats_Kubernetes_in_7_minutes.html">
            
                    
                    10.2 What’s Kubernetes (in 7 minutes)
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="2.1.3" data-path="Setting_up_a_Kubernetes_cluster.html">
            
                <a href="Setting_up_a_Kubernetes_cluster.html">
            
                    
                    10.3 Setting up a Kubernetes cluster
            
                </a>
            

            
        </li>
    
        <li class="chapter active" data-level="2.1.4" data-path="Testing_out_software_running_on_Kubernetes.html">
            
                <a href="Testing_out_software_running_on_Kubernetes.html">
            
                    
                    10.4 Testing out software running on Kubernetes
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="2.1.5" data-path="Summary_10.html">
            
                <a href="Summary_10.html">
            
                    
                    10.5 Summary
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="2.2" data-path="Automating_Kubernetes_experiments.html">
            
                <a href="Automating_Kubernetes_experiments.html">
            
                    
                    11 Automating Kubernetes experiments
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="2.2.1" data-path="Automating_chaos_with_PowerfulSeal.html">
            
                <a href="Automating_chaos_with_PowerfulSeal.html">
            
                    
                    11.1 Automating chaos with PowerfulSeal
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="2.2.2" data-path="Ongoing_testing_Service_Level_Objectives_SLOs.html">
            
                <a href="Ongoing_testing_Service_Level_Objectives_SLOs.html">
            
                    
                    11.2 Ongoing testing & Service Level Objectives (SLOs)
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="2.2.3" data-path="Cloud_layer.html">
            
                <a href="Cloud_layer.html">
            
                    
                    11.3 Cloud layer
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="2.2.4" data-path="Summary_11.html">
            
                <a href="Summary_11.html">
            
                    
                    11.4 Summary
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="2.3" data-path="Under_the_hood_of_Kubernetes.html">
            
                <a href="Under_the_hood_of_Kubernetes.html">
            
                    
                    12 Under the hood of Kubernetes
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="2.3.1" data-path="Anatomy_of_a_Kubernetes_cluster_and_how_to_break_it.html">
            
                <a href="Anatomy_of_a_Kubernetes_cluster_and_how_to_break_it.html">
            
                    
                    12.1 Anatomy of a Kubernetes cluster and how to break it
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="2.3.2" data-path="Summary_of_key_components.html">
            
                <a href="Summary_of_key_components.html">
            
                    
                    12.2 Summary of key components
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="2.3.3" data-path="Summary_12.html">
            
                <a href="Summary_12.html">
            
                    
                    12.3 Summary
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="2.4" data-path="Chaos_engineering_for_people.html">
            
                <a href="Chaos_engineering_for_people.html">
            
                    
                    13 Chaos engineering (for) people
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="2.4.1" data-path="Chaos_engineering_mindset.html">
            
                <a href="Chaos_engineering_mindset.html">
            
                    
                    13.1 Chaos engineering mindset
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="2.4.2" data-path="Getting_the_buy_in.html">
            
                <a href="Getting_the_buy_in.html">
            
                    
                    13.2 Getting the buy-in
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="2.4.3" data-path="Teams_as_distributed_systems.html">
            
                <a href="Teams_as_distributed_systems.html">
            
                    
                    13.3 Teams as distributed systems
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="2.4.4" data-path="Summary_13.html">
            
                <a href="Summary_13.html">
            
                    
                    13.4 Summary
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="2.4.5" data-path="Where_to_go_from_here.html">
            
                <a href="Where_to_go_from_here.html">
            
                    
                    13.5 Where to go from here
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    

    

    <li class="divider"></li>

    <li>
        <a href="https://www.gitbook.com" target="blank" class="gitbook-link">
            Published with GitBook
        </a>
    </li>
</ul>


                </nav>
            
        
    </div>

    <div class="book-body">
        
            <div class="body-inner">
                
                    

<div class="book-header" role="navigation">
    

    <!-- Title -->
    <h1>
        <i class="fa fa-circle-o-notch fa-spin"></i>
        <a href=".." >10.4 Testing out software running on Kubernetes</a>
    </h1>
</div>




                    <div class="page-wrapper" tabindex="-1" role="main">
                        <div class="page-inner">
                            
<div id="book-search-results">
    <div class="search-noresults">
    
                                <section class="normal markdown-section">
                                
                                <h1 id="104-testing-out-software-running-on-kubernetes">10.4 Testing out software running on Kubernetes</h1>
<p>With a functional Kubernetes cluster at our disposal, we&#x2019;re now ready to start working on the High Profile Project, aka ICANT. The pressure is on, we have a project to save!</p>
<p>As always, the first step is to build an understanding of how things work before we can reason about how they break. We&#x2019;ll do that by kicking the tires and looking how ICANT is deployed and configured. Once we&#x2019;re done with that, we&#x2019;ll conduct two experiments and then finish this section by seeing how to make things easier for ourselves for the next time. Let&#x2019;s start at the beginning - by running the actual project</p>
<h2 id="1041-running-the-icant-project">10.4.1 Running the ICANT Project</h2>
<p>As we discovered earlier when reading the documentation you inherited, the project didn&#x2019;t get very far. They took an off-the-shelf component (Goldpinger), deployed it, and called it a day. All of which is bad news for the project, but good news to me; I have less explaining to do!</p>
<p>Goldpinger works by querying Kubernetes for all the instances of itself, and then periodically calling each of these instances and measuring the response time. It then uses that data to generate statistics (metrics) and plot a pretty connectivity graph. Each instance works in the same way - it periodically gets the address of its peers, and makes a request to each one of them. This is illustrated in figure 10.4. Goldpinger was invented to detect network slow-downs and problems, especially in larger clusters. It&#x2019;s really simple and very effective.</p>
<p>Figure 10.4 Overview of how Goldpinger works</p>
<p><img src="../images/10.4.jpg" alt=""></p>
<p>How do we go about running it? We&#x2019;ll do it in two steps:</p>
<ol>
<li>Set up the right permissions, so that Goldpinger can query Kubernetes for its peer</li>
<li>Deploy it on the cluster</li>
</ol>
<p>We&#x2019;re about to step into Kubernetes Wonderland, so let me introduce you to some Kubernetes lingo.</p>
<h3 id="kubernetes-terminology">Kubernetes terminology</h3>
<p>The documentation often mentions resources to mean the objects representing various abstractions that Kubernetes offers. For now, I&#x2019;m going to introduce you to three basic building blocks used to describe software on Kubernetes:</p>
<ul>
<li>Pod. A pod is a collection of containers that are grouped together, run on the same host and share some system resources, for example an IP address. This is the unit of software that you can schedule on Kubernetes. You can schedule pods directly, but most of the time you will be using a higher level abstraction, such as a Deployment.</li>
<li>Deployment. A deployment describes a blueprint for creating pods, along with extra metadata, like for example the number of replicas to run. Importantly, it also manages the lifecycle of pods that it creates. For example, if you modify a deployment to update a version of the image you want to run, the deployment can handle a rollout, deleting old pods and creating new ones one by one to avoid an outage. It also offers other things, like roll-back, if the roll out ever fails.</li>
<li>Service. A service matches an arbitrary set of pods, and provides a single IP address that resolves to the matched pods. That IP is kept up to date with the changes made to the cluster. For example, if a pod goes down, it will be taken out of the pool.</li>
</ul>
<p>You can see a visual representation of how these fit together in figure 10.5.</p>
<p>Figure 10.5 Pods, deployments and services example in Kubernetes</p>
<p><img src="../images/10.5.jpg" alt=""></p>
<p>Another thing you need to know to understand how Goldpinger works is that to query Kubernetes, you need to have the right permissions.</p>
<p><div class="alert alert-info hints-alert"><div class="hints-icon"><i class="fa fa-info-circle"></i></div><div class="hints-container"><p><strong>NOTE POP QUIZ: WHAT&#x2019;S A KUBERNETES DEPLOYMENT?</strong></p>
<p>Pick one:</p>
<ol>
<li>A description of how to reach software running on your cluster</li>
<li>A description of how to deploy some software on your cluster</li>
<li>A description of how to build a container</li>
</ol>
<p>See appendix B for answers.</p>
</div></div></p>
<h3 id="permissions">Permissions</h3>
<p>Kubernetes has an elegant way of managing permissions. First, it has a concept of a ClusterRole, that allows you to define a role and a corresponding set of permissions to execute verbs (create, get, delete, list, &#x2026;) on various resources. Second, it has the concept of ServiceAccounts, which can be linked to any software running on Kubernetes, so that it inherits all the permissions that the ServiceAccount was granted. And finally, to make a link between a ServiceAccount and a ClusterRole, you can use a ClusterRoleBinding, which does exactly what it says on the tin.</p>
<p>If you&#x2019;re new to it, this permissioning might sound a little bit abstract, so take a look at figure 10.6 for a graphical representation of how all of this comes together.</p>
<p>Figure 10.6 Kubernetes permissioning example</p>
<p><img src="../images/10.6.jpg" alt=""></p>
<p>In our case, we want to allow Goldpinger pods to list its peers, so all we need is a single ClusterRole, and the corresponding ServiceAccount and ClusterRoleBinding. Later, we will use that ServiceAccount to permission the Goldpinger pods.</p>
<h3 id="creating-the-resources">Creating the resources</h3>
<p>Time for some code! In Kubernetes, we can describe all resources we want to create using a Yaml file (.yml; <a href="https://yaml.org/" target="_blank">https://yaml.org/</a>) that follows the specific format that Kubernetes accepts. See listing 10.1 to see how all of this permissioning translates into .yml. For each element we described, there is a Yaml object, specifying the corresponding type (kind) and the expected parameters. First, a ClusterRole called <code>goldpinger-clusterrol</code> that allows for listing pods (bold font). Then a ServiceAccount called <code>goldpinger-serviceaccount</code> (bold font). And finally, a ClusterRoleBinding, linking the ClusterRole to the ServiceAccount. If you&#x2019;re new to Yaml, note that the <code>---</code> separators allow for describing multiple resources in a single file.</p>
<pre><code class="lang-yaml">Listing <span class="hljs-number">10.1</span> goldpinger-rbac.yaml
<span class="hljs-meta">---</span>
<span class="hljs-attr">apiVersion:</span> rbac.authorization.k8s.io/v1
<span class="hljs-attr">kind:</span> ClusterRole                              <span class="hljs-comment">#A</span>
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> goldpinger-clusterrole
<span class="hljs-attr">rules:</span>
<span class="hljs-attr">- apiGroups:</span>
<span class="hljs-bullet">  -</span> <span class="hljs-string">&quot;&quot;</span>
<span class="hljs-attr">  resources:</span>
<span class="hljs-bullet">  -</span> pods                                       <span class="hljs-comment">#B</span>
<span class="hljs-attr">  verbs:</span>
<span class="hljs-bullet">  -</span> list                                       <span class="hljs-comment">#C</span>
<span class="hljs-meta">---</span>
<span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> ServiceAccount                           <span class="hljs-comment">#D</span>
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> goldpinger-serviceaccount
<span class="hljs-attr">  namespace:</span> default
<span class="hljs-meta">---</span>
<span class="hljs-attr">apiVersion:</span> rbac.authorization.k8s.io/v1beta1
<span class="hljs-attr">kind:</span> ClusterRoleBinding
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> goldpinger-clusterrolebinding
<span class="hljs-attr">roleRef:</span>
<span class="hljs-attr">  apiGroup:</span> rbac.authorization.k8s.io
<span class="hljs-attr">  kind:</span> ClusterRole
<span class="hljs-attr">  name:</span> goldpinger-clusterrole                 <span class="hljs-comment">#E</span>
<span class="hljs-attr">subjects:</span>
<span class="hljs-attr">  - kind:</span> ServiceAccount
<span class="hljs-attr">    name:</span> goldpinger-serviceaccount            <span class="hljs-comment">#F</span>
<span class="hljs-attr">    namespace:</span> default
</code></pre>
<p>#A we start with a cluster role</p>
<p>#B the cluster role gets permissions for resource of type pod</p>
<p>#C the cluster role gets permissions to list the resource of type pod</p>
<p>#D we create a service account to use later</p>
<p>#E we create a cluster role binding, that binds the cluster role...</p>
<p>#F &#x2026; to the service account</p>
<p>This takes care of the permissionsing part. Let&#x2019;s now go ahead and see what deploying the actual Goldpinger looks like.</p>
<h3 id="goldpinger-yml-files">Goldpinger .yml files</h3>
<p>To make sense of deploying Goldpinger, I need to explain one more detail that I skipped over so far: matching and labels.</p>
<p>Kubernetes makes extensive use of labels, which are simple key-value pairs of type string. Every resource can have arbitrary metadata attached to it, including labels. They are used by Kubernetes to match sets of resources, and are fairly flexible and easy to use.</p>
<p>For example, let&#x2019;s say that you have two pods, with the following labels:</p>
<ol>
<li>Pod A, with labels <code>app=goldpinger</code> and <code>stage=dev</code></li>
<li>Pod B, with labels <code>app=goldpinger</code> and <code>stage=prod</code></li>
</ol>
<p>If you match (select) all pods with label <code>app=goldpinger</code>, you will get both pods. But if you match with label <code>stage=dev</code>, you will only get pod A. You can also query by multiple labels, and in that case Kubernetes will return pods matching all requested labels (a logical AND).</p>
<p>Labels are useful for manually grouping resources, but they&#x2019;re also leveraged by Kubernetes, for example to implement deployments. When you create a deployment, you need to specify the selector (a set of labels to match), and that selector needs to match the pods created by the deployment. The connection between the deployment and the pods it manages relies on labels.</p>
<p>Label-matching is also the same mechanism that Goldpinger leverages to query for its peers: it just asks Kubernetes for all pods with a specific label (by default <code>app=goldpinger</code>). Figure 10.7 shows that graphically.</p>
<p>Figure 10.7 Kubernetes permissioning example</p>
<p><img src="../images/10.7.jpg" alt=""></p>
<p>Putting this all together, we can finally write a .yml file with two resource descriptors: a deployment and a matching service.</p>
<p>Inside the deployment, we need to specify the following:</p>
<ul>
<li>The number of replicas (we&#x2019;ll go with three for demonstration purposes)</li>
<li>The selector (again the default app=goldpinger),</li>
<li>The actual template of pods to create</li>
</ul>
<p>In the pod template, we will specify the container image to run, some environment values required for Goldpinger to work and ports to expose so that other instances can reach it. The important bit is that we need to specify some arbitrary port that matches the <code>PORT</code> environment variable (this is what Goldpinger uses to know what port to listen on). We&#x2019;ll go with 8080. Finally, we also specify the service account we created earlier on, to permission the Goldpinger pods to query Kubernetes for their peers.</p>
<p>Inside the service, we once again use the same selector (<code>app=goldpinger</code>), so that the service matches the pods created by the deployment, and the same port 8080 that we specified on the deployment.</p>
<p><div class="alert alert-info hints-alert"><div class="hints-icon"><i class="fa fa-info-circle"></i></div><div class="hints-container"><p><strong>NOTE DEPLOYMENTS AND DAEMONSETS</strong></p>
<p>In a typical installation, we would like to have one Goldpinger pod per node (physical machine, VM) in your cluster. That can be easily achieved by using a DaemonSet (it works a lot like a deployment, but instead of specifying the number of replicas, it just assumes one replica per node - learn more at <a href="https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/" target="_blank">https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/</a>). In our example setup we will use a Deployment instead, because with only one node, we would only have a single pod of Goldpinger, which defeats the purpose of this demonstration.</p>
</div></div></p>
<p>Listing 10.2 contains the .yml file we can use to create the deployment and the service. Take a look.</p>
<pre><code class="lang-yaml">Listing <span class="hljs-number">10.2</span> goldpinger.yml .
<span class="hljs-meta">---</span>
<span class="hljs-attr">apiVersion:</span> apps/v1
<span class="hljs-attr">kind:</span> Deployment
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> goldpinger
<span class="hljs-attr">  namespace:</span> default
<span class="hljs-attr">  labels:</span>
<span class="hljs-attr">    app:</span> goldpinger
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  replicas:</span> <span class="hljs-number">3</span>                                                <span class="hljs-comment">#A</span>
<span class="hljs-attr">  selector:</span>                                                  <span class="hljs-comment">#B</span>
<span class="hljs-attr">    matchLabels:</span>
<span class="hljs-attr">      app:</span> goldpinger
<span class="hljs-attr">  template:</span>
<span class="hljs-attr">    metadata:</span>                                                <span class="hljs-comment">#C</span>
<span class="hljs-attr">      labels:</span>
<span class="hljs-attr">        app:</span> goldpinger
<span class="hljs-attr">    spec:</span>
<span class="hljs-attr">      serviceAccount:</span> <span class="hljs-string">&quot;goldpinger-serviceaccount&quot;</span>
<span class="hljs-attr">      containers:</span>
<span class="hljs-attr">      - name:</span> goldpinger
<span class="hljs-attr">        image:</span> <span class="hljs-string">&quot;docker.io/bloomberg/goldpinger:v3.0.0&quot;</span>
<span class="hljs-attr">        env:</span>
<span class="hljs-attr">        - name:</span> REFRESH_INTERVAL
<span class="hljs-attr">          value:</span> <span class="hljs-string">&quot;2&quot;</span>
<span class="hljs-attr">        - name:</span> HOST
<span class="hljs-attr">          value:</span> <span class="hljs-string">&quot;0.0.0.0&quot;</span>
<span class="hljs-attr">        - name:</span> PORT
<span class="hljs-attr">          value:</span> <span class="hljs-string">&quot;8080&quot;</span>                                      <span class="hljs-comment">#D</span>
        <span class="hljs-comment"># injecting real pod IP will make things easier to understand</span>
<span class="hljs-attr">        - name:</span> POD_IP
<span class="hljs-attr">          valueFrom:</span>
<span class="hljs-attr">            fieldRef:</span>
<span class="hljs-attr">              fieldPath:</span> status.podIP
<span class="hljs-attr">        ports:</span>
<span class="hljs-attr">        - containerPort:</span> <span class="hljs-number">8080</span>                                <span class="hljs-comment">#E</span>
<span class="hljs-attr">          name:</span> http
<span class="hljs-meta">---</span>
<span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Service
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> goldpinger
<span class="hljs-attr">  namespace:</span> default
<span class="hljs-attr">  labels:</span>
<span class="hljs-attr">    app:</span> goldpinger
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  type:</span> LoadBalancer
<span class="hljs-attr">  ports:</span>
<span class="hljs-attr">    - port:</span> <span class="hljs-number">8080</span>                                             <span class="hljs-comment">#F</span>
<span class="hljs-attr">      name:</span> http
<span class="hljs-attr">  selector:</span>
<span class="hljs-attr">    app:</span> goldpinger                                          <span class="hljs-comment">#G</span>
</code></pre>
<p>#A the deployment will create three replicas of the pods (three pods)</p>
<p>#B the deployment is configured to match pods with label app=goldpinger</p>
<p>#C the pods template actually gets the label app=goldpinger</p>
<p>#D we configure the Goldpinger pods to run on port 8080</p>
<p>#E we expose the port 8080 on the pod, so that it&#x2019;s reachable</p>
<p>#F in the service, we target port 8080 that we made available on the pods</p>
<p>#G the service will target pods based on the label app=goldpinger</p>
<p>With that, we&#x2019;re now ready to actually start it! If you&#x2019;re following along, you can find the source code for both of these files (<code>goldpinger-rbac.yml</code> and <code>goldpinger.yml</code>) at <a href="https://github.com/seeker89/chaos-engineering-book/tree/master/examples/kubernetes" target="_blank">https://github.com/seeker89/chaos-engineering-book/tree/master/examples/kubernetes</a>. Let&#x2019;s make sure that both files are in the same folder, and let&#x2019;s go ahead and run them.</p>
<h3 id="deploying-goldpinger">Deploying Goldpinger</h3>
<pre><code class="lang-shell">Start by creating the permissioning resources (the goldpinger-rbac.yml file), by running the following command: 
   kubectl apply -f goldpinger-rbac.yml
</code></pre>
<p>You will see Kubernetes confirming the three resources were created successfully, with the following output:</p>
<pre><code class="lang-shell">clusterrole.rbac.authorization.k8s.io/goldpinger-clusterrole created 
  serviceaccount/goldpinger-serviceaccount created 
  clusterrolebinding.rbac.authorization.k8s.io/goldpinger-clusterrolebinding created
</code></pre>
<p>Then, create the actual deployment and a service by running the following command:</p>
<pre><code class="lang-shell">kubectl apply -f goldpinger.yml
</code></pre>
<p>Just like before, you will see the confirmation that the resources were created:</p>
<pre><code class="lang-shell">deployment.apps/goldpinger created
service/goldpinger created
</code></pre>
<p>Once that&#x2019;s done, let&#x2019;s confirm that pods are running as expected. To do that, list the pods by running the following command:</p>
<pre><code class="lang-shell">kubectl get pods
</code></pre>
<p>You should see an output similar to the following, with three pods in status <code>Running</code> (bold font). If they&#x2019;re not, you might need to give it a few seconds to start:</p>
<pre><code class="lang-shell">NAME            READY   STATUS    RESTARTS   AGE
goldpinger-c86c78448-5kwpp   1/1     Running   0          1m4s
goldpinger-c86c78448-gtbvv   1/1     Running   0          1m4s
goldpinger-c86c78448-vcwx2   1/1     Running   0          1m4s
</code></pre>
<p>The pods are running, meaning that the deployment did its job. Goldpinger crashes if it can&#x2019;t list its peers, which means that the permissioning we set up also works as expected. The last thing to check, is that the service was configured correctly. You can do that by running the following command, specifying the name of the service we created (&#x201C;goldpinger&#x201D;):</p>
<pre><code class="lang-shell">kubectl describe svc goldpinger
</code></pre>
<p>You will see the details of the service, just like in the following output (abbreviated). Note the Endpoints field, specifying three IP addresses, for the three pods that it&#x2019;s configured to match.</p>
<pre><code class="lang-shell">Name:        goldpinger
Namespace:                default
Labels:      app=goldpinger
(...)
Endpoints:                172.17.0.3:8080,172.17.0.4:8080,172.17.0.5:8080
(...)
</code></pre>
<p>If you want to be 100% sure that the IPs are correct, you can compare them to the IPs of Goldpinger pods. You can display them easily, by appending <code>-o wide</code> (for wide output) to the kubectl get pods command. Try it by running the following:</p>
<pre><code class="lang-shell">kubectl get pods -o wide
</code></pre>
<p>You will see the same list as before, but this time with extra details, including the IP (bold font). They should correspond to the list specified in the service. If they weren&#x2019;t, it would point to misconfigured labels. Note, that depending on your internet connection speed and your setup, it might take a little bit of time to start. If you see pods in pending state, give it an extra minute:</p>
<pre><code class="lang-shell">NAME            READY   STATUS    RESTARTS   AGE   IP           NODE       NOMINATED NODE   READINESS GATES
goldpinger-c86c78448-5kwpp   1/1     Running   0          15m   172.17.0.4   minikube   &lt;none&gt;           &lt;none&gt;
goldpinger-c86c78448-gtbvv   1/1     Running   0          15m   172.17.0.3   minikube   &lt;none&gt;           &lt;none&gt;
goldpinger-c86c78448-vcwx2   1/1     Running   0          15m   172.17.0.5   minikube   &lt;none&gt;           &lt;none&gt;
</code></pre>
<p>Everything&apos;s up and running, so let&#x2019;s access Goldpinger to see what it&#x2019;s really doing. To do that, we&#x2019;ll need to access the service we created.</p>
<p><div class="alert alert-info hints-alert"><div class="hints-icon"><i class="fa fa-info-circle"></i></div><div class="hints-container"><p><strong>NOTE ACCESSING THE SOFTWARE RUNNING ON KUBERNETES FROM OUTSIDE THE CLUSTER</strong></p>
<p>Kubernetes does a great job standardizing the way people run their software. Unfortunately, not everything is easily standardized. Although every Kubernetes cluster supports services, the way you access the cluster and therefore its services depends on the way the cluster was set up. In this chapter, we will stick to Minikube, because it&#x2019;s simple and easily accessible to anyone. If you&#x2019;re running your own Kubernetes cluster, or use a managed solution from one of the cloud providers, accessing software running on the cluster might involve some extra setup (for example setting up an Ingress <a href="https://kubernetes.io/docs/concepts/services-networking/ingress/" target="_blank">https://kubernetes.io/docs/concepts/services-networking/ingress/</a>). Refer to the relevant documentation.</p>
</div></div></p>
<p>On Minikube, we can leverage the command <code>minikube service</code>, which will figure out a way to access the service directly from your host machine and open the browser for you. To do that, run the following command:</p>
<pre><code class="lang-shell">minikube service goldpinger
</code></pre>
<p>You will see an output similar to the following, specifying the special URL that Minikube prepared for you (bold font). Your default browser will be launched to open that URL.</p>
<pre><code class="lang-shell">|-----------|------------|-------------|-----------------------------|
| NAMESPACE |    NAME    | TARGET PORT |             URL             |
|-----------|------------|-------------|-----------------------------|
| default   | goldpinger | http/8080   | http://192.168.99.100:30426 |
|-----------|------------|-------------|-----------------------------|
&#x1F389;  Opening service default/goldpinger in default browser&#x2026;
</code></pre>
<p>Inside the newly launched browser window, you will see the Goldpinger UI. It will look similar to what&#x2019;s shown in figure 10.8. It&#x2019;s a graph, on which every point represents an instance of Goldpinger, and every arrow represents the last connectivity check (an HTTP request) between the instances. You can click a node to select it and display extra information. It also provides other functionality like a heatmap, showing hotspots of any potential networking slowness; and metrics, providing statistics that can be used to generate alerts and pretty dashboards. Goldpinger is a really handy tool for detecting any network issues, downloaded more than a million times from Docker Hub!</p>
<p>Figure 10.8 Goldpinger UI in action</p>
<p><img src="../images/10.8.jpg" alt=""></p>
<p>Feel free to take some time to play around, but otherwise we&#x2019;re done setting it all up. We have a running application that we can interact with, all deployed with just two kubectl commands.</p>
<p>Unfortunately, on our little test cluster, all three instances are running on the same host, so we&#x2019;re unlikely to see any network slowness, which is pretty boring. Fortunately, as chaos engineering practitioners, we&#x2019;re well equipped to introduce failure and make things interesting again. Let&#x2019;s start with the basics - an experiment to kill some pods.</p>
<h2 id="1042-experiment-1-kill-50-of-pods">10.4.2 Experiment 1: kill 50% of pods</h2>
<p>Much like a villain from a comic book movie, we might be interested in seeing what happens when we kill 50% of Goldpinger pods. Why do that? It&#x2019;s an inexpensive experiment that can answer a lot of questions about what happens when one of these instances goes down (simulating a machine going down). For example:</p>
<ul>
<li>Do the other instances detect that to begin with?</li>
<li>If so, how long before they detect it?</li>
<li>How does Goldpinger configuration affect all of that?</li>
<li>If we had an alert set up, would it get triggered?</li>
</ul>
<p>How should we go about implementing this? In the previous chapters, we&#x2019;ve covered different ways this could be addressed. For example, you could log into the machine running the Goldpinger process you want to kill, and simply run a kill command, like we did before. Or, if your cluster uses Docker to run the containers (more on that soon), you could leverage the tools we&#x2019;ve covered in chapter 5. The point is that all of the techniques you learned in the previous chapter still apply. That said, Kubernetes gives us other options, like directly deleting pods. It&#x2019;s definitely the most convenient way of achieving that, so let&#x2019;s go with that option.</p>
<p>There is another crucial detail to our experiment: Goldpinger works by periodically making HTTP requests to all of its peers. That period is controlled by the environment variable called <code>REFRESH_PERIOD</code>. In the goldpinger.yml file you deployed, that value was set to 2 seconds:</p>
<pre><code class="lang-shell">name: REFRESH_INTERVAL 
  value: &quot;2&quot;
</code></pre>
<p>That means that the maximum time it takes for an instance to notice another instance being down is 2 seconds. This is pretty aggressive and in a large cluster would result in a lot of traffic and CPU time spent on this, but I chose that value for our demonstration purposes. It will be handy to see the changes detected quickly. With that, we now have all the elements, so let&#x2019;s turn this into a concrete plan of an experiment.</p>
<h3 id="experiment-1-plan">Experiment 1: plan</h3>
<p>If we take the first question we mentioned (do other Goldpinger instances detect a peer down), we can design a simple experiment plan like so:</p>
<ol>
<li>Observability: use Goldpinger UI to see if there are any pods marked as inaccessible; use kubectl to see new pods come and go</li>
<li>Steady state: all nodes healthy</li>
<li>Hypothesis: if we delete one pod, we should see it in marked as failed in Goldpinger UI, and then be replaced by a new, healthy pod</li>
<li>Run the experiment</li>
</ol>
<p>That&#x2019;s it! Let&#x2019;s see how to implement it.</p>
<h3 id="experiment-1-implementation">Experiment 1: implementation</h3>
<p>To implement this experiment, the pod labels come in useful once again. All we need to do is leverage <code>kubectl get pods</code> to get all pods with label <code>app=goldpinger</code>, pick a random pod and kill it, using <code>kubectl delete</code>. To make things easy, we can also leverage kubectl&#x2019;s -o name flag to only display the pod names, and use a combination of <code>sort --random-sort</code> and <code>head -n1</code> to pick a random line of the output. Put all of this together, and you get a script like <code>kube-thanos.sh</code> from listing 10.3. Store it somewhere on your system (or clone it from the Github repo).</p>
<pre><code class="lang-shell">Listing 10.3 kube-thanos.sh
#!/bin/bash

kubectl get pods \                #A
  -l app=goldpinger \             #B
  -o name \                       #C
    | sort --random-sort \        #D
    | head -n 1 \                 #E
    | xargs kubectl delete        #F
</code></pre>
<p>#A use kubectl to list pods</p>
<p>#B only list pods with label app=goldpinger</p>
<p>#C only display the name as the output</p>
<p>#D sort in random order</p>
<p>#E pick the first one</p>
<p>#F delete the pod</p>
<p>Armed with that, we&#x2019;re ready to rock. Let&#x2019;s run the experiment.</p>
<h3 id="experiment-1-run">Experiment 1: run!</h3>
<p>Let&#x2019;s start by double-checking the steady state. Your Goldpinger installation should still be running and you should have the UI open in a browser window. If it&#x2019;s not, you can bring both back up by running the following commands:</p>
<pre><code class="lang-shell">kubectl apply -f goldpinger-rbac.yml 
  kubectl apply -f goldpinger.yml 
  minikube service goldpinger
</code></pre>
<p>To confirm all nodes are OK, simply refresh the graph by clicking the &#x201C;reload&#x201D; button, and verify that all three nodes are showing in green. So far so good.</p>
<p>To confirm that our script works, let&#x2019;s also set up some observability for the pods being deleted and created. We can leverage the <code>--watch</code> flag of the <code>kubectl get</code> command to print the names of all pods coming and going to the console. You can do that by opening a new terminal window, and running the following command:</p>
<pre><code class="lang-shell">kubectl get pods --watch
</code></pre>
<p>You will see the familiar output, showing all the Goldpinger pods, but this time the command will stay active, blocking the terminal. You can use Ctrl-C to exit at any time, if needed.</p>
<pre><code class="lang-shell">NAME            READY   STATUS    RESTARTS   AGE 
goldpinger-c86c78448-6rtw4   1/1     Running   0          20h 
goldpinger-c86c78448-mj76q   1/1     Running   0          19h 
goldpinger-c86c78448-xbj7s   1/1     Running   0          19h
</code></pre>
<p>Now, to the fun part! To conduct our experiment, we&#x2019;ll open another terminal window for the <code>kube-thanos.sh</code> script, run it to kill a random pod, and then quickly go to the Goldpinger UI to observe what the Goldpinger pods saw. Bear in mind that in the local setup, the pods will recover very rapidly, so you might need to be quick to actually observe the pod becoming unavailable and then healing. In the meantime, the <code>kubectl get pods --watch</code> command will record the pod going down and a replacement coming up. Let&#x2019;s do that!</p>
<p>Open a new terminal window and run the script to kill a random pod:</p>
<pre><code class="lang-shell">bash kube-thanos.sh
</code></pre>
<p>You will see an output showing the name of the pod being deleted, like in the following:</p>
<pre><code class="lang-shell">pod &quot;goldpinger-c86c78448-shtdq&quot; deleted
</code></pre>
<p>Go quickly to the Goldpinger UI and click refresh. You should see some failure, like in figure 10.9. Nodes that can&#x2019;t be reached by at least one other node will be marked as unhealthy. I marked the unhealthy node in the figure. The live UI also uses a red color to differentiate them. You will also notice that there are four nodes showing up. This is because after the pod is deleted, Kubernetes tries to recoverge to the desired state (three replicas), so it creates a new pod to replace the one we deleted.</p>
<p><div class="alert alert-info hints-alert"><div class="hints-icon"><i class="fa fa-info-circle"></i></div><div class="hints-container"><p><strong>NOTE BE QUICK!</strong></p>
<p>If you&#x2019;re not seeing any errors, the pods probably recovered before you switched to the UI window, because your computer is quicker than mine when I was writing this and chose the parameters. If you re-run the command and refresh the UI more quickly, you should be able to see it.</p>
</div></div></p>
<p>Figure 10.9 Goldpinger UI showing an unavailable pod being replaced by a new one</p>
<p><img src="../images/10.9.jpg" alt=""></p>
<p>Now, go back to the terminal window that is running <code>kubectl get pods --watch</code>. You will see an output similar to the following. Note the pod that we killed (<code>-shtdq</code>) going into Terminating state, and a new pod (<code>-lwxrq</code>) taking its place (both in bold font). You will also notice that the new pod goes through a lifecycle of Pending to ContainerCreating to Running, while the old one goes to Terminating.</p>
<pre><code class="lang-shell">NAME            READY   STATUS    RESTARTS   AGE 
goldpinger-c86c78448-pfqmc   1/1     Running   0          47s 
goldpinger-c86c78448-shtdq   1/1     Running   0          22s 
goldpinger-c86c78448-xbj7s   1/1     Running   0          20h 
goldpinger-c86c78448-shtdq   1/1     Terminating   0          38s 
goldpinger-c86c78448-lwxrq   0/1     Pending       0          0s 
goldpinger-c86c78448-lwxrq   0/1     Pending       0          0s 
goldpinger-c86c78448-lwxrq   0/1     ContainerCreating   0          0s 
goldpinger-c86c78448-shtdq   0/1     Terminating         0          39s 
goldpinger-c86c78448-lwxrq   1/1     Running             0          2s 
goldpinger-c86c78448-shtdq   0/1     Terminating         0          43s 
goldpinger-c86c78448-shtdq   0/1     Terminating         0          43s
</code></pre>
<p>Finally, let&#x2019;s check that everything recovered smoothly. To do that, go back to the browser window with Goldpinger UI, and refresh once more. You should now see the three new pods happily pinging each other, all in green. Which means that our hypothesis was correct, on both fronts.</p>
<p>Nice job. Another one bites the dust, another experiment under your belt. But before we move on, let&#x2019;s just discuss a few points.</p>
<p><div class="alert alert-info hints-alert"><div class="hints-icon"><i class="fa fa-info-circle"></i></div><div class="hints-container"><p><strong>NOTE POP QUIZ: WHAT HAPPENS WHEN A POD DIES ON A KUBERNETES CLUSTER?</strong></p>
<p>Pick one:</p>
<ol>
<li>Kubernetes detects it and send you an alert</li>
<li>Kubernetes detects it, and will restart it as necessary to make sure the expected number of replicas are running</li>
<li>Nothing</li>
</ol>
<p>See appendix B for answers.</p>
</div></div></p>
<h3 id="experiment-1-discussion">Experiment 1: discussion</h3>
<p>For the sake of teaching, I took a few shortcuts here that I want to make you aware of. First, when accessing the pods through the UI, we&#x2019;re using a service, which resolves to a pseudo-random instance of Goldpinger every time you make a new call. That means that it&#x2019;s possible to get routed to the instance we just killed, and get an error in the UI. It also means that every time you refresh the view, you get the reality from a point of view of a different pod. For illustration purposes, that&#x2019;s not a deal-breaker on a small test cluster but if you run a large cluster and want to make sure that a network partition doesn&#x2019;t obscure your view, you will need to make sure you consult all available instances, or at least a reasonable subset. Goldpinger addresses that issue with metrics, and you can learn more about that at <a href="https://github.com/bloomberg/goldpinger#prometheus" target="_blank">https://github.com/bloomberg/goldpinger#prometheus</a></p>
<p>Second, using a GUI-based tool this way is a little bit awkward. If you see what you expect, that&#x2019;s great. But if you don&#x2019;t, it doesn&#x2019;t necessarily mean it didn&#x2019;t happen, it might be that you simply missed it. Again, this can be alleviated by using the metrics, which I skipped here for the sake of simplicity.</p>
<p>Third, if you look closely at the failures that you see in the graph, you will see that the pods sometimes start receiving traffic before they are actually up. This is because, again for simplicity, I skipped the readiness probe that serves exactly that purpose. If set, a readiness probe prevents a pod from receiving any traffic until a certain condition is met (see the documentation at <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/" target="_blank">https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/</a>). For an example of how to use it, see the installation docs of Goldpinger (<a href="https://github.com/bloomberg/goldpinger#installation" target="_blank">https://github.com/bloomberg/goldpinger#installation</a>).</p>
<p>Finally, remember that depending on the refresh period you&#x2019;re running Goldpinger with, the data you&#x2019;re looking at is up to that many seconds stale, which means that for the pods we killed, we&#x2019;ll keep seeing them for an extra number of seconds equal to the refresh period (2 seconds in our setup).</p>
<p>These are the caveats my lawyers advised me to clarify before this goes to print. In case that makes you think I&#x2019;m not fun at parties, let me prove you wrong. Let&#x2019;s play some Invaders, like it&#x2019;s 1978.</p>
<h2 id="1043-party-trick-killing-pods-in-style">10.4.3 Party trick: killing pods in style</h2>
<p>If you really want to make a point that chaos engineering is fun, I&#x2019;ve got two tools for you.</p>
<p>First, KubeInvaders (<a href="https://github.com/lucky-sideburn/KubeInvaders" target="_blank">https://github.com/lucky-sideburn/KubeInvaders</a>). It gamifies the process of killing pods by starting a clone of Space Invaders, where the aliens are pods in the specified namespace. You guessed it, the ones you shoot down are deleted in Kubernetes. Installation involves deploying it on a cluster, and then connecting a local client that actually displays the game content. See figure 10.10 to see what it looks like in action.</p>
<p>Figure 10.10 Kubeinvader screenshot from <a href="https://github.com/lucky-sideburn/KubeInvaders" target="_blank">https://github.com/lucky-sideburn/KubeInvaders</a></p>
<p><img src="../images/10.10.jpg" alt=""></p>
<p>The second one is for fans of the first-person shooter genre: Kube DOOM (<a href="https://github.com/storax/kubedoom" target="_blank">https://github.com/storax/kubedoom</a>). Similar to KubeInvaders, it represents pods as enemies, and kills in Kubernetes the ones that die in the game. Tip to justify using it: it&#x2019;s often much quicker than copying and pasting a name of a pod, saving so much time (mandatory reference: <a href="https://xkcd.com/303/" target="_blank">https://xkcd.com/303/</a>). See figure 10.11 for a screenshot.</p>
<p>Figure 10.11 Kube DOOM screenshot from <a href="https://github.com/storax/kubedoom" target="_blank">https://github.com/storax/kubedoom</a></p>
<p><img src="../images/10.11.jpg" alt=""></p>
<p>For Kube DOOM, the installation is pretty straightforward: you run a pod on the host, pass a kubectl configuration file to it, and then use a desktop sharing client to connect to the game. After a long day of debugging, it might be just what you need. I&#x2019;ll just leave it there.</p>
<p>I&#x2019;m sure that will help with your next house party. When you finish the game, let&#x2019;s take a look at another experiment - some good old network slowness.</p>
<h2 id="1044-experiment-2-network-slowness">10.4.4 Experiment 2: network slowness</h2>
<p>Slowness, my nemesis, we meet again. If you&#x2019;re a software engineer, chances are you&#x2019;re spending a lot of your time trying to outwit slowness. When things go wrong, actual failure is often easier to debug than situations when things mostly work. And slowness tends to fall into the latter category.</p>
<p>Slowness is such an important topic that we touch upon it in nearly every chapter of this book. We introduced some slowness using <code>tc</code> in chapter 4, and then again using Pumba in Docker in chapter 5. We use some in the context of JVM, application level ,and even browser in other chapters. Time to take a look at what&#x2019;s different when running on Kubernetes.</p>
<p>It&#x2019;s worth mentioning that everything we covered before still applies here. We could very well use <code>tc</code> or Pumba directly on one of the machines running the processes we&#x2019;re interested in, and modify them to introduce the failure we care about. In fact, using <code>kubectl cp</code> and <code>kubectl exec</code>, we could upload and execute tc commands directly in a pod, without even worrying about accessing the host. Or we could even add a second container to the Goldpinger pod that would execute the necessary <code>tc</code> commands.</p>
<p>All of these options are viable, but share one downside: they modify the existing software that&#x2019;s running on your cluster, and so by definition carry risks of messing things up. A convenient alternative is to add extra software, tweaked to implement the failure we care about, but otherwise identical to the original and introduce the extra software in a way that will integrate with the rest of the system. Kubernetes makes it really easy. Let me show you what I mean; let&#x2019;s design an experiment around simulated network slowness.</p>
<h3 id="experiment-2-plan">Experiment 2: plan</h3>
<p>Let&#x2019;s say that we want to see what happens when one instance of Goldpinger is slow to respond to queries of its peers. After all, this is what this piece of software was designed to help with, so before we rely on it, we should test that it works as expected.</p>
<p>A convenient way of doing that is to deploy a copy of Goldpinger that we can modify to add a delay. Once again, we could do it with <code>tc</code>, but to show you some new tools, let&#x2019;s use a standalone network proxy instead. That proxy will sit in front of that new Goldpinger instance, receive the calls from its peers, add the delay, and relay the calls to Goldpinger. Thanks to Kubernetes, setting it all up is pretty straightforward.</p>
<p>Let&#x2019;s iron out some details. Goldpinger&#x2019;s default timeout for all calls is 300ms, so let&#x2019;s pick an arbitrary value of 250ms for our delay: enough to be clearly seen, but not enough to cause a timeout. And thanks to the built-in heatmap, we will be able to visually show the connections that take longer than others, so the observability aspect is taken care of. The plan of the experiment figuratively writes itself:</p>
<ol>
<li>Observability: use Goldpinger UI to read delays using the graph UI and the heatmap</li>
<li>Steady state: all existing Goldpinger instantes report healthy</li>
<li>Hypothesis: if we add a new instance that has a 250ms delay, the connectivity graph will show all four instances healthy, and the 250ms delay will be visible in the heatmap</li>
<li>Run the experiment!</li>
</ol>
<p>Sounds good? Let&#x2019;s see how to implement it.</p>
<h3 id="experiment-2-implementation">Experiment 2: implementation</h3>
<p>Time to dig into what the implementation will look like. Do you remember figure 10.4 that showed how Goldpinger worked? Let me copy it for your convenience in figure 10.12. Every instance asks Kubernetes for all its peers, and then periodically makes calls to them to measure latency and detect problems.</p>
<p>Figure 10.12 Overview of how Goldpinger works (again)</p>
<p><img src="../images/10.12.jpg" alt=""></p>
<p>Now, what we want to do is add a copy of the Goldpinger pod that has the extra proxy we just discussed in front of it. A pod in Kubernetes can have multiple containers running alongside each other and able to communicate via localhost. If we use the same label <code>app=goldpinger</code>, the other instances will detect the new pod and start calling. But we will configure the ports in a way that instead of directly reaching the new instance, the peers will first reach the proxy (in port 8080). And the proxy will add the desired latency. The extra Goldpinger instance will be able to ping the other hosts freely, like a regular instance. This is summarized in figure 10.13.</p>
<p>Figure 10.13 A modified copy of Goldpinger with an extra proxy in front of it</p>
<p><img src="../images/10.13.jpg" alt=""></p>
<p>We&#x2019;ve got the idea of what the setup will look like, now we need the actual networking proxy. Goldpinger communicates via HTTP/1.1, so we&#x2019;re in luck. It&#x2019;s a text-based, reasonably simple protocol running on top of TCP. All we need is the protocol specification (RFC 7230<sup><a href="#fn_1" id="reffn_1">1</a></sup>, RFC 7231<sup><a href="#fn_2" id="reffn_2">2</a></sup>, RFC 7232<sup><a href="#fn_3" id="reffn_3">3</a></sup>, RFC 7233<sup><a href="#fn_4" id="reffn_4">4</a></sup> and RFC 7234<sup><a href="#fn_5" id="reffn_5">5</a></sup>) and we should be able to implement a quick proxy in no time. Dust off your C compiler, stretch your arms, and let&#x2019;s do it!</p>
<h3 id="experiment-2-toxiproxy">Experiment 2: Toxiproxy</h3>
<p>Just kidding! We&#x2019;ll use an existing, open-source project designed for this kind of thing, called Toxiproxy (<a href="https://github.com/shopify/toxiproxy" target="_blank">https://github.com/shopify/toxiproxy</a>). It works as a proxy on TCP level (L4 OSI model), which is fine for us, because we don&#x2019;t actually need to understand anything about what&#x2019;s going on on the HTTP level (L7) to introduce a simple latency. The added benefit is that you can use the same tool for any other TCP-based protocol in the exact same way, so what we&#x2019;re about to do will be equally applicable to a lot of other popular software, like Redis, MySQL, PostgreSQL and many more.</p>
<p>ToxiProxy consists of two pieces:</p>
<ul>
<li>the actual proxy server, which exposes an API you can use to configure what should be proxied where and the kind of failure that you expect</li>
<li>and a CLI client, that connects to that API and can change the configuration live</li>
</ul>
<p><div class="alert alert-info hints-alert"><div class="hints-icon"><i class="fa fa-info-circle"></i></div><div class="hints-container"><p><strong>NOTE CLI AND API</strong></p>
<p>Instead of using the CLI, you can also talk to the API directly, and ToxiProxy offers ready-to-use clients in a variety of languages.</p>
</div></div></p>
<p>The dynamic nature of ToxiProxy makes it really useful when used in unit and integration testing. For example, your integration test could start by configuring the proxy to add latency when connecting to a database, and then your test could verify that timeouts are triggered accordingly. It&#x2019;s also going to be handy for us in implementing our experiment.</p>
<p>The version we&#x2019;ll use, 2.1.4 (<a href="https://github.com/Shopify/toxiproxy/releases/tag/v2.1.4" target="_blank">https://github.com/Shopify/toxiproxy/releases/tag/v2.1.4</a>), is the latest available release at the time of writing. We&#x2019;re going to run the proxy server as part of the extra Goldpinger pod using a prebuilt, publicly available image from Docker Hub. We&#x2019;ll also need to use the CLI locally on your machine. To install it, download the CLI executable for your system (Ubuntu/Debian, Windows, MacOS) from <a href="https://github.com/Shopify/toxiproxy/releases/tag/v2.1.4" target="_blank">https://github.com/Shopify/toxiproxy/releases/tag/v2.1.4</a> and add it to your PATH. To confirm it works, run the following command:</p>
<pre><code>toxiproxy-cli &#x2013;version
</code></pre><p>You should see the version 2.1.4 displayed, like in the following output:</p>
<pre><code>toxiproxy-cli version 2.1.4
</code></pre><p>When a ToxiProxy server starts, by default it doesn&#x2019;t do anything apart from running its HTTP API. By calling the API, you can configure and dynamically change the behavior of the proxy server. You can define arbitrary configurations defined by:</p>
<ol>
<li>a unique name</li>
<li>a host and port to bind to and listen for connections</li>
<li>a destination server to proxy to</li>
</ol>
<p>For every configuration like this, you can attach failures. In ToxiProxy lingo, these failures are called &#x201C;toxics&#x201D;. Currently, the following toxics are available:</p>
<ol>
<li>latency - add arbitrary latency to the connection (in either direction)</li>
<li>down - take the connection down</li>
<li>bandwidth - throttle the connection to the desired speed</li>
<li>slow close - delay the TCP socket from closing for an arbitrary time</li>
<li>timeout - wait for an arbitrary time and then close the connection</li>
<li>slicer - slices the received data into smaller bits before sending it to the destination</li>
</ol>
<p>You can attach an arbitrary combination of failures to every proxy configuration you define. For our needs, the latency toxic will do exactly what we want it to. Let&#x2019;s see how all of this fits together.</p>
<p><div class="alert alert-info hints-alert"><div class="hints-icon"><i class="fa fa-info-circle"></i></div><div class="hints-container"><p><strong>NOTE POP QUIZ: WHAT&#x2019;S TOXIPROXY?</strong></p>
<p>Pick one:</p>
<ol>
<li>A configurable TCP proxy, that can simulate various problems, like dropped packets or network slowness</li>
<li>A K-pop band singing about the environmental consequences of dumping large amounts of toxic waste sent to third world countries through the use of proxy and shell companies</li>
</ol>
<p>See appendix B for answers.</p>
</div></div></p>
<h3 id="experiment-2-implementation-continued">Experiment 2: implementation continued</h3>
<p>To sum it all up, we want to create a new pod with two containers: one for Goldpinger and one for the ToxiProxy. We&#x2019;ll need to configure Goldpinger to run on a different port, so that the proxy can listen on the default port 8080 that the other Goldpinger instances will try to connect to. We&#x2019;ll also create a service that routes connections to the proxy API on port 8474, so that we can use toxiproxy-cli commands to configure the proxy and add the latency that we want,just like in figure 10.14.</p>
<p>Figure 10.14 Interacting with the modified version of Goldpinger using toxiproxy-cli</p>
<p><img src="../images/10.14.jpg" alt=""></p>
<p>Let&#x2019;s now translate this into a Kubernetes .yml file. You can see the resulting goldpinger-chaos.yml in listing 10.4. You will see two resource descriptions, a pod (with two containers) and a service. Note, that we use the same service account we created before, to give Goldpinger the same permissions. We&#x2019;re also using two environment variables, <code>PORT</code> and <code>CLIENT_PORT_OVERRIDE</code>, to make Goldpinger listen on port 9090, but call its peers on port 8080, respectively. This is because by default, Goldpinger calls its peers on the same port that it runs itself. Finally, notice that the service is using a label <code>chaos=absolutely</code> to match to the new pod we created. It&#x2019;s important that the Goldpinger pod has the label <code>app=goldpinger</code>, so that it can be found by its peers, but we also need another label to be able to route connections to the proxy API.</p>
<pre><code class="lang-yaml"><span class="hljs-meta">---</span>
<span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Pod
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> goldpinger-chaos
<span class="hljs-attr">  namespace:</span> default
<span class="hljs-attr">  labels:</span>
<span class="hljs-attr">    app:</span> goldpinger                                <span class="hljs-comment">#A</span>
<span class="hljs-attr">    chaos:</span> absolutely
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  serviceAccount:</span> <span class="hljs-string">&quot;goldpinger-serviceaccount&quot;</span>      <span class="hljs-comment">#B</span>
<span class="hljs-attr">  containers:</span>
<span class="hljs-attr">  - name:</span> goldpinger
<span class="hljs-attr">    image:</span> docker.io/bloomberg/goldpinger:v3<span class="hljs-number">.0</span><span class="hljs-number">.0</span>
<span class="hljs-attr">    env:</span>
<span class="hljs-attr">    - name:</span> REFRESH_INTERVAL
<span class="hljs-attr">      value:</span> <span class="hljs-string">&quot;2&quot;</span>
<span class="hljs-attr">    - name:</span> HOST
<span class="hljs-attr">      value:</span> <span class="hljs-string">&quot;0.0.0.0&quot;</span>
<span class="hljs-attr">    - name:</span> PORT
<span class="hljs-attr">      value:</span> <span class="hljs-string">&quot;9090&quot;</span>                               <span class="hljs-comment">#C</span>
<span class="hljs-attr">    - name:</span> CLIENT_PORT_OVERRIDE
<span class="hljs-attr">      value:</span> <span class="hljs-string">&quot;8080&quot;</span>
<span class="hljs-attr">    - name:</span> POD_IP
<span class="hljs-attr">      valueFrom:</span>
<span class="hljs-attr">        fieldRef:</span>
<span class="hljs-attr">          fieldPath:</span> status.podIP
<span class="hljs-attr">  - name:</span> toxiproxy
<span class="hljs-attr">    image:</span> docker.io/shopify/toxiproxy:<span class="hljs-number">2.1</span><span class="hljs-number">.4</span>
<span class="hljs-attr">    ports:</span>
<span class="hljs-attr">    - containerPort:</span> <span class="hljs-number">8474</span>                         <span class="hljs-comment">#D</span>
<span class="hljs-attr">      name:</span> toxiproxy-api
<span class="hljs-attr">    - containerPort:</span> <span class="hljs-number">8080</span>
<span class="hljs-attr">      name:</span> goldpinger
<span class="hljs-meta">---</span>
<span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Service
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> goldpinger-chaos
<span class="hljs-attr">  namespace:</span> default
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  type:</span> LoadBalancer
<span class="hljs-attr">  ports:</span>
<span class="hljs-attr">    - port:</span> <span class="hljs-number">8474</span>                                  <span class="hljs-comment">#E</span>
<span class="hljs-attr">      name:</span> toxiproxy-api
<span class="hljs-attr">  selector:</span>
<span class="hljs-attr">    chaos:</span> absolutely                             <span class="hljs-comment">#F</span>
</code></pre>
<p>#A the new pod has the same label app=goldpinger to be detected by its peers, but also chaos=absolutely to be matched by the proxy api service</p>
<p>#B we use the same service account as other instances to give Goldpinger permission to list its peers</p>
<p>#C we use HOST envvar to make Goldpinger listen on port 9090, and CLIENT_PORT_OVERRIDE to make it call itse peers on the default port 8080</p>
<p>#D ToxiProxy container will expose two ports: 8474 with the ToxiProxy API and 8080 to proxy through to Goldpinger</p>
<p>#E the service will route traffic to port 8474 (ToxiProxy API)</p>
<p>#F the service will use label chaos=absolutely to select the pods running ToxiProxy</p>
<p>And that&#x2019;s all we need. Make sure you have this file handy (or clone it from the repo like before). Ready to rock? Let the games begin!</p>
<h3 id="experiment-2-run">Experiment 2: run!</h3>
<p>To run this experiment, we&#x2019;re going to use the Goldpinger UI. If you closed the browser window before, restart it by running the following command in the terminal:</p>
<pre><code>minikube service goldpinger
</code></pre><p>Let&#x2019;s start with the steady state, and confirm that all three nodes are visible and report as healthy. In the top bar, click Heatmap. You will see a heatmap similar to the one in figure 10.15. Each square represents connectivity between nodes and is color-coded based on the time it took to execute a request.</p>
<ul>
<li>Columns represent source (from)</li>
<li>Rows represent destinations (to)</li>
<li>The legend clarifies which number corresponds to which pod.</li>
</ul>
<p>In this example, all squares are the same color and shade, meaning that all requests took below 2ms, which is to be expected when all instances run on the same host. You can also tweak the values to your liking and click &#x201C;refresh&#x201D; to show a new heatmap. Close it when you&#x2019;re ready.</p>
<p>Figure 10.15 Example of Goldpinger Heatmap</p>
<p><img src="../images/10.15.jpg" alt=""></p>
<p>Let&#x2019;s introduce our new pod! To do that, we&#x2019;ll <code>kubectl apply</code> the goldpinger-chaos.yml file from listing 10.4. Run the following command:</p>
<pre><code>kubectl apply -f goldpinger-chaos.yml
</code></pre><p>You will see an output confirming creation of a pod and service:</p>
<pre><code>pod/goldpinger-chaos created
service/goldpinger-chaos created
</code></pre><p>Let&#x2019;s confirm it&#x2019;s running by going to the UI. You will now see an extra node, just like in figure 10.16. But notice that the new pod is marked as unhealthy - all of its peers are failing to connect to it. In the live UI, the node is marked in red, and in the figure 10.16 I annotated the new, unhealthy node for you. This is because we haven&#x2019;t configured the proxy to pass the traffic yet.</p>
<p>Figure 10.16 Extra Goldpinger instance, detected by its peers, but inaccessible</p>
<p><img src="../images/10.16.jpg" alt=""></p>
<p>Let&#x2019;s address that by configuring the ToxiProxy. This is where the extra service we deployed comes in handy: we will use it to connect to the ToxiProxy API using toxiproxy-cli. Do you remember how we used <code>minikube service</code> to get a special URL to access the Goldpinger service? We&#x2019;ll leverage that again, but this time with the <code>--url</code> flag, to only print the url itself. Run the following command in a bash session to store the url in a variable:</p>
<pre><code>TOXIPROXY_URL=$(minikube service --url goldpinger-chaos)
</code></pre><p>We can now use the variable to point toxiproxy-cli to the right ToxiProxy API. That&#x2019;s done using the -h flag. Confusingly, -h is not for &#x201C;help&#x201D;, it&#x2019;s for &#x201C;host&#x201D;. Let&#x2019;s confirm it works by listing the existing proxy configuration:</p>
<pre><code>toxiproxy-cli -h $TOXIPROXY_URL list
</code></pre><p>You will see the following output, saying there are no proxies configured. It even goes so far as to hint we create some proxies (bold font):</p>
<pre><code>Name       Listen          Upstream                Enabled         Toxics
no proxies

Hint: create a proxy with `toxiproxy-cli create`
</code></pre><p>Let&#x2019;s configure one. We&#x2019;ll call it chaos, make it route to localhost:9090 (where we configured Goldpinger to listen to) and listen on 0.0.0.0:8080 to make it accessible to its peers to call. Run the following command to make that happen:</p>
<pre><code>toxiproxy-cli \ 
  -h $TOXIPROXY_URL \        #A
  create chaos \             #B
  -l 0.0.0.0:8080 \          #C
  -u localhost:9090          #D
</code></pre><p>#A connect to specific proxy</p>
<p>#B create a new proxy configuration called &#x201C;chaos&#x201D;</p>
<p>#C listen on 0.0.0.0:8080 (default Goldpinger port)</p>
<p>#D relay connections to localhost:9090 (where we configured Goldpinger to run)</p>
<p>You will see a simple confirmation that the proxy was created:</p>
<pre><code>Created new proxy chaos
</code></pre><p>Rerun the <code>toxiproxy-cli list</code> command to see the new proxy appear this time:</p>
<pre><code>toxiproxy-cli -h $TOXIPROXY_URL list
</code></pre><p>You will see the following output, listing a new proxy configuration called &#x201C;chaos&#x201D; (bold font):</p>
<pre><code>Name       Listen          Upstream                Enabled         Toxics 
  ================================================ 
  chaos      [::]:8080       localhost:9090          enabled         None 

Hint: inspect toxics with `toxiproxy-cli inspect &lt;proxyName&gt;`
</code></pre><p>If you go back to the UI and click refresh, you will see that the <code>goldpinger-chaos</code> extra instance is now green, and all instances happily report healthy state in all directions. If you check the heatmap, it will also show all green.</p>
<p>Let&#x2019;s change that. Using the command <code>toxiproxy-cli toxic add</code>, let&#x2019;s add a single toxic with 250ms latency. Do that by running the following command:</p>
<pre><code>toxiproxy-cli \
-h $TOXIPROXY_URL \
toxic add \                  #A
--type latency \             #B
--a latency=250 \            #C
--upstream \                 #D
chaos                        #E
</code></pre><p>#A add a toxic to an existing proxy configuration</p>
<p>#B toxic type is latency</p>
<p>#C we want to add 250ms of latency</p>
<p>#D we set it in the upstream direction, towards the Goldpinger instance</p>
<p>#E we attach this toxic to a proxy configuration called &#x201C;chaos&#x201D;</p>
<p>You will see a confirmation:</p>
<pre><code>Added upstream latency toxic &apos;latency_upstream&apos; on proxy &apos;chaos&apos;
To confirm that the proxy got it right, we can inspect our proxy called &#x201C;chaos&#x201D;. To do that, run the following command:
toxiproxy-cli -h $TOXIPROXY_URL inspect chaos
</code></pre><p>You will see an output just like the following, listing our brand new toxic (bold font):</p>
<pre><code>Name: chaos     Listen: [::]:8080       Upstream: localhost:9090 
  ====================================================================== 
  Upstream toxics: 
  latency_upstream:       type=latency    stream=upstream toxicity=1.00   attributes=[    jitter=0        latency=250     ] 

  Downstream toxics: 
  Proxy has no Downstream toxics enabled.
</code></pre><p>Now, go back to the Goldpinger UI in the browser and refresh. You will still see all four instances reporting healthy and happy (the 250ms delay fits within the default timeout of 300ms). But if you open the heatmap, this time it will tell a different story. The row with <code>goldpinger-chaos</code> pod will be marked in red (problem threshold), implying that all its peers detected slowness. See figure 10.17 for a screenshot.</p>
<p>Figure 10.17 Goldpinger heatmap, showing slowness accessing pod goldpinger-chaos</p>
<p><img src="../images/10.17.jpg" alt=""></p>
<p>This means that our hypothesis was correct: Goldpinger correctly detects and reports the slowness, and at 250ms, below the default timeout of 300ms, the Goldpinger graph UI reports all as healthy. And we did all of that without modifying the existing pods.</p>
<p>This wraps up the experiment, but before we go, let&#x2019;s clean up the extra pod. To do that, run the following command to delete everything we created using the goldpinger-chaos.yml file:</p>
<pre><code>kubectl delete -f goldpinger-chaos.yml
</code></pre><p>Let&#x2019;s discuss our findings.</p>
<h3 id="experiment-2-discussion">Experiment 2: Discussion</h3>
<p>How well did we do? We took some time to learn new tools, but the entire implementation of the experiment boiled down to a single .yml file and a handful of commands with ToxiProxy. We also had a tangible benefit of working on a copy of the software that we wanted to test, leaving the existing running processes unmodified. We effectively rolled out some extra capacity and then had 25% of running software affected, limiting the blast radius. Does it mean we could do that in production? As with any sufficiently complex question, the answer is, &#x201C;it depends.&#x201D; In this example, if we wanted to verify the robustness of some alerting that relies on metrics from Goldpinger to trigger, this could be a good way to do it. But the extra software could also affect the existing instances in a more profound way, making it more risky. At the end of the day, it really depends on your application.</p>
<p>There is, of course, room for improvement. For example, the service we&#x2019;re using to access the Goldpinger UI is routing traffic to any instance matched in a pseudo-random fashion. That means that sometimes it will route to the instance that has the 250ms delay. In our case, that will be difficult to spot with the naked eye, but if you wanted to test a larger delay, it could be a problem.</p>
<p>Time to wrap up this first part. Coming in part 2: making your chaos engineer life easier with PowerfulSeal.</p>
<hr>
<blockquote id="fn_1">
<sup>1</sup>. <a href="https://tools.ietf.org/html/rfc7230" target="_blank">https://tools.ietf.org/html/rfc7230</a><a href="#reffn_1" title="Jump back to footnote [1] in the text."> &#x21A9;</a>
</blockquote>
<blockquote id="fn_2">
<sup>2</sup>. <a href="https://tools.ietf.org/html/rfc7231" target="_blank">https://tools.ietf.org/html/rfc7231</a><a href="#reffn_2" title="Jump back to footnote [2] in the text."> &#x21A9;</a>
</blockquote>
<blockquote id="fn_3">
<sup>3</sup>. <a href="https://tools.ietf.org/html/rfc7232" target="_blank">https://tools.ietf.org/html/rfc7232</a><a href="#reffn_3" title="Jump back to footnote [3] in the text."> &#x21A9;</a>
</blockquote>
<blockquote id="fn_4">
<sup>4</sup>. <a href="https://tools.ietf.org/html/rfc7233" target="_blank">https://tools.ietf.org/html/rfc7233</a><a href="#reffn_4" title="Jump back to footnote [4] in the text."> &#x21A9;</a>
</blockquote>
<blockquote id="fn_5">
<sup>5</sup>. <a href="https://tools.ietf.org/html/rfc7234" target="_blank">https://tools.ietf.org/html/rfc7234</a><a href="#reffn_5" title="Jump back to footnote [5] in the text."> &#x21A9;</a>
</blockquote>

                                
                                </section>
                            
    </div>
    <div class="search-results">
        <div class="has-results">
            
            <h1 class="search-results-title"><span class='search-results-count'></span> results matching "<span class='search-query'></span>"</h1>
            <ul class="search-results-list"></ul>
            
        </div>
        <div class="no-results">
            
            <h1 class="search-results-title">No results matching "<span class='search-query'></span>"</h1>
            
        </div>
    </div>
</div>

                        </div>
                    </div>
                
            </div>

            
                
                <a href="Setting_up_a_Kubernetes_cluster.html" class="navigation navigation-prev " aria-label="Previous page: 10.3 Setting up a Kubernetes cluster">
                    <i class="fa fa-angle-left"></i>
                </a>
                
                
                <a href="Summary_10.html" class="navigation navigation-next " aria-label="Next page: 10.5 Summary">
                    <i class="fa fa-angle-right"></i>
                </a>
                
            
        
    </div>

    <script>
        var gitbook = gitbook || [];
        gitbook.push(function() {
            gitbook.page.hasChanged({"page":{"title":"10.4 Testing out software running on Kubernetes","level":"2.1.4","depth":2,"next":{"title":"10.5 Summary","level":"2.1.5","depth":2,"path":"docs/Summary_10.md","ref":"docs/Summary_10.md","articles":[]},"previous":{"title":"10.3 Setting up a Kubernetes cluster","level":"2.1.3","depth":2,"path":"docs/Setting_up_a_Kubernetes_cluster.md","ref":"docs/Setting_up_a_Kubernetes_cluster.md","articles":[]},"dir":"ltr"},"config":{"plugins":["page-toc-button","cnzz","hints","sitemap-general"],"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"pluginsConfig":{"search":{},"cnzz":{"style":"1","visible":true,"siteid":"1278605528"},"hints":{"danger":"fa fa-bug","info":"fa fa-info-circle","tip":"fa fa-sticky-note","working":"fa fa-cog fa-spin"},"lunr":{"maxIndexSize":1000000,"ignoreSpecialCharacters":false},"sitemap-general":{"prefix":"https://wangwei1237.gitee.io/chaos-engineering"},"fontsettings":{"theme":"white","family":"serif","size":2},"highlight":{},"page-toc-button":{"maxTocDepth":2,"minTocSize":2},"sharing":{"facebook":true,"twitter":true,"google":false,"weibo":false,"instapaper":false,"vk":false,"all":["facebook","google","twitter","weibo","instapaper"]},"theme-default":{"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"showLevel":false}},"theme":"default","author":"wangwei17","pdf":{"pageBreaksBefore":"/","headerTemplate":null,"paperSize":"a4","margin":{"right":62,"left":62,"top":36,"bottom":36},"fontSize":16,"fontFamily":"Arial","footerTemplate":null,"chapterMark":"pagebreak","pageNumbers":true},"structure":{"langs":"LANGS.md","readme":"README.md","glossary":"GLOSSARY.md","summary":"SUMMARY.md"},"isbn":"","variables":{},"title":"Chaos Engineering","links":{"sharing":{"all":null,"facebook":null,"google":null,"twitter":null,"weibo":null},"sidebar":{"17哥的个人网站":"https://wangwei1237.gitee.io/"}},"gitbook":"*","description":"Chaos-Engineering","extension":null},"file":{"path":"docs/Testing_out_software_running_on_Kubernetes.md","mtime":"2020-11-30T02:28:13.400Z","type":"markdown"},"gitbook":{"version":"3.2.3","time":"2020-12-09T07:58:23.938Z"},"basePath":"..","book":{"language":""}});
        });
    </script>
</div>

        
    <script src="../gitbook/gitbook.js"></script>
    <script src="../gitbook/theme.js"></script>
    
        
        <script src="../gitbook/gitbook-plugin-page-toc-button/plugin.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-cnzz/plugin.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-search/search-engine.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-search/search.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-lunr/lunr.min.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-lunr/search-lunr.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-sharing/buttons.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-fontsettings/fontsettings.js"></script>
        
    

    </body>
</html>

