
<!DOCTYPE HTML>
<html lang="" >
    <head>
        <meta charset="UTF-8">
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <title>10.4 Testing out software running on Kubernetes · Chaos Engineering</title>
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="description" content="">
        <meta name="generator" content="GitBook 3.2.3">
        <meta name="author" content="wangwei17">
        
        
    
    <link rel="stylesheet" href="../gitbook/style.css">

    
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-page-toc-button/plugin.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-hints/plugin-hints.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-highlight/website.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-search/search.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-fontsettings/website.css">
                
            
        

    

    
        
    
        
    
        
    
        
    
        
    
        
    

        
    
    
    <meta name="HandheldFriendly" content="true"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <link rel="apple-touch-icon-precomposed" sizes="152x152" href="../gitbook/images/apple-touch-icon-precomposed-152.png">
    <link rel="shortcut icon" href="../gitbook/images/favicon.ico" type="image/x-icon">

    
    <link rel="next" href="Summary_10.html" />
    
    
    <link rel="prev" href="Setting_up_a_Kubernetes_cluster.html" />
    

    </head>
    <body>
        
<div class="book">
    <div class="book-summary">
        
            
<div id="book-search-input" role="search">
    <input type="text" placeholder="Type to search" />
</div>

            
                <nav role="navigation">
                


<ul class="summary">
    
    
    
        
        <li>
            <a href="https://wangwei1237.gitee.io/" target="_blank" class="custom-link">17哥的个人网站</a>
        </li>
    
    

    
    <li class="divider"></li>
    

    
        
        
    
        <li class="chapter " data-level="1.1" data-path="../">
            
                <a href="../">
            
                    
                    Introduction
            
                </a>
            

            
        </li>
    

    
        
        <li class="header">PART 3: Chaos Engineering beyond machines</li>
        
        
    
        <li class="chapter " data-level="2.1" data-path="Chaos_in_Kubernetes.html">
            
                <a href="Chaos_in_Kubernetes.html">
            
                    
                    10 Chaos in Kubernetes
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="2.1.1" data-path="Porting_things_onto_Kubernetes.html">
            
                <a href="Porting_things_onto_Kubernetes.html">
            
                    
                    10.1 Porting things onto Kubernetes
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="2.1.2" data-path="Whats_Kubernetes_in_7_minutes.html">
            
                <a href="Whats_Kubernetes_in_7_minutes.html">
            
                    
                    10.2 What’s Kubernetes (in 7 minutes)
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="2.1.3" data-path="Setting_up_a_Kubernetes_cluster.html">
            
                <a href="Setting_up_a_Kubernetes_cluster.html">
            
                    
                    10.3 Setting up a Kubernetes cluster
            
                </a>
            

            
        </li>
    
        <li class="chapter active" data-level="2.1.4" data-path="Testing_out_software_running_on_Kubernetes.html">
            
                <a href="Testing_out_software_running_on_Kubernetes.html">
            
                    
                    10.4 Testing out software running on Kubernetes
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="2.1.5" data-path="Summary_10.html">
            
                <a href="Summary_10.html">
            
                    
                    10.5 Summary
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="2.2" data-path="Automating_Kubernetes_experiments.html">
            
                <a href="Automating_Kubernetes_experiments.html">
            
                    
                    11 Automating Kubernetes experiments
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="2.3" data-path="Under_the_hood_of_Kubernetes.html">
            
                <a href="Under_the_hood_of_Kubernetes.html">
            
                    
                    12 Under the hood of Kubernetes
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="2.4" data-path="Chaos_engineering_for_people.html">
            
                <a href="Chaos_engineering_for_people.html">
            
                    
                    13 Chaos engineering (for) people
            
                </a>
            

            
        </li>
    

    

    <li class="divider"></li>

    <li>
        <a href="https://www.gitbook.com" target="blank" class="gitbook-link">
            Published with GitBook
        </a>
    </li>
</ul>


                </nav>
            
        
    </div>

    <div class="book-body">
        
            <div class="body-inner">
                
                    

<div class="book-header" role="navigation">
    

    <!-- Title -->
    <h1>
        <i class="fa fa-circle-o-notch fa-spin"></i>
        <a href=".." >10.4 Testing out software running on Kubernetes</a>
    </h1>
</div>




                    <div class="page-wrapper" tabindex="-1" role="main">
                        <div class="page-inner">
                            
<div id="book-search-results">
    <div class="search-noresults">
    
                                <section class="normal markdown-section">
                                
                                <h1 id="104-testing-out-software-running-on-kubernetes">10.4 Testing out software running on Kubernetes</h1>
<p>With a functional Kubernetes cluster at our disposal, we&#x2019;re now ready to start working on the High Profile Project, aka ICANT. The pressure is on, we have a project to save!</p>
<p>As always, the first step is to build an understanding of how things work before we can reason about how they break. We&#x2019;ll do that by kicking the tires and looking how ICANT is deployed and configured. Once we&#x2019;re done with that, we&#x2019;ll conduct two experiments and then finish this section by seeing how to make things easier for ourselves for the next time. Let&#x2019;s start at the beginning - by running the actual project</p>
<h2 id="1041-running-the-icant-project">10.4.1 Running the ICANT Project</h2>
<p>As we discovered earlier when reading the documentation you inherited, the project didn&#x2019;t get very far. They took an off-the-shelf component (Goldpinger), deployed it, and called it a day. All of which is bad news for the project, but good news to me; I have less explaining to do!</p>
<p>Goldpinger works by querying Kubernetes for all the instances of itself, and then periodically calling each of these instances and measuring the response time. It then uses that data to generate statistics (metrics) and plot a pretty connectivity graph. Each instance works in the same way - it periodically gets the address of its peers, and makes a request to each one of them. This is illustrated in figure 10.4. Goldpinger was invented to detect network slow-downs and problems, especially in larger clusters. It&#x2019;s really simple and very effective.</p>
<p>Figure 10.4 Overview of how Goldpinger works</p>
<p><img src="../images/10.4.jpg" alt=""></p>
<p>How do we go about running it? We&#x2019;ll do it in two steps:</p>
<ol>
<li>Set up the right permissions, so that Goldpinger can query Kubernetes for its peer</li>
<li>Deploy it on the cluster</li>
</ol>
<p>We&#x2019;re about to step into Kubernetes Wonderland, so let me introduce you to some Kubernetes lingo.</p>
<h3 id="kubernetes-terminology">Kubernetes terminology</h3>
<p>The documentation often mentions resources to mean the objects representing various abstractions that Kubernetes offers. For now, I&#x2019;m going to introduce you to three basic building blocks used to describe software on Kubernetes:</p>
<ul>
<li>Pod. A pod is a collection of containers that are grouped together, run on the same host and share some system resources, for example an IP address. This is the unit of software that you can schedule on Kubernetes. You can schedule pods directly, but most of the time you will be using a higher level abstraction, such as a Deployment.</li>
<li>Deployment. A deployment describes a blueprint for creating pods, along with extra metadata, like for example the number of replicas to run. Importantly, it also manages the lifecycle of pods that it creates. For example, if you modify a deployment to update a version of the image you want to run, the deployment can handle a rollout, deleting old pods and creating new ones one by one to avoid an outage. It also offers other things, like roll-back, if the roll out ever fails.</li>
<li>Service. A service matches an arbitrary set of pods, and provides a single IP address that resolves to the matched pods. That IP is kept up to date with the changes made to the cluster. For example, if a pod goes down, it will be taken out of the pool.</li>
</ul>
<p>You can see a visual representation of how these fit together in figure 10.5.</p>
<p>Figure 10.5 Pods, deployments and services example in Kubernetes</p>
<p><img src="../images/10.5.jpg" alt=""></p>
<p>Another thing you need to know to understand how Goldpinger works is that to query Kubernetes, you need to have the right permissions.</p>
<p><div class="alert alert-info hints-alert"><div class="hints-icon"><i class="fa fa-info-circle"></i></div><div class="hints-container"><p><strong>NOTE POP QUIZ: WHAT&#x2019;S A KUBERNETES DEPLOYMENT?</strong></p>
<p>Pick one:</p>
<ol>
<li>A description of how to reach software running on your cluster</li>
<li>A description of how to deploy some software on your cluster</li>
<li>A description of how to build a container</li>
</ol>
<p>See appendix B for answers.</p>
</div></div></p>
<h3 id="permissions">Permissions</h3>
<p>Kubernetes has an elegant way of managing permissions. First, it has a concept of a ClusterRole, that allows you to define a role and a corresponding set of permissions to execute verbs (create, get, delete, list, &#x2026;) on various resources. Second, it has the concept of ServiceAccounts, which can be linked to any software running on Kubernetes, so that it inherits all the permissions that the ServiceAccount was granted. And finally, to make a link between a ServiceAccount and a ClusterRole, you can use a ClusterRoleBinding, which does exactly what it says on the tin.</p>
<p>If you&#x2019;re new to it, this permissioning might sound a little bit abstract, so take a look at figure 10.6 for a graphical representation of how all of this comes together.</p>
<p>Figure 10.6 Kubernetes permissioning example</p>
<p><img src="../images/10.6.jpg" alt=""></p>
<p>In our case, we want to allow Goldpinger pods to list its peers, so all we need is a single ClusterRole, and the corresponding ServiceAccount and ClusterRoleBinding. Later, we will use that ServiceAccount to permission the Goldpinger pods.</p>
<h3 id="creating-the-resources">Creating the resources</h3>
<p>Time for some code! In Kubernetes, we can describe all resources we want to create using a Yaml file (.yml; <a href="https://yaml.org/" target="_blank">https://yaml.org/</a>) that follows the specific format that Kubernetes accepts. See listing 10.1 to see how all of this permissioning translates into .yml. For each element we described, there is a Yaml object, specifying the corresponding type (kind) and the expected parameters. First, a ClusterRole called <code>goldpinger-clusterrol</code> that allows for listing pods (bold font). Then a ServiceAccount called <code>goldpinger-serviceaccount</code> (bold font). And finally, a ClusterRoleBinding, linking the ClusterRole to the ServiceAccount. If you&#x2019;re new to Yaml, note that the <code>---</code> separators allow for describing multiple resources in a single file.</p>
<pre><code class="lang-yaml">Listing <span class="hljs-number">10.1</span> goldpinger-rbac.yaml
<span class="hljs-meta">---</span>
<span class="hljs-attr">apiVersion:</span> rbac.authorization.k8s.io/v1
<span class="hljs-attr">kind:</span> ClusterRole                              <span class="hljs-comment">#A</span>
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> goldpinger-clusterrole
<span class="hljs-attr">rules:</span>
<span class="hljs-attr">- apiGroups:</span>
<span class="hljs-bullet">  -</span> <span class="hljs-string">&quot;&quot;</span>
<span class="hljs-attr">  resources:</span>
<span class="hljs-bullet">  -</span> pods                                       <span class="hljs-comment">#B</span>
<span class="hljs-attr">  verbs:</span>
<span class="hljs-bullet">  -</span> list                                       <span class="hljs-comment">#C</span>
<span class="hljs-meta">---</span>
<span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> ServiceAccount                           <span class="hljs-comment">#D</span>
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> goldpinger-serviceaccount
<span class="hljs-attr">  namespace:</span> default
<span class="hljs-meta">---</span>
<span class="hljs-attr">apiVersion:</span> rbac.authorization.k8s.io/v1beta1
<span class="hljs-attr">kind:</span> ClusterRoleBinding
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> goldpinger-clusterrolebinding
<span class="hljs-attr">roleRef:</span>
<span class="hljs-attr">  apiGroup:</span> rbac.authorization.k8s.io
<span class="hljs-attr">  kind:</span> ClusterRole
<span class="hljs-attr">  name:</span> goldpinger-clusterrole                 <span class="hljs-comment">#E</span>
<span class="hljs-attr">subjects:</span>
<span class="hljs-attr">  - kind:</span> ServiceAccount
<span class="hljs-attr">    name:</span> goldpinger-serviceaccount            <span class="hljs-comment">#F</span>
<span class="hljs-attr">    namespace:</span> default
</code></pre>
<p>#A we start with a cluster role</p>
<p>#B the cluster role gets permissions for resource of type pod</p>
<p>#C the cluster role gets permissions to list the resource of type pod</p>
<p>#D we create a service account to use later</p>
<p>#E we create a cluster role binding, that binds the cluster role...</p>
<p>#F &#x2026; to the service account</p>
<p>This takes care of the permissionsing part. Let&#x2019;s now go ahead and see what deploying the actual Goldpinger looks like.</p>
<h3 id="goldpinger-yml-files">Goldpinger .yml files</h3>
<p>To make sense of deploying Goldpinger, I need to explain one more detail that I skipped over so far: matching and labels.</p>
<p>Kubernetes makes extensive use of labels, which are simple key-value pairs of type string. Every resource can have arbitrary metadata attached to it, including labels. They are used by Kubernetes to match sets of resources, and are fairly flexible and easy to use.</p>
<p>For example, let&#x2019;s say that you have two pods, with the following labels:</p>
<ol>
<li>Pod A, with labels <code>app=goldpinger</code> and <code>stage=dev</code></li>
<li>Pod B, with labels <code>app=goldpinger</code> and <code>stage=prod</code></li>
</ol>
<p>If you match (select) all pods with label <code>app=goldpinger</code>, you will get both pods. But if you match with label <code>stage=dev</code>, you will only get pod A. You can also query by multiple labels, and in that case Kubernetes will return pods matching all requested labels (a logical AND).</p>
<p>Labels are useful for manually grouping resources, but they&#x2019;re also leveraged by Kubernetes, for example to implement deployments. When you create a deployment, you need to specify the selector (a set of labels to match), and that selector needs to match the pods created by the deployment. The connection between the deployment and the pods it manages relies on labels.</p>
<p>Label-matching is also the same mechanism that Goldpinger leverages to query for its peers: it just asks Kubernetes for all pods with a specific label (by default <code>app=goldpinger</code>). Figure 10.7 shows that graphically.</p>
<p>Figure 10.7 Kubernetes permissioning example</p>
<p><img src="../images/10.7.jpg" alt=""></p>
<p>Putting this all together, we can finally write a .yml file with two resource descriptors: a deployment and a matching service.</p>
<p>Inside the deployment, we need to specify the following:</p>
<ul>
<li>The number of replicas (we&#x2019;ll go with three for demonstration purposes)</li>
<li>The selector (again the default app=goldpinger),</li>
<li>The actual template of pods to create</li>
</ul>
<p>In the pod template, we will specify the container image to run, some environment values required for Goldpinger to work and ports to expose so that other instances can reach it. The important bit is that we need to specify some arbitrary port that matches the <code>PORT</code> environment variable (this is what Goldpinger uses to know what port to listen on). We&#x2019;ll go with 8080. Finally, we also specify the service account we created earlier on, to permission the Goldpinger pods to query Kubernetes for their peers.</p>
<p>Inside the service, we once again use the same selector (<code>app=goldpinger</code>), so that the service matches the pods created by the deployment, and the same port 8080 that we specified on the deployment.</p>
<p><div class="alert alert-info hints-alert"><div class="hints-icon"><i class="fa fa-info-circle"></i></div><div class="hints-container"><p><strong>NOTE DEPLOYMENTS AND DAEMONSETS</strong></p>
<p>In a typical installation, we would like to have one Goldpinger pod per node (physical machine, VM) in your cluster. That can be easily achieved by using a DaemonSet (it works a lot like a deployment, but instead of specifying the number of replicas, it just assumes one replica per node - learn more at <a href="https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/" target="_blank">https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/</a>). In our example setup we will use a Deployment instead, because with only one node, we would only have a single pod of Goldpinger, which defeats the purpose of this demonstration.</p>
</div></div></p>
<p>Listing 10.2 contains the .yml file we can use to create the deployment and the service. Take a look.</p>
<pre><code class="lang-yaml">Listing <span class="hljs-number">10.2</span> goldpinger.yml .
<span class="hljs-meta">---</span>
<span class="hljs-attr">apiVersion:</span> apps/v1
<span class="hljs-attr">kind:</span> Deployment
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> goldpinger
<span class="hljs-attr">  namespace:</span> default
<span class="hljs-attr">  labels:</span>
<span class="hljs-attr">    app:</span> goldpinger
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  replicas:</span> <span class="hljs-number">3</span>                                                <span class="hljs-comment">#A</span>
<span class="hljs-attr">  selector:</span>                                                  <span class="hljs-comment">#B</span>
<span class="hljs-attr">    matchLabels:</span>
<span class="hljs-attr">      app:</span> goldpinger
<span class="hljs-attr">  template:</span>
<span class="hljs-attr">    metadata:</span>                                                <span class="hljs-comment">#C</span>
<span class="hljs-attr">      labels:</span>
<span class="hljs-attr">        app:</span> goldpinger
<span class="hljs-attr">    spec:</span>
<span class="hljs-attr">      serviceAccount:</span> <span class="hljs-string">&quot;goldpinger-serviceaccount&quot;</span>
<span class="hljs-attr">      containers:</span>
<span class="hljs-attr">      - name:</span> goldpinger
<span class="hljs-attr">        image:</span> <span class="hljs-string">&quot;docker.io/bloomberg/goldpinger:v3.0.0&quot;</span>
<span class="hljs-attr">        env:</span>
<span class="hljs-attr">        - name:</span> REFRESH_INTERVAL
<span class="hljs-attr">          value:</span> <span class="hljs-string">&quot;2&quot;</span>
<span class="hljs-attr">        - name:</span> HOST
<span class="hljs-attr">          value:</span> <span class="hljs-string">&quot;0.0.0.0&quot;</span>
<span class="hljs-attr">        - name:</span> PORT
<span class="hljs-attr">          value:</span> <span class="hljs-string">&quot;8080&quot;</span>                                      <span class="hljs-comment">#D</span>
        <span class="hljs-comment"># injecting real pod IP will make things easier to understand</span>
<span class="hljs-attr">        - name:</span> POD_IP
<span class="hljs-attr">          valueFrom:</span>
<span class="hljs-attr">            fieldRef:</span>
<span class="hljs-attr">              fieldPath:</span> status.podIP
<span class="hljs-attr">        ports:</span>
<span class="hljs-attr">        - containerPort:</span> <span class="hljs-number">8080</span>                                <span class="hljs-comment">#E</span>
<span class="hljs-attr">          name:</span> http
<span class="hljs-meta">---</span>
<span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Service
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> goldpinger
<span class="hljs-attr">  namespace:</span> default
<span class="hljs-attr">  labels:</span>
<span class="hljs-attr">    app:</span> goldpinger
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  type:</span> LoadBalancer
<span class="hljs-attr">  ports:</span>
<span class="hljs-attr">    - port:</span> <span class="hljs-number">8080</span>                                             <span class="hljs-comment">#F</span>
<span class="hljs-attr">      name:</span> http
<span class="hljs-attr">  selector:</span>
<span class="hljs-attr">    app:</span> goldpinger                                          <span class="hljs-comment">#G</span>
</code></pre>
<p>#A the deployment will create three replicas of the pods (three pods)</p>
<p>#B the deployment is configured to match pods with label app=goldpinger</p>
<p>#C the pods template actually gets the label app=goldpinger</p>
<p>#D we configure the Goldpinger pods to run on port 8080</p>
<p>#E we expose the port 8080 on the pod, so that it&#x2019;s reachable</p>
<p>#F in the service, we target port 8080 that we made available on the pods</p>
<p>#G the service will target pods based on the label app=goldpinger</p>
<p>With that, we&#x2019;re now ready to actually start it! If you&#x2019;re following along, you can find the source code for both of these files (<code>goldpinger-rbac.yml</code> and <code>goldpinger.yml</code>) at <a href="https://github.com/seeker89/chaos-engineering-book/tree/master/examples/kubernetes" target="_blank">https://github.com/seeker89/chaos-engineering-book/tree/master/examples/kubernetes</a>. Let&#x2019;s make sure that both files are in the same folder, and let&#x2019;s go ahead and run them.</p>
<h3 id="deploying-goldpinger">Deploying Goldpinger</h3>
<pre><code class="lang-shell">Start by creating the permissioning resources (the goldpinger-rbac.yml file), by running the following command: 
   kubectl apply -f goldpinger-rbac.yml
</code></pre>
<p>You will see Kubernetes confirming the three resources were created successfully, with the following output:</p>
<pre><code class="lang-shell">clusterrole.rbac.authorization.k8s.io/goldpinger-clusterrole created 
  serviceaccount/goldpinger-serviceaccount created 
  clusterrolebinding.rbac.authorization.k8s.io/goldpinger-clusterrolebinding created
</code></pre>
<p>Then, create the actual deployment and a service by running the following command:</p>
<pre><code class="lang-shell">kubectl apply -f goldpinger.yml
</code></pre>
<p>Just like before, you will see the confirmation that the resources were created:</p>
<pre><code class="lang-shell">deployment.apps/goldpinger created
service/goldpinger created
</code></pre>
<p>Once that&#x2019;s done, let&#x2019;s confirm that pods are running as expected. To do that, list the pods by running the following command:</p>
<pre><code class="lang-shell">kubectl get pods
</code></pre>
<p>You should see an output similar to the following, with three pods in status <code>Running</code> (bold font). If they&#x2019;re not, you might need to give it a few seconds to start:</p>
<pre><code class="lang-shell">NAME            READY   STATUS    RESTARTS   AGE
goldpinger-c86c78448-5kwpp   1/1     Running   0          1m4s
goldpinger-c86c78448-gtbvv   1/1     Running   0          1m4s
goldpinger-c86c78448-vcwx2   1/1     Running   0          1m4s
</code></pre>
<p>The pods are running, meaning that the deployment did its job. Goldpinger crashes if it can&#x2019;t list its peers, which means that the permissioning we set up also works as expected. The last thing to check, is that the service was configured correctly. You can do that by running the following command, specifying the name of the service we created (&#x201C;goldpinger&#x201D;):</p>
<pre><code class="lang-shell">kubectl describe svc goldpinger
</code></pre>
<p>You will see the details of the service, just like in the following output (abbreviated). Note the Endpoints field, specifying three IP addresses, for the three pods that it&#x2019;s configured to match.</p>
<pre><code class="lang-shell">Name:        goldpinger
Namespace:                default
Labels:      app=goldpinger
(...)
Endpoints:                172.17.0.3:8080,172.17.0.4:8080,172.17.0.5:8080
(...)
</code></pre>
<p>If you want to be 100% sure that the IPs are correct, you can compare them to the IPs of Goldpinger pods. You can display them easily, by appending <code>-o wide</code> (for wide output) to the kubectl get pods command. Try it by running the following:</p>
<pre><code class="lang-shell">kubectl get pods -o wide
</code></pre>
<p>You will see the same list as before, but this time with extra details, including the IP (bold font). They should correspond to the list specified in the service. If they weren&#x2019;t, it would point to misconfigured labels. Note, that depending on your internet connection speed and your setup, it might take a little bit of time to start. If you see pods in pending state, give it an extra minute:</p>
<pre><code class="lang-shell">NAME            READY   STATUS    RESTARTS   AGE   IP           NODE       NOMINATED NODE   READINESS GATES
goldpinger-c86c78448-5kwpp   1/1     Running   0          15m   172.17.0.4   minikube   &lt;none&gt;           &lt;none&gt;
goldpinger-c86c78448-gtbvv   1/1     Running   0          15m   172.17.0.3   minikube   &lt;none&gt;           &lt;none&gt;
goldpinger-c86c78448-vcwx2   1/1     Running   0          15m   172.17.0.5   minikube   &lt;none&gt;           &lt;none&gt;
</code></pre>
<p>Everything&apos;s up and running, so let&#x2019;s access Goldpinger to see what it&#x2019;s really doing. To do that, we&#x2019;ll need to access the service we created.</p>
<p><div class="alert alert-info hints-alert"><div class="hints-icon"><i class="fa fa-info-circle"></i></div><div class="hints-container"><p><strong>NOTE ACCESSING THE SOFTWARE RUNNING ON KUBERNETES FROM OUTSIDE THE CLUSTER</strong></p>
<p>Kubernetes does a great job standardizing the way people run their software. Unfortunately, not everything is easily standardized. Although every Kubernetes cluster supports services, the way you access the cluster and therefore its services depends on the way the cluster was set up. In this chapter, we will stick to Minikube, because it&#x2019;s simple and easily accessible to anyone. If you&#x2019;re running your own Kubernetes cluster, or use a managed solution from one of the cloud providers, accessing software running on the cluster might involve some extra setup (for example setting up an Ingress <a href="https://kubernetes.io/docs/concepts/services-networking/ingress/" target="_blank">https://kubernetes.io/docs/concepts/services-networking/ingress/</a>). Refer to the relevant documentation.</p>
</div></div></p>
<p>On Minikube, we can leverage the command <code>minikube service</code>, which will figure out a way to access the service directly from your host machine and open the browser for you. To do that, run the following command:</p>
<pre><code class="lang-shell">minikube service goldpinger
</code></pre>
<p>You will see an output similar to the following, specifying the special URL that Minikube prepared for you (bold font). Your default browser will be launched to open that URL.</p>
<pre><code class="lang-shell">|-----------|------------|-------------|-----------------------------|
| NAMESPACE |    NAME    | TARGET PORT |             URL             |
|-----------|------------|-------------|-----------------------------|
| default   | goldpinger | http/8080   | http://192.168.99.100:30426 |
|-----------|------------|-------------|-----------------------------|
&#x1F389;  Opening service default/goldpinger in default browser&#x2026;
</code></pre>
<p>Inside the newly launched browser window, you will see the Goldpinger UI. It will look similar to what&#x2019;s shown in figure 10.8. It&#x2019;s a graph, on which every point represents an instance of Goldpinger, and every arrow represents the last connectivity check (an HTTP request) between the instances. You can click a node to select it and display extra information. It also provides other functionality like a heatmap, showing hotspots of any potential networking slowness; and metrics, providing statistics that can be used to generate alerts and pretty dashboards. Goldpinger is a really handy tool for detecting any network issues, downloaded more than a million times from Docker Hub!</p>
<p>Figure 10.8 Goldpinger UI in action</p>
<p><img src="../images/10.8.jpg" alt=""></p>
<p>Feel free to take some time to play around, but otherwise we&#x2019;re done setting it all up. We have a running application that we can interact with, all deployed with just two kubectl commands.</p>
<p>Unfortunately, on our little test cluster, all three instances are running on the same host, so we&#x2019;re unlikely to see any network slowness, which is pretty boring. Fortunately, as chaos engineering practitioners, we&#x2019;re well equipped to introduce failure and make things interesting again. Let&#x2019;s start with the basics - an experiment to kill some pods.</p>
<h2 id="1042-experiment-1-kill-50-of-pods">10.4.2 Experiment 1: kill 50% of pods</h2>
<p>Much like a villain from a comic book movie, we might be interested in seeing what happens when we kill 50% of Goldpinger pods. Why do that? It&#x2019;s an inexpensive experiment that can answer a lot of questions about what happens when one of these instances goes down (simulating a machine going down). For example:</p>
<ul>
<li>Do the other instances detect that to begin with?</li>
<li>If so, how long before they detect it?</li>
<li>How does Goldpinger configuration affect all of that?</li>
<li>If we had an alert set up, would it get triggered?</li>
</ul>
<p>How should we go about implementing this? In the previous chapters, we&#x2019;ve covered different ways this could be addressed. For example, you could log into the machine running the Goldpinger process you want to kill, and simply run a kill command, like we did before. Or, if your cluster uses Docker to run the containers (more on that soon), you could leverage the tools we&#x2019;ve covered in chapter 5. The point is that all of the techniques you learned in the previous chapter still apply. That said, Kubernetes gives us other options, like directly deleting pods. It&#x2019;s definitely the most convenient way of achieving that, so let&#x2019;s go with that option.</p>
<p>There is another crucial detail to our experiment: Goldpinger works by periodically making HTTP requests to all of its peers. That period is controlled by the environment variable called <code>REFRESH_PERIOD</code>. In the goldpinger.yml file you deployed, that value was set to 2 seconds:</p>
<pre><code class="lang-shell">name: REFRESH_INTERVAL 
  value: &quot;2&quot;
</code></pre>
<p>That means that the maximum time it takes for an instance to notice another instance being down is 2 seconds. This is pretty aggressive and in a large cluster would result in a lot of traffic and CPU time spent on this, but I chose that value for our demonstration purposes. It will be handy to see the changes detected quickly. With that, we now have all the elements, so let&#x2019;s turn this into a concrete plan of an experiment.</p>
<h3 id="experiment-1-plan">Experiment 1: plan</h3>
<p>If we take the first question we mentioned (do other Goldpinger instances detect a peer down), we can design a simple experiment plan like so:</p>
<ol>
<li>Observability: use Goldpinger UI to see if there are any pods marked as inaccessible; use kubectl to see new pods come and go</li>
<li>Steady state: all nodes healthy</li>
<li>Hypothesis: if we delete one pod, we should see it in marked as failed in Goldpinger UI, and then be replaced by a new, healthy pod</li>
<li>Run the experiment</li>
</ol>
<p>That&#x2019;s it! Let&#x2019;s see how to implement it.</p>
<h3 id="experiment-1-implementation">Experiment 1: implementation</h3>
<p>To implement this experiment, the pod labels come in useful once again. All we need to do is leverage <code>kubectl get pods</code> to get all pods with label <code>app=goldpinger</code>, pick a random pod and kill it, using <code>kubectl delete</code>. To make things easy, we can also leverage kubectl&#x2019;s -o name flag to only display the pod names, and use a combination of <code>sort --random-sort</code> and <code>head -n1</code> to pick a random line of the output. Put all of this together, and you get a script like <code>kube-thanos.sh</code> from listing 10.3. Store it somewhere on your system (or clone it from the Github repo).</p>
<pre><code class="lang-shell">Listing 10.3 kube-thanos.sh
#!/bin/bash

kubectl get pods \                #A
  -l app=goldpinger \             #B
  -o name \                       #C
    | sort --random-sort \        #D
    | head -n 1 \                 #E
    | xargs kubectl delete        #F
</code></pre>
<p>#A use kubectl to list pods</p>
<p>#B only list pods with label app=goldpinger</p>
<p>#C only display the name as the output</p>
<p>#D sort in random order</p>
<p>#E pick the first one</p>
<p>#F delete the pod</p>
<p>Armed with that, we&#x2019;re ready to rock. Let&#x2019;s run the experiment.</p>
<h3 id="experiment-1-run">Experiment 1: run!</h3>
<p>Let&#x2019;s start by double-checking the steady state. Your Goldpinger installation should still be running and you should have the UI open in a browser window. If it&#x2019;s not, you can bring both back up by running the following commands:</p>
<pre><code class="lang-shell">kubectl apply -f goldpinger-rbac.yml 
  kubectl apply -f goldpinger.yml 
  minikube service goldpinger
</code></pre>
<p>To confirm all nodes are OK, simply refresh the graph by clicking the &#x201C;reload&#x201D; button, and verify that all three nodes are showing in green. So far so good.</p>
<p>To confirm that our script works, let&#x2019;s also set up some observability for the pods being deleted and created. We can leverage the <code>--watch</code> flag of the <code>kubectl get</code> command to print the names of all pods coming and going to the console. You can do that by opening a new terminal window, and running the following command:</p>
<pre><code class="lang-shell">kubectl get pods --watch
</code></pre>
<p>You will see the familiar output, showing all the Goldpinger pods, but this time the command will stay active, blocking the terminal. You can use Ctrl-C to exit at any time, if needed.</p>
<pre><code class="lang-shell">NAME            READY   STATUS    RESTARTS   AGE 
goldpinger-c86c78448-6rtw4   1/1     Running   0          20h 
goldpinger-c86c78448-mj76q   1/1     Running   0          19h 
goldpinger-c86c78448-xbj7s   1/1     Running   0          19h
</code></pre>
<p>Now, to the fun part! To conduct our experiment, we&#x2019;ll open another terminal window for the <code>kube-thanos.sh</code> script, run it to kill a random pod, and then quickly go to the Goldpinger UI to observe what the Goldpinger pods saw. Bear in mind that in the local setup, the pods will recover very rapidly, so you might need to be quick to actually observe the pod becoming unavailable and then healing. In the meantime, the <code>kubectl get pods --watch</code> command will record the pod going down and a replacement coming up. Let&#x2019;s do that!</p>
<p>Open a new terminal window and run the script to kill a random pod:</p>
<pre><code class="lang-shell">bash kube-thanos.sh
</code></pre>
<p>You will see an output showing the name of the pod being deleted, like in the following:</p>
<pre><code class="lang-shell">pod &quot;goldpinger-c86c78448-shtdq&quot; deleted
</code></pre>
<p>Go quickly to the Goldpinger UI and click refresh. You should see some failure, like in figure 10.9. Nodes that can&#x2019;t be reached by at least one other node will be marked as unhealthy. I marked the unhealthy node in the figure. The live UI also uses a red color to differentiate them. You will also notice that there are four nodes showing up. This is because after the pod is deleted, Kubernetes tries to recoverge to the desired state (three replicas), so it creates a new pod to replace the one we deleted.</p>
<p><div class="alert alert-info hints-alert"><div class="hints-icon"><i class="fa fa-info-circle"></i></div><div class="hints-container"><p><strong>NOTE BE QUICK!</strong></p>
<p>If you&#x2019;re not seeing any errors, the pods probably recovered before you switched to the UI window, because your computer is quicker than mine when I was writing this and chose the parameters. If you re-run the command and refresh the UI more quickly, you should be able to see it.</p>
</div></div></p>
<p>Figure 10.9 Goldpinger UI showing an unavailable pod being replaced by a new one</p>
<p><img src="../images/10.9.jpg" alt=""></p>
<p>Now, go back to the terminal window that is running <code>kubectl get pods --watch</code>. You will see an output similar to the following. Note the pod that we killed (<code>-shtdq</code>) going into Terminating state, and a new pod (<code>-lwxrq</code>) taking its place (both in bold font). You will also notice that the new pod goes through a lifecycle of Pending to ContainerCreating to Running, while the old one goes to Terminating.</p>
<pre><code class="lang-shell">NAME            READY   STATUS    RESTARTS   AGE 
goldpinger-c86c78448-pfqmc   1/1     Running   0          47s 
goldpinger-c86c78448-shtdq   1/1     Running   0          22s 
goldpinger-c86c78448-xbj7s   1/1     Running   0          20h 
goldpinger-c86c78448-shtdq   1/1     Terminating   0          38s 
goldpinger-c86c78448-lwxrq   0/1     Pending       0          0s 
goldpinger-c86c78448-lwxrq   0/1     Pending       0          0s 
goldpinger-c86c78448-lwxrq   0/1     ContainerCreating   0          0s 
goldpinger-c86c78448-shtdq   0/1     Terminating         0          39s 
goldpinger-c86c78448-lwxrq   1/1     Running             0          2s 
goldpinger-c86c78448-shtdq   0/1     Terminating         0          43s 
goldpinger-c86c78448-shtdq   0/1     Terminating         0          43s
</code></pre>
<p>Finally, let&#x2019;s check that everything recovered smoothly. To do that, go back to the browser window with Goldpinger UI, and refresh once more. You should now see the three new pods happily pinging each other, all in green. Which means that our hypothesis was correct, on both fronts.</p>
<p>Nice job. Another one bites the dust, another experiment under your belt. But before we move on, let&#x2019;s just discuss a few points.</p>
<p><div class="alert alert-info hints-alert"><div class="hints-icon"><i class="fa fa-info-circle"></i></div><div class="hints-container"><p><strong>NOTE POP QUIZ: WHAT HAPPENS WHEN A POD DIES ON A KUBERNETES CLUSTER?</strong></p>
<p>Pick one:</p>
<ol>
<li>Kubernetes detects it and send you an alert</li>
<li>Kubernetes detects it, and will restart it as necessary to make sure the expected number of replicas are running</li>
<li>Nothing</li>
</ol>
<p>See appendix B for answers.</p>
</div></div></p>
<h3 id="experiment-1-discussion">Experiment 1: discussion</h3>
<p>For the sake of teaching, I took a few shortcuts here that I want to make you aware of. First, when accessing the pods through the UI, we&#x2019;re using a service, which resolves to a pseudo-random instance of Goldpinger every time you make a new call. That means that it&#x2019;s possible to get routed to the instance we just killed, and get an error in the UI. It also means that every time you refresh the view, you get the reality from a point of view of a different pod. For illustration purposes, that&#x2019;s not a deal-breaker on a small test cluster but if you run a large cluster and want to make sure that a network partition doesn&#x2019;t obscure your view, you will need to make sure you consult all available instances, or at least a reasonable subset. Goldpinger addresses that issue with metrics, and you can learn more about that at <a href="https://github.com/bloomberg/goldpinger#prometheus" target="_blank">https://github.com/bloomberg/goldpinger#prometheus</a></p>
<p>Second, using a GUI-based tool this way is a little bit awkward. If you see what you expect, that&#x2019;s great. But if you don&#x2019;t, it doesn&#x2019;t necessarily mean it didn&#x2019;t happen, it might be that you simply missed it. Again, this can be alleviated by using the metrics, which I skipped here for the sake of simplicity.</p>
<p>Third, if you look closely at the failures that you see in the graph, you will see that the pods sometimes start receiving traffic before they are actually up. This is because, again for simplicity, I skipped the readiness probe that serves exactly that purpose. If set, a readiness probe prevents a pod from receiving any traffic until a certain condition is met (see the documentation at <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/" target="_blank">https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/</a>). For an example of how to use it, see the installation docs of Goldpinger (<a href="https://github.com/bloomberg/goldpinger#installation" target="_blank">https://github.com/bloomberg/goldpinger#installation</a>).</p>
<p>Finally, remember that depending on the refresh period you&#x2019;re running Goldpinger with, the data you&#x2019;re looking at is up to that many seconds stale, which means that for the pods we killed, we&#x2019;ll keep seeing them for an extra number of seconds equal to the refresh period (2 seconds in our setup).</p>
<p>These are the caveats my lawyers advised me to clarify before this goes to print. In case that makes you think I&#x2019;m not fun at parties, let me prove you wrong. Let&#x2019;s play some Invaders, like it&#x2019;s 1978.</p>
<h2 id="1043-party-trick-killing-pods-in-style">10.4.3 Party trick: killing pods in style</h2>
<p>If you really want to make a point that chaos engineering is fun, I&#x2019;ve got two tools for you.</p>
<p>First, KubeInvaders (<a href="https://github.com/lucky-sideburn/KubeInvaders" target="_blank">https://github.com/lucky-sideburn/KubeInvaders</a>). It gamifies the process of killing pods by starting a clone of Space Invaders, where the aliens are pods in the specified namespace. You guessed it, the ones you shoot down are deleted in Kubernetes. Installation involves deploying it on a cluster, and then connecting a local client that actually displays the game content. See figure 10.10 to see what it looks like in action.</p>
<p>Figure 10.10 Kubeinvader screenshot from <a href="https://github.com/lucky-sideburn/KubeInvaders" target="_blank">https://github.com/lucky-sideburn/KubeInvaders</a></p>
<p><img src="../images/10.10.jpg" alt=""></p>
<p>The second one is for fans of the first-person shooter genre: Kube DOOM (<a href="https://github.com/storax/kubedoom" target="_blank">https://github.com/storax/kubedoom</a>). Similar to KubeInvaders, it represents pods as enemies, and kills in Kubernetes the ones that die in the game. Tip to justify using it: it&#x2019;s often much quicker than copying and pasting a name of a pod, saving so much time (mandatory reference: <a href="https://xkcd.com/303/" target="_blank">https://xkcd.com/303/</a>). See figure 10.11 for a screenshot.</p>
<p>Figure 10.11 Kube DOOM screenshot from <a href="https://github.com/storax/kubedoom" target="_blank">https://github.com/storax/kubedoom</a></p>
<p><img src="../images/10.11.jpg" alt=""></p>
<p>For Kube DOOM, the installation is pretty straightforward: you run a pod on the host, pass a kubectl configuration file to it, and then use a desktop sharing client to connect to the game. After a long day of debugging, it might be just what you need. I&#x2019;ll just leave it there.</p>
<p>I&#x2019;m sure that will help with your next house party. When you finish the game, let&#x2019;s take a look at another experiment - some good old network slowness.</p>
<h2 id="1044-experiment-2-network-slowness">10.4.4 Experiment 2: network slowness</h2>
<p>Slowness, my nemesis, we meet again. If you&#x2019;re a software engineer, chances are you&#x2019;re spending a lot of your time trying to outwit slowness. When things go wrong, actual failure is often easier to debug than situations when things mostly work. And slowness tends to fall into the latter category.</p>
<p>Slowness is such an important topic that we touch upon it in nearly every chapter of this book. We introduced some slowness using <code>tc</code> in chapter 4, and then again using Pumba in Docker in chapter 5. We use some in the context of JVM, application level ,and even browser in other chapters. Time to take a look at what&#x2019;s different when running on Kubernetes.</p>
<p>It&#x2019;s worth mentioning that everything we covered before still applies here. We could very well use <code>tc</code> or Pumba directly on one of the machines running the processes we&#x2019;re interested in, and modify them to introduce the failure we care about. In fact, using <code>kubectl cp</code> and <code>kubectl exec</code>, we could upload and execute tc commands directly in a pod, without even worrying about accessing the host. Or we could even add a second container to the Goldpinger pod that would execute the necessary <code>tc</code> commands.</p>
<p>All of these options are viable, but share one downside: they modify the existing software that&#x2019;s running on your cluster, and so by definition carry risks of messing things up. A convenient alternative is to add extra software, tweaked to implement the failure we care about, but otherwise identical to the original and introduce the extra software in a way that will integrate with the rest of the system. Kubernetes makes it really easy. Let me show you what I mean; let&#x2019;s design an experiment around simulated network slowness.</p>
<h3 id="experiment-2-plan">Experiment 2: plan</h3>
<p>Let&#x2019;s say that we want to see what happens when one instance of Goldpinger is slow to respond to queries of its peers. After all, this is what this piece of software was designed to help with, so before we rely on it, we should test that it works as expected.</p>
<p>A convenient way of doing that is to deploy a copy of Goldpinger that we can modify to add a delay. Once again, we could do it with <code>tc</code>, but to show you some new tools, let&#x2019;s use a standalone network proxy instead. That proxy will sit in front of that new Goldpinger instance, receive the calls from its peers, add the delay, and relay the calls to Goldpinger. Thanks to Kubernetes, setting it all up is pretty straightforward.</p>
<p>Let&#x2019;s iron out some details. Goldpinger&#x2019;s default timeout for all calls is 300ms, so let&#x2019;s pick an arbitrary value of 250ms for our delay: enough to be clearly seen, but not enough to cause a timeout. And thanks to the built-in heatmap, we will be able to visually show the connections that take longer than others, so the observability aspect is taken care of. The plan of the experiment figuratively writes itself:</p>
<h1 id="todo">TODO</h1>

                                
                                </section>
                            
    </div>
    <div class="search-results">
        <div class="has-results">
            
            <h1 class="search-results-title"><span class='search-results-count'></span> results matching "<span class='search-query'></span>"</h1>
            <ul class="search-results-list"></ul>
            
        </div>
        <div class="no-results">
            
            <h1 class="search-results-title">No results matching "<span class='search-query'></span>"</h1>
            
        </div>
    </div>
</div>

                        </div>
                    </div>
                
            </div>

            
                
                <a href="Setting_up_a_Kubernetes_cluster.html" class="navigation navigation-prev " aria-label="Previous page: 10.3 Setting up a Kubernetes cluster">
                    <i class="fa fa-angle-left"></i>
                </a>
                
                
                <a href="Summary_10.html" class="navigation navigation-next " aria-label="Next page: 10.5 Summary">
                    <i class="fa fa-angle-right"></i>
                </a>
                
            
        
    </div>

    <script>
        var gitbook = gitbook || [];
        gitbook.push(function() {
            gitbook.page.hasChanged({"page":{"title":"10.4 Testing out software running on Kubernetes","level":"2.1.4","depth":2,"next":{"title":"10.5 Summary","level":"2.1.5","depth":2,"path":"docs/Summary_10.md","ref":"docs/Summary_10.md","articles":[]},"previous":{"title":"10.3 Setting up a Kubernetes cluster","level":"2.1.3","depth":2,"path":"docs/Setting_up_a_Kubernetes_cluster.md","ref":"docs/Setting_up_a_Kubernetes_cluster.md","articles":[]},"dir":"ltr"},"config":{"plugins":["page-toc-button","cnzz","hints","sitemap-general"],"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"pluginsConfig":{"search":{},"cnzz":{"style":"1","visible":true,"siteid":"1278605528"},"hints":{"danger":"fa fa-bug","info":"fa fa-info-circle","tip":"fa fa-sticky-note","working":"fa fa-cog fa-spin"},"lunr":{"maxIndexSize":1000000,"ignoreSpecialCharacters":false},"sitemap-general":{"prefix":"https://wangwei1237.gitee.io/chaos-engineering"},"fontsettings":{"theme":"white","family":"serif","size":2},"highlight":{},"page-toc-button":{"maxTocDepth":2,"minTocSize":2},"sharing":{"facebook":true,"twitter":true,"google":false,"weibo":false,"instapaper":false,"vk":false,"all":["facebook","google","twitter","weibo","instapaper"]},"theme-default":{"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"showLevel":false}},"theme":"default","author":"wangwei17","pdf":{"pageBreaksBefore":"/","headerTemplate":null,"paperSize":"a4","margin":{"right":62,"left":62,"top":36,"bottom":36},"fontSize":16,"fontFamily":"Arial","footerTemplate":null,"chapterMark":"pagebreak","pageNumbers":true},"structure":{"langs":"LANGS.md","readme":"README.md","glossary":"GLOSSARY.md","summary":"SUMMARY.md"},"isbn":"","variables":{},"title":"Chaos Engineering","links":{"sharing":{"all":null,"facebook":null,"google":null,"twitter":null,"weibo":null},"sidebar":{"17哥的个人网站":"https://wangwei1237.gitee.io/"}},"gitbook":"*","description":"Chaos-Engineering","extension":null},"file":{"path":"docs/Testing_out_software_running_on_Kubernetes.md","mtime":"2020-11-29T07:21:36.306Z","type":"markdown"},"gitbook":{"version":"3.2.3","time":"2020-11-29T07:35:49.950Z"},"basePath":"..","book":{"language":""}});
        });
    </script>
</div>

        
    <script src="../gitbook/gitbook.js"></script>
    <script src="../gitbook/theme.js"></script>
    
        
        <script src="../gitbook/gitbook-plugin-page-toc-button/plugin.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-cnzz/plugin.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-search/search-engine.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-search/search.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-lunr/lunr.min.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-lunr/search-lunr.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-sharing/buttons.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-fontsettings/fontsettings.js"></script>
        
    

    </body>
</html>

